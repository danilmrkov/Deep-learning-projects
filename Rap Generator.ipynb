{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lyricsgenius\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "#For MSE models\n",
    "import nltk\n",
    "from nltk.util import bigrams, trigrams\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "from nltk.lm import MLE\n",
    "\n",
    "\n",
    "#For RNN model\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization,LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngenius = lyricsgenius.Genius(\"jOoA2dCGT_F8pHJJmmd3HgXotfZlbpDr2Dwd9QTA2v122saEfQ3S-RDCu_xd76u7\")\\n\\ngenius.skip_non_songs = True\\ngenius.verbose = False\\ngenius.remove_section_headers = True\\ngenius.excluded_terms = [\"(Remix)\", \"(Live)\"]\\n\\nrapper = \\'Oxxxymiron\\'\\nnum_songs = 50\\n\\nartist = genius.search_artist(rapper, max_songs=num_songs)\\nprint(artist.songs)\\n\\nlyrics = []\\nfor i in range(len(artist.songs)):\\n    song = [artist.songs[i].lyrics]\\n    lyrics.append(song)\\n    \\nwith open(\"oxxxymiron_lyrics.txt\", \"wb\") as fp:\\n    pickle.dump(lyrics, fp)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "genius = lyricsgenius.Genius(\"jOoA2dCGT_F8pHJJmmd3HgXotfZlbpDr2Dwd9QTA2v122saEfQ3S-RDCu_xd76u7\")\n",
    "\n",
    "genius.skip_non_songs = True\n",
    "genius.verbose = False\n",
    "genius.remove_section_headers = True\n",
    "genius.excluded_terms = [\"(Remix)\", \"(Live)\"]\n",
    "\n",
    "rapper = 'Oxxxymiron'\n",
    "num_songs = 50\n",
    "\n",
    "artist = genius.search_artist(rapper, max_songs=num_songs)\n",
    "print(artist.songs)\n",
    "\n",
    "lyrics = []\n",
    "for i in range(len(artist.songs)):\n",
    "    song = [artist.songs[i].lyrics]\n",
    "    lyrics.append(song)\n",
    "    \n",
    "with open(\"oxxxymiron_lyrics.txt\", \"wb\") as fp:\n",
    "    pickle.dump(lyrics, fp)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpickling our songs\n",
    "with open(\"oxxxymiron_lyrics.txt\", \"rb\") as fp:   \n",
    "    lyrics = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"Дон ли, Волга ли течёт — котомку на плечо\\nБоль в груди — там тайничок, открытый фомкой, не ключом\\nСколько миль ещё? Перелет короткий был не в счёт\\nДолгий пыльный чёс, фургон набит коробками с мерчём\\nВерим — подфартит, наши постели портативны\\nМенестрелю два пути: корпоратив или квартирник\\nСхемы однотипны, все теперь MC\\nВедь, смену породив, мы здесь достигли смены парадигмы\\nТеперь рэп — многопартийный; бэттлов наплодив\\nЯ смотрю в зеркало по типу: «Сколько бед наворотил ты!»\\nЯ б весь рэп поработил, но всё время в пути\\nУ индустрии нервный тик, валокордин — стенокардийным\\nСоберите суд, но победителей не судят\\nМы первые кроманьонцы — мы выбились в люди\\nНе пизди! Я кладу на вас, челядь, пятикратно\\nВедь мы выступаем сильно, будто челюсть питекантропа\\n\\nВесь мой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то, что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nМимо тополей и спелого хлеба полей\\nГде привидение Есенина, крест, молебен, елей\\nИз минивэна вижу землю, вижу небо над ней\\nМы всё преодолеем — если нет, то я не водолей\\nНаша земля топит одиночек как щенят\\nБыл чужой, но Охра, Порчи, Илья — больше, чем семья\\nБомбу ночью сочинял, что есть мочи начинял\\nЯ так хотел принадлежать чему-то большему, чем я\\nМир пустой, хоть с каждым вторым перезнакомься\\nЯ не биоробот с позитивной лыбой комсомольца (ay)\\nИзбавь меня от ваших панацей, домашний Парацельс\\nВедь для меня ебашить — самоцель\\nПодустал? Нам насрать! Тони Старк как стандарт\\nПара стран, автострад: Краснодар, Татарстан, Москвабад\\nПаспорта, гам эстрад — нарасхват\\nХоть по МКАД'у на старт, хоть на Мадагаскар (Ты знаешь!)\\n\\nВесь мой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nДай силёнок тут не свернуть и не сломаться\\nЕсть маршрут и есть на трассе населённый пункт\\nИ там нас сегодня ждут; нытик, не будь женственным\\nУ Руслана в деке саундтреки к путешествию\\nСнова ебло заспано, снова подъём засветло\\nСнова броник, снова дорога, мешок за спину\\nВсё наскоро, в поле насрано, дождь, пасмурно\\nМост в Асгард — после, пусть просто везёт с транспортом\\nЯ делаю каждый свой куплет автопортретом\\nЧас на чек, читаем рэп, как логопед под марафетом\\nТрафарет на парапетах: лого на стене везде\\nМоё ученье — всем, как Магомета с Бафометом\\nЯ — звезда? Дайте тёплый плед и капюшон\\nСалфетки жопу вытирать — и всё, отметка «хорошо»\\nРаньше говорили: «Я бы с ним в разведку не пошёл»\\nЯ с тобой в тур не поехал, ты проверку не прошёл (Homie, знай!)\\n\\nМой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то, что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\"], ['Sino hora sancta morta\\nSino hora sancta morta\\nSino hora sancta morta\\n\\nТам, где нас нет — горит невиданный рассвет\\nГде нас нет — море и рубиновый закат\\nГде нас нет — лес, как малахитовый браслет\\nГде нас нет, на Лебединых островах\\nГде нас нет, услышь меня и вытащи из омута\\nВеди в мой вымышленный город, вымощенный золотом\\nВо сне я вижу дали иноземные\\nГде милосердие правит, где берега кисельные\\n\\nЭй, йоу\\n«Ну-ка, слёзы вытер!\\nТо ли дело их сын, сразу видно, что он лидер»\\n«Слышишь, если спросят, то ты ничего не видел»\\n«Ай, он весь в отца, из него ничего не выйдет»\\n«Кто ж её не знает-то, всему двору сосала»\\n«Это что такое? Руки! Я кому сказала?»\\n«Всё разворовали, а бывал непобедимым»\\n«Ваш ребёнок замкнут и не ладит с коллективом»\\n«Марш в детский сад!» «Дружный класс». «Дважды два»\\n«Раз на раз, баш на баш». «Чё, зассал? Не пацан?»\\n«Тока глянь на себя, тут фингал, там синяк\\nХулиган! Стыдоба! Как ты смел, кем ты стал?»\\n«Мой-то? Да всё в облаках, как в детстве, витает»\\n«Ты ничем не лучше других, чудес не бывает»\\n«С нею? Да без шансов, он же пугалище с виду!»\\n«Хули ты всё умничаешь, сука, ты, чё, пидор?»\\n«На, сделай пару тяг — стены полетят\\nЧё, ништяк? По шестьдесят, бери сейчас»\\n«Тихий час». «Твои друзья — десять негритят»\\n«Все пиздят». «Скажи, где взял?» «Наперекосяк»\\n«Строгача!» «Как вышел, и каждое лето квасит»\\n«Сожалеем, но у нас всё так же нет вакансий»\\n«Как ты был неблагодарный, так жизнь сломал мне»\\n«На могильном камне пусть выбьют как-нибудь пошикарней»\\n\\nГде нас нет — горит невиданный рассвет\\nГде нас нет — море и рубиновый закат\\nГде нас нет — лес, как малахитовый браслет\\nГде нас нет, на Лебединых островах\\nГде нас нет, услышь меня и вытащи из омута\\nПусти в мой вымышленный город, вымощенный золотом\\nВо тьме я вижу дали иноземные\\nГде милосердие правит, где берега кисельные\\n\\n«Ты ж моя принцесса!»\\n«Ваша цель: выжать всё из её учебного процесса\\nМы в Женеву на месяц, не жалейте их, профессор»\\n«Младшая будет красавица, а эта так, в довесок»\\n«Сядьте прямо! Тут обеденный стол, юная леди!»\\n«Что за ветер в голове, что за тон и манеры эти?»\\n«Мне тут напели, кто-то в кресло вице-мэра метит?»\\n«Постеснялся бы, хотя б, своим блядям звонить при детях!»\\n«Завтра важный этап: частный пансионат\\nЕё нрав исправит, как высококлассный остеопат\\nРаз так страсти кипят, её враз тут остепенят!»\\n«Почему про отца твердят, что он властный социопат?»\\n«Гляньте-ка, вон та самая, новенькая, любуйтесь»\\n«Погоди, дитя, после школы, пока побудь здесь»\\n«Пастор поцеловал? Лазал под сарафан?»\\n«Не сопротивляйся, дитя, все дела во славу творца!»\\n«Сам не тиран и деспот, но надо знать своё место\\nА ты непутёвая недотёпа с самого детства\\nЯ даю на роскошь, новшества — тебе недаром!\\nЭта тварь из отбросов общества тебе не пара!»\\n«Съешь их, добавишь к серой рутине цветов, оттенков»\\n«Эксклюзивный реабилитационный центр»\\n«Как с настроением у нас?» «К выздоровлению\\nВместо поздравлений пусть вышлют как-нибудь поскромнее»\\n\\nГде нас нет — горит невиданный рассвет\\nГде нас нет — море и рубиновый закат\\nГде нас нет — лес, как малахитовый браслет\\nГде нас нет, на Лебединых островах\\nГде нас нет, услышь меня и вытащи из омута\\nПусти в мой вымышленный город, вымощенный золотом\\nВо тьме я вижу дали иноземные\\nГде милосердие правит и свет над берегами\\nГде нас нет — горит невиданный рассвет\\nГде нас нет — море и рубиновый закат\\nГде нас нет — лес, как малахитовый браслет\\nГде нас нет, на Лебединых островах\\nГде нас нет, услышь меня и вытащи из омута\\nВеди в мой вымышленный город, вымощенный золотом\\nВо тьме я вижу дали иноземные\\nГде милосердие правит и свет над берегами\\n\\nГде нас нет...']]\n"
     ]
    }
   ],
   "source": [
    "print(lyrics[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our set of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_songs = list(flatten(pad_both_ends(song, n=2) for song in lyrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', \"Дон ли, Волга ли течёт — котомку на плечо\\nБоль в груди — там тайничок, открытый фомкой, не ключом\\nСколько миль ещё? Перелет короткий был не в счёт\\nДолгий пыльный чёс, фургон набит коробками с мерчём\\nВерим — подфартит, наши постели портативны\\nМенестрелю два пути: корпоратив или квартирник\\nСхемы однотипны, все теперь MC\\nВедь, смену породив, мы здесь достигли смены парадигмы\\nТеперь рэп — многопартийный; бэттлов наплодив\\nЯ смотрю в зеркало по типу: «Сколько бед наворотил ты!»\\nЯ б весь рэп поработил, но всё время в пути\\nУ индустрии нервный тик, валокордин — стенокардийным\\nСоберите суд, но победителей не судят\\nМы первые кроманьонцы — мы выбились в люди\\nНе пизди! Я кладу на вас, челядь, пятикратно\\nВедь мы выступаем сильно, будто челюсть питекантропа\\n\\nВесь мой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то, что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nМимо тополей и спелого хлеба полей\\nГде привидение Есенина, крест, молебен, елей\\nИз минивэна вижу землю, вижу небо над ней\\nМы всё преодолеем — если нет, то я не водолей\\nНаша земля топит одиночек как щенят\\nБыл чужой, но Охра, Порчи, Илья — больше, чем семья\\nБомбу ночью сочинял, что есть мочи начинял\\nЯ так хотел принадлежать чему-то большему, чем я\\nМир пустой, хоть с каждым вторым перезнакомься\\nЯ не биоробот с позитивной лыбой комсомольца (ay)\\nИзбавь меня от ваших панацей, домашний Парацельс\\nВедь для меня ебашить — самоцель\\nПодустал? Нам насрать! Тони Старк как стандарт\\nПара стран, автострад: Краснодар, Татарстан, Москвабад\\nПаспорта, гам эстрад — нарасхват\\nХоть по МКАД'у на старт, хоть на Мадагаскар (Ты знаешь!)\\n\\nВесь мой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nДай силёнок тут не свернуть и не сломаться\\nЕсть маршрут и есть на трассе населённый пункт\\nИ там нас сегодня ждут; нытик, не будь женственным\\nУ Руслана в деке саундтреки к путешествию\\nСнова ебло заспано, снова подъём засветло\\nСнова броник, снова дорога, мешок за спину\\nВсё наскоро, в поле насрано, дождь, пасмурно\\nМост в Асгард — после, пусть просто везёт с транспортом\\nЯ делаю каждый свой куплет автопортретом\\nЧас на чек, читаем рэп, как логопед под марафетом\\nТрафарет на парапетах: лого на стене везде\\nМоё ученье — всем, как Магомета с Бафометом\\nЯ — звезда? Дайте тёплый плед и капюшон\\nСалфетки жопу вытирать — и всё, отметка «хорошо»\\nРаньше говорили: «Я бы с ним в разведку не пошёл»\\nЯ с тобой в тур не поехал, ты проверку не прошёл (Homie, знай!)\\n\\nМой рэп, если коротко, про то, что\\nУж который год который город под подошвой\\nВ гору, когда прёт; потом под гору, когда тошно\\nЯ не то, что Гулливер, но всё же город под подошвой\\nГород под подошвой, город под подошвой\\nСветофоры, госпошлины, сборы и таможни\\nЯ не знаю, вброд или на дно эта дорожка\\nТы живёшь под каблуком, у меня город под подошвой\\n\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\\nMy whole life, whole-whole, whole life\\nWhole-whole, whole life on the road\", '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(padded_songs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize our text\n",
    "bad_chars = [';', ':', '!', '*', 'n', ',', '.', '\"', \"'\", '(', ')', '?', '—', '\\\\', '[', ']', '»', '«']\n",
    "for i in padded_songs:\n",
    "    for char in bad_chars:\n",
    "        padded_songs = str(padded_songs).replace(char,' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <s>    дон ли  волга ли течёт   котомку на плечо  боль в груди   там тайничок  открытый фомкой  не ключом  сколько миль ещё  перелет короткий был не в счёт  долгий пыльный чёс  фургон набит коробками с мерчём  верим   подфартит  наши постели портативны  менестрелю два пути  корпоратив или квартирник  схемы однотипны  все теперь mc  ведь  смену породив  мы здесь достигли смены парадигмы  теперь рэп   многопартийный  бэттлов наплодив  я смотрю в зеркало по типу   сколько бед наворотил ты    я б весь рэп поработил  но всё время в пути  у индустрии нервный тик  валокордин   стенокардийным  соберите суд  но победителей не судят  мы первые кроманьонцы   мы выбились в люди  не пизди  я кладу на вас  челядь  пятикратно  ведь мы выступаем сильно  будто челюсть питекантропа    весь мой рэп  если коротко  про то  что  уж который год который город под подошвой  в гору  когда прёт  потом под гору  когда тошно  я не то  что гулливер  но всё же город под подошвой  город под подошвой  город под подошвой  светофоры  госпошлины  сборы и таможни  я не знаю  вброд или на дно эта дорожка  ты живёшь под каблуком  у меня город под подошвой    мимо тополей и спелого хлеба полей  где привидение есенина  крест  молебен  елей  из минивэна вижу землю  вижу небо над ней  мы всё преодолеем   если нет  то я не водолей  наша земля топит одиночек как щенят  был чужой  но охра  порчи  илья   больше  чем семья  бомбу ночью сочинял  что есть мочи начинял  я так хотел принадлежать чему-то большему  чем я  мир пустой  хоть с каждым вторым перезнакомься  я не биоробот с позитивной лыбой комсомольца  ay   избавь меня от ваших панацей  домашний парацельс  ведь для меня ебашить   самоцель  подустал  нам насрать  тони старк как стандарт  пара стран  автострад  краснодар  татарстан  москвабад  паспорта  гам эстрад   нарасхват  хоть по мкад у на старт  хоть на мадагаскар  ты знаешь      весь мой рэп  если коротко  про то  что  уж который год который город под подошвой  в гору  когда прёт  потом под гору  когда тошно  я не то что гулливер  но всё же город под подошвой  город под подошвой  город под подошвой  светофоры  госпошлины  сборы и таможни  я не знаю  вброд или на дно эта дорожка  ты живёшь под каблуком  у меня город под подошвой    дай силёнок тут не свернуть и не сломаться  есть маршрут и есть на трассе населённый пункт  и там нас сегодня ждут  нытик  не будь женственным  у руслана в деке саундтреки к путешествию  снова ебло заспано  снова подъём засветло  снова броник  снова дорога  мешок за спину  всё наскоро  в поле насрано  дождь  пасмурно  мост в асгард   после  пусть просто везёт с транспортом  я делаю каждый свой куплет автопортретом  час на чек  читаем рэп  как логопед под марафетом  трафарет на парапетах  лого на стене везде  моё ученье   всем  как магомета с бафометом  я   звезда  дайте тёплый плед и капюшон  салфетки жопу вытирать   и всё  отметка  хорошо   раньше говорили   я бы с ним в разведку не пошёл   я с тобой в тур не поехал  ты проверку не прошёл  homie  знай      мой рэп  если коротко  про то  что  уж который год который город под подошвой  в гору  когда прёт  потом под гору  когда тошно  я не то  что гулливер  но всё же город под подошвой  город под подошвой  город под подошвой  светофоры  госпошлины  сборы и таможни  я не знаю  вброд или на дно эта дорожка  ты живёшь под каблуком  у меня город под подошвой    my whole life  whole-whole  whole life  whole-whole  whole life o  the road  my whole life  whole-whole  whole life  whole-whole  whole life o  the road  my whole life  whole-whole  whole life  whole-whole  whole life o  the road  my whole life  whole-whole  whole life  whole-whole  whole life o  the road    </s>    <s>    si o hora sa cta morta  si o hora sa cta morta  si o hora sa cta morta    там  где нас нет   горит невиданный рассвет  где нас нет   море и рубиновый закат  где нас нет   лес  как малахитовый браслет  где нас нет  на лебединых островах  где нас нет  услышь меня и вытащи из омута  веди в мой вымышленный город  вымощенный золотом  во сне я вижу дали иноземные  где милосердие правит  где берега кисельные    эй  йоу   ну-ка  слёзы вытер   то ли дело их сын  сразу видно  что он лидер    слышишь  если спросят  то ты ничего не видел    ай  он весь в отца  из него ничего не выйдет    кто ж её не знает-то  всему двору сосала    это что такое  руки  я кому сказала     всё разворовали  а бывал непобедимым    ваш ребёнок замкнут и не ладит с коллективом    марш в детский сад    дружный класс    дважды два    раз на раз  баш на баш    чё  зассал  не пацан     тока глянь на себя  тут фингал  там синяк  хулиган  стыдоба  как ты смел  кем ты стал     мой-то  да всё в облаках  как в детстве  витает    ты ничем не лучше других  чудес не бывает    с нею  да без шансов  он же пугалище с виду     хули ты всё умничаешь  сука  ты  чё  пидор     на  сделай пару тяг   стены полетят  чё  ништяк  по шестьдесят  бери сейчас    тихий час    твои друзья   десять негритят    все пиздят    скажи  где взял    наперекосяк    строгача    как вышел  и каждое лето квасит    сожалеем  но у нас всё так же нет вакансий    как ты был неблагодарный  так жизнь сломал мне    на могильном камне пусть выбьют как-нибудь пошикарней     где нас нет   горит невиданный рассвет  где нас нет   море и рубиновый закат  где нас нет   лес  как малахитовый браслет  где нас нет  на лебединых островах  где нас нет  услышь меня и вытащи из омута  пусти в мой вымышленный город  вымощенный золотом  во тьме я вижу дали иноземные  где милосердие правит  где берега кисельные     ты ж моя принцесса     ваша цель  выжать всё из её учебного процесса  мы в женеву на месяц  не жалейте их  профессор    младшая будет красавица  а эта так  в довесок    сядьте прямо  тут обеденный стол  юная леди     что за ветер в голове  что за тон и манеры эти     мне тут напели  кто-то в кресло вице-мэра метит     постеснялся бы  хотя б  своим блядям звонить при детях     завтра важный этап  частный пансионат  её нрав исправит  как высококлассный остеопат  раз так страсти кипят  её враз тут остепенят     почему про отца твердят  что он властный социопат     гляньте-ка  вон та самая  новенькая  любуйтесь    погоди  дитя  после школы  пока побудь здесь    пастор поцеловал  лазал под сарафан     не сопротивляйся  дитя  все дела во славу творца     сам не тиран и деспот  но надо знать своё место  а ты непутёвая недотёпа с самого детства  я даю на роскошь  новшества   тебе недаром   эта тварь из отбросов общества тебе не пара     съешь их  добавишь к серой рутине цветов  оттенков    эксклюзивный реабилитационный центр    как с настроением у нас    к выздоровлению  вместо поздравлений пусть вышлют как-нибудь поскромнее     где нас нет   горит невиданный рассвет  где нас нет   море и рубиновый закат  где нас нет   лес  как малахитовый браслет  где нас нет  на лебединых островах  где нас нет  услышь меня и вытащи из омута  пусти в мой вымышленный город  вымощенный золотом  во тьме я вижу дали иноземные  где милосердие правит и свет над берегами  где нас нет   горит невиданный рассвет  где нас нет   море и рубиновый закат  где нас нет   лес  как малахитовый браслет  где нас нет  на лебединых островах  где нас нет  услышь меня и вытащи из омута  веди в мой вымышленный город  вымощенный золотом  во тьме я вижу дали иноземные  где милосердие правит и свет над берегами    где нас нет       </s>    <s>    o e   </s>    <s>    из точки а в точку б вышел юноша бледный со взором горящим  по дороге слегка располнел  пропил доспехи  женился на прачке  таков каждый второй тут  их рой тут  отряд не заметит потери бойца  а я жизни учился у мёртвых  как принц датский у тени отца  говорят  стать толерантным надо  соблюдать меморандум  дабы  знать все рамки и табель о рангах  а назад бумерангом не надо   рано  мир всё тот же  но кроме того  что ты винишь подошвы и сходишь с дистанции  это сомнения   вошь лезет под кожу сквозь прорези в панцире  мол  ты же вроде делаешь деньги  что же другие втирают их в дёсны   и вокруг только тернии  тернии  тернии  блядь  когда уже звёзды   напролом как обычно   через бурелом и колючки лесов пограничных  у кого-то к успеху есть ключ  но у кого-то есть лом и отмычка  дым  позабытые лица в подъезде  быть тут своим удивительно просто  только всё не сидится на месте  будто гиперактивным подросткам  так что к чёрту жалеть себя  меньше никчёмных рефлексий и больше рефлексов  когда ставится чёткая цель  то пустые скитанья становятся квестом    что ведёт нас ещё дальше  ещё дольше  всё не так  как раньше  лёд всё тоньше  нас всё меньше  и хоть это тяжко  выживает сильнейший  но побеждает неваляшка  неваляшка    наш творец то ли хлопал ушами  то ли толком не шарил  и  мы родились не в тот век  в холодной державе  не на том полушарии  помним каждое слово  знатоки того  за что не светят хрустальные совы  тут важно учиться терпеть и не ссать   санитарная зона  говорят  что смирение   благо  я пропал бы  наверное  на год  так надолго  если бы не толпы  что напишут на меня заявление в гаагу  мир всё тот же  но кроме того  что ты почти сдаёшься и клонит на дно  но ведь то и оно  говорят  что не тонет говно  а ты  хоть полумёртвый  ты помнишь   на том берегу золотое руно  и  да  одно это стоит того  тут всё иносказательно  но я в душе не ебу  как другим рассказать это  напролом  как обычно  только уже без неразлучных и без закадычных  зуботычины даже сподручны  ведь больше не нужно быть чем-то в кавычках  тут свободное плаванье  ты доплыл и всего-то пята кровоточит  тут повсюду подводные камни  но я слышал  вода камень точит  так что к чёрту жалеть себя  к чёрту рефлексии  всё поменяется быстро  пока кто-то спасается бегством  откуда-то вдруг появляется трикстер    и ведёт нас ещё дальше  ещё дольше  всё не так  как раньше  лёд всё тоньше  нас всё меньше  и хоть это тяжко  выживает сильнейший  но побеждает неваляшка  неваляшка    ты однажды проснёшься и поймёшь   это просто кончается детство  позади переезды и вёрсты  перекрёстки  перелески  скока лет я проходил переростком  скока ныл  скока верил химерам  потом был пир во время чумы  а у нас была любовь во время холеры  будь что будет  кучка судеб  тех  кто ведомы не тем же  чем скрудж  и ведь это не то  что везде   скучный студень  тут мой путь  я на нем вьючный мул  и мне дела нет  что жужжит ушлый трутень  мы так быстро взбирались  потом быстро срывались  но тут либо вверх по отвесной стене  либо вниз по спирали    ещё дальше  ещё дольше  всё не так  как раньше  лёд всё тоньше  нас всё меньше и хоть это тяжко  выживает сильнейший  но побеждает неваляшка  неваляшка    какой ты душевный  ты не с душою  ты с душком    </s>    <s>    марк  я тебе вообще кто  литературный агент или мамочка   ты когда-нибудь трубку возьмёшь  я совершенно не хочу лезть в твои дела  но та юная особа  с которой тебя видели под утро    короче  будь осторожен  помехи    чается вплоть до знаешь кого  гуру  давай серьёзно  если она вдруг потащит тебя на фавелы или  не дай бог  к нему  пожалуйста  не ведись на эту хрень про  переплетено  и на всю эту остальную эзотерику    всё переплетено  море нитей  но  потяни за нить  за ней потянется клубок  этот мир   веретено  совпадений ноль  нитью быть или струной или для битвы тетивой  всё переплетено в единый моток  нитяной комок и не ситцевый платок  перекати-поле гонит с неба ветерок  всё переплетено  но не предопределено    это картина мира тех  кто  вашей давно противится  как секта  ведь у всего не единый архитектор  всё переплетено  мне суждено тут помереть еретиком  ваша картина мира   сетка  полотно  текстильная салфетка  будто работала ткачиха или швейка  но всё переплетено  само собою   набекрень  наискосок    всё переплетено  в руке сертификат  что я сдерживаю мозг  тока сердце   никак  мой город устаёт чинить за деспотами власть  в разрезе предстаёт причинно-следственная связь  и там всё переплетено  везде сатирикон  бездействие закона при содействии икон  убейся  если ты не коп и если ты не власть  наш город не спасёт и чудодейственная мазь  хотя всё переплетено  время и цейтнот  смерти натюрморт  постель чиновника  делооборот  но каждый в своём теле одинок   рабовладелец и зелот  но город   слоёный пирог  знай  мир  по сути  прост  не берите в долг  не ведите торг  стерегите кров  не ебите голову  не говорите  гоп   берегите психов  чужаков  еретиков  ведь всё переплетено  телик и террор  с церковью бордели  казино  картель и детдом  над мэрией темно  где всей этой системе антипод  ежели с денег за дерьмо  концерны делают патент на антидот   здесь не понимая целых категорий  экосистем  эти олухи всё делят  как совет директоров  но кабинеты  фавелы  притоны  горсовет  политтехнологи  кредиторы  синод      вся картина мира тех  кто  вашей давно противится  как секта  ведь у всего не единый архитектор  всё переплетено  мне суждено тут помереть еретиком  ваша картина мира   сетка  полотно  текстильная салфетка  будто работала ткачиха или швейка  всё переплетено  само собою  чёр-те с чем  наискосок    всё переплетено  в руке сертификат  что я выше держу нос  тока сердцем в бегах  люто хочется весны  слепо на краю  осознанные сны  флэшбэки  дежавю    тут ногу сломит чёрт  и даже астарот  всё так же прыг-скок с островка на островок  шиномонтаж и пит-стоп  а дальше остановок  не видать  пока нам ног не сломит вражий костолом  вот так мы и живём  и так мы и умрём    удобрим эту гору собой  став её углём  в недобром этом городе  рабом ли  бунтарём    это круговорот природы  червяков доест орёл  а после   червяки орла  всё переплетено  внедрим полутона в их чёрно-белое кино  оттенки и цвета  левиафан ли  бегемот ли   мэр   лишь серый кардинал  а нас тут целый легион  всё переплетено  лев и козерог  с девою телец и скорпион  стрелец или водолей у близнецов   тут нету эзотерики  сынок  на соседей идут войной  если у населения спермотоксикоз  пусть  не понимая всех моих теорий  из нас лепят конспирологов  мол  у нас варит еле котелок  но чья наркоимперия  по-твоему  по артериям города гонит эти контейнеры с отходами переработки  добытой под горою рудой  проданной за бугор  пока дома  в лабораториях  из её же отходов путём обработки  гонят в народ тот самый наркотик  что называется  гор      ну  как сходили к гуру  ты теперь тоже жаждешь крови мэра  а вообще смешного мало  конечно  я тебе говорила  что эта твоя загадочная алиса тебя до добра не доведёт  откуда у тебя вообще время на заигрывание с радикалами  хотя  если тебя это вдохновляет  и ты снова хоть что-то пишешь    кстати  у тебя будет прекрасная возможность    короче  можно мы с мужем единственный раз в жизни выберемся из дома  посидишь с ником  ладно  он быстро отрубится  и ты сможешь спокойно закончить  поли       полигон   или как ты там говорил  а то я уже  честно  не помню  когда в последний раз что-то твоё читала    </s>    <s>    обложку раньше могла украшать резьба  к переплёту часто была приартачена тесьма  книга выглядела иначе   от печати u2005до u2005письма  антиквар пожмёт плечами  u2005 не судьба   значит  так    извините  в книге моей u2005судьбы давно засалены страницы  а   титульный лист исписан  смысл иллюстраций выцвел  нет абзацев целых  с форзаца сдирается экслибрис  ляссе вырвано с мясом  на свой риск перелистните  и вы поймете  как всё отличается  от литер до типографской краски  изначальный стиль не виден  в книге моей судьбы давно засалены страницы  ведь я переписываю чистовик    я хотел бы  чтоб мои песни были повеселее  легкомысленней  смелее  естественней  дульсинея  ты знаешь  я такой  только если навеселе  и с тобой  ведь наедине с собой цирк сильней  дю солей   я снова бегу дать ветряным мельницам пиздюлей  хоть полезней было б себе за этот беспочвенный страх  четверть века сомнений в себе щерит волчий оскал  рра   творческий спад  я сделал много  ещё больше просрал  к-к-как часто пиздюка утешают    не плачь  увы  не судьба победить тут   но  раз тебе выдали шанс  еби не вола   бери за рога быка  не отпускай  чтоб наверняка  закрепи  как флаг  у них гандикап  кандидат на вакансию остаться в веках  не зевай  молодняк по пятам идёт  мне бы побыть умней  но прилетев из la и  выиграв лотерею  финт выкинул  как умею  погряз в самокопаньи  фреш бладе  делах бм  а  хватку растеряв  сунул себя в  мать его  бадибэг  куда тебе-то знать  какова усталость от побед   но я сумел сломать сам  что не удавалось оппонентам  и знаю достоверно  видеть  как твою статую стаскивают с постамента  страшно  но бесценно    извините  в книге моей судьбы давно засалены страницы  а   титульный лист исписан  смысл иллюстраций выцвел  нет абзацев целых  с форзаца сдирается экслибрис  ляссе вырвано с мясом  на свой риск перелистните  и вы поймете  как всё отличается  от литер до типографской краски  изначальный стиль не виден  в книге моей судьбы давно засалены страницы  ведь я переписываю чистовик  да-да     я хотел бы  чтоб мои песни выходили почаще  представьте  настал рассвет  пока вы ходили по чаще  шаг из леса  вы щуритесь  мир  как ненастоящий  лишь мысли   где я бродил  как свалил  почему не раньше    так я выхожу из очередного эпизода  самоуничижительного психоза раз в сезон  и грабли  всё те же  я так же упрям  не доверяю таблам  хоть явно зря  реальный struggle  вот такая thug life  любимая из всех методик  чтоб обрести спец-способность  посадить себя в безвыходный колодец  так что  я сам себе тюремщик  судья  пациент  питомец  забывший  за что приговорен эксперимент контролить  чёрт   я пишу это в день рождения   35  ведь опять  всё в последний день зачем-то  вскрыться б  дядь  мой принцип   взять  обесценить себя  время мчится вспять  цирк и детсад  всем плясать  ланца-дрица-оп-ца-ца  саботирую свой труд который год подряд  как крупин  прокрастинировал  страх игру  блять  не перевернуть  идея фикс  mo  ami  но представьте себе на миг  если за голову возьмусь  как вы схватитесь за свои      плюшевый нашептал  что дефолтный казах ни на что не способен  твой флоу   модный 140  но будешь  семь-сорок плясать  я те устрою кроссовер  снится уйма тенге   делай мамбл  не bumble beezy озвучку  бля  isla  мы в паре  но мой кошерный хуй не тебе   этот изя для лучника  isla de muerta  свинячие глазки слезятся от ветра  казахские реки  как эмба  текут на твой плоский ебальник австралопитека  плак-плак  ты зассал как kollegah  ты   шпала два метра  но карлик на треках  если остров мёртвых   я архипелаг гулаг  назван ислам  но не знает  где мекка      бля  чувак  это не isla de muerta    кто не isla de muerta     ну у тебя в паре не isla de muerta    в смысле не isla de muerta     ну у тебя в паре другой казах  он просто тоже читает быстро и тоже казах    балять  реально  приношу свои извинения всему казахскому народу    погодите  эй  я беру всё назад  вы все разные  это не рандомный казах  совершенно другое лицо  я укор вижу в ваших широких  огромных глазах  мой оппонент   асылбек  асыл  asylllum  отсталый мамбет  спёр кассету bo e thugs  подписал  das efx   теперь быстрый плов на ужин  завтрак  обед  та-ра-тор-ка  ram  revolt    кал  стайл нашёл на барахолках  эй  revolt ещё та хуерга  лучше с редькиным за руку на буерак  асылбек  я сварю из тебя куырдак  не позорь тамерлана   банк империал  слышь  у меня в нике три икс  эй  завяжи  как  iki l  брось  как тебе выиграть приз здесь   у тебя в нике три l   loss   не понял отсылок  asylllum   китаец  держи от бациллы вакцину  оциллококцинум  шепни от меня своей маме   родная  спасибо за сына     в этом раунде ты антагонист и борец типа биба  керамзита  зябэ  ха   но твой первый концерт был в 11-ом в алма-ате  ты пришел на меня и витю сд  я бы никогда не спутал тебя  асылбек  называй меня только на  вы   я собрал в астане больше твоих друзей  чем способны собрать твои похороны  real talk   вы мне кого подослали   наивней искусства нико пиросмани  считать  что я буду с тобой долго базарить  опомнись  пацан  ты никто в казахстане  эй-эй   этот щенок думал  хасл   назвать себя в честь легендарных фитуль  типа  смоки сказал то что рэп   это асл  окси сказал  что ты сядешь на уй  бич  мне не кидали жирный инсайд  про мажорский твой вуз и про фирму отца  да-да   саудовский лейбл  кино где попса  и про фонд сороса  что саппортил ваш сквад  но  о-ой   ты настолько пресный шельпек  всем срать  что сливает информатор куда  принеси мне шек-шек  послушай совет  пока ещё остывают бешпармак и шурпа   а-а-а   асылым асыл қалай мақалай  атыңнан қазы жасаймын жарайд  уоу   жоқ болуыңды қазақтар қалайд  қазақ болуыңа балдар қарамайд  э қотақбас тездеп бағала  эй   трэк соңында табыласың қаза  эй   ты не красавчик  хоть ты и казах  ақ жолыңды бітірді аққұлақ  а-а   қотағым жеме  шешең амы  тьфу  блядь    </s>    <s>    у прилавков супермаркетов сутолока  в барах давка  хоть этот год уже не на носу  как бородавка  мыши норы под полом роют  лицо в доме напротив в белой рамке стеклопакета  как polaroid  дикий пейзаж  за окном бухие крики – кураж  сюда не едут хипари купить в бутике винтаж  если в  wiki  задашь  кеннинг таун   выкинет аж  статью  что тут полная жопа – ники минаж  издохший год летит в ведро отходов  работа с доходом в похоронном бюро находок  я сжигал мосты  им не дав остыть  забыл срам и стыд  как женский монастырь  скажи  как мне быть жизнерадостным  а   вы все натуры тонкие  не так ли   вам бы только киноплёнки  канн и громкие спектакли  а в моей каморке по полкам иконки и пентакли  и чтоб кончить  мне нужны плётки  японки и тентакли    я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу  я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу    кругом ханыги  звуки драк  крики  во дворы  будто коты  прошмыгивают барыги  мрак  блики  со стены подмигивают pac  biggie  макдак  сиги – финал ещё одной из глав книги  что мудакам обещают – не верьте  ведь жизнь коротка  полна страданий  кончается смертью  и я понял  всё – ни то  ни сё  дым всё  женился  устроился  уволился  смылся  меня на погосте ждёт ад в гости  да бросьте  как кости  был агностик  от злости стал как гностик  я вошёл в симбиоз с этим болотом  как юннат с енотом  сенат с синодом  катод с анодом  я знаю  мне не хватает самоорганизации  ведь я умею только жаловаться и огрызаться  повсюду пыль и насекомые  если б я изучал людей  то я был бы паразитологом    я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу  я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу    я ни черта не делал  я просто жил  и дети нарисуют на асфальте мелом очертания тела  кто-то другой пойдёт гулять с оравой по садовой  данс-макабр  приглашу костлявую на пасодобль  мне говорят  что я уже пишу  как смоки мо  одни намёки  мол  и тягомотнее  чем покемон с пиноккио в немом кино  или битников проза  хуй знает  бит путает мысли  как прозак    я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу  я влез на ходу  и даже  если здесь пропаду  крест на горбу  бегство – табу  неизвестно кому зачитывая текст vagabu d  пока не найдётся место в гробу    я влез на ходу  в экспресс попаду     да  я с детства в аду  стресс  я тону     та-на-н    эверест  катманду     я под    я под каждым из небес  vagabu d  ммм    тошнит  как от невест тамаду  ахиллеса пяту  мест на борту     нет  это уже левак какой-то  на хуй    </s>    <s>    vagabu d cla     я не просто баламут  хам  я свой собственный плутарх  это летопись  нужны разные флоу   у меня девять есть  зови меня wu-ta g  и пусть ты хоть аламут брал  но я неприступен  я не преступен  хотя тут вам  даже дети продадут грамм  половину зовут хан  если тут кто-то курд  то он не воннегут  дам слово  что я не расист  но тут каждый второй   орангутан  всюду блуд  болливуд  хлам  это e16  вперемежку нации   бангладеш  вьетнамцы  и полно матерей в 16  а белый   как мутант  да  это дом  хоть и город не питер  битло на репите  и я сру на вас  для кого-то борат   юпитер  для кого-то бруно   марс  а я делаю драм или дабстеп  грайм  и не бабский рэп  хотя  это и вряд ли придаст мне  в глазах вашего тайного братства  вес или статус блаватской  мне угнать пепелац бы  улететь и не видать всего блядства  но  видать  я натаскан на вас  будто амстафф на  фас  или янки на фастфуд  да  я хочу заграбастать богатство  но в пизду ваш парад педерастов  быть властью обласканным пастором паствы  уж лучше пересадка на пластоу  мне ваши слова  будто капли с гуся  я уйду  лишь когда будет под пятьдесят  не живя тут  понять сути грайма нельзя  я с тех самых улиц  где грайм родился  и тут грайм не иссяк  это крайний ист-сайд  а не север  где хайм и исаак  твоя банда   детсад  а не команда друзя  я придумал себя и собрал по частям  бррря  это восточный лондон  он как отчий дом нам  но забудь его лубочный образ  тебе тут будет очень стрёмно  я ночью чёрной поведу через восточный мордор  подзамочный город  оживёт в водоёмах и сточных водах  в проёмах барочных окон  e2–e4   районы с почтовым кодом  а не ход против чёрных  их тут полчища  орды  привлечённые сочным кормом  как мы в 94-м  когда я ещё был не в счёт и никчёмным  капюшон оторочен чёрным  все хотят весь пирог  а не кусочек торта  всюду почерк чёрта  под его чечёткой  хрустят мои позвоночник и рёбра  большой куш  а не лондон джама  и тебя душит новая жаба  ведь вы вроде украли все  но знай  каждый мой трек родит новый жанр  у нас изобретают стили  будто банкноты в китае  и не говорят  oi   когда что-то роняют  а когда кого-то кидают       </s>    <s>    год назад я сидел на скамейке в общественном парке  на углу beckto  и barki g  думал  как из англии выйти пешкою в дамки  к верху иерархии  я себе всё это накаркал    год назад я сидел на скамейке в общественном парке  на углу beckto  и barki g  думал  как из англии выйти пешкою в дамки  к верху иерархии  я себе всё это накаркал  теперь меня слушают хипстеры  арт-богема   ведь он из лондона  где выставки   portobello   не понимая  что внутри строки наболело  не понимая  что внутри смолит карфагеном  жополизы  недруги-журналисты  первые компромиссы  лейблы-монополисты  я лишний  будто новички на вакханалии  у всех  кто повыше  воротнички накрахмалены  был самодел  самопал  кто-то не захотел  сам отпал  мы   в отель  вам   подвал  но тут чертополох  тут  если не успел  то подох  там  где ты видишь успех  я   подвох  ощущаю себя стариком  хоть просыпаюсь со стояком  не спасёт благодать ста икон от сознания  комплексы съедают изнутри  я зову это  комплексное питание   от бетона east лондона на кавер биллборда  в клубе на малой бронной блюю под ивана дорна  зависаю с бомондом  засыпаю в уборных  под хлорофилом  но чаще под хлороформом  на десерт рвота с hei eke   боже  как же я себя люблю   отто вайнингер  моя жизнь   это приключения незнайки  наизнанку  будто шлюха в america  psycho  эстеты напридумывали терминов  не зная  что выгуливали цербера   наконец-то русский рэп признали     мне не похуй ли   субкультура разрастается   опухоль  подземка не принимает и бог с ней  эстрада напоминает клубок змей  я  как итог  злей  все играют роли   косплей  моя трасса уже не слалом  а бобслей  перебираю тщетно аллегории  меж тем моя любовь ебётся с кем-то в черногории  мне нужен врач  лучше на дом  и немедленно  хаус  дре  живаго  менгеле  я подарю свою шкуру  кому по нраву  цели  до которых дошёл    не дают отраду  из е16  где на углу продают отраву  говорун на левом плече  гамаюн   на правом  уже зовут большие дяди на  пикник „афиши“   после меня там читает влади  мы фит запишем  и всё вокруг замечательно  но  убей меня  пока я не скурвился окончательно  я всегда начинаю самоуверенно  но забиваю хуй и тону у самого берега  они жаждут подгон  а я всё решаю  кто я  андеграунд или продажный гондон  дева не давала в жопу  говорила   принцип   но это во мне убило принца  как гаврило принцип  мироздание дремлет  но  кажется  дождь собирается  очень приятно  гремлин    </s>    <s>    а впрочем похуй  бэттл  я разоружаю себя  да  я такой же  как ты  пью и разрушаю себя  я инфантилен  сколько б не платили  жаль я синяк  и я бы сдох  но теперь у меня есть небольшая семья    из муми-долов  хоббитонов и слонопотамов  из кладовок  антресолей и полуподвалов  из переходов  что познали баллоны вандалов  офисных окон  где я пробыл рабом феодалов  я что-то тихо вынес ночами  заныкал  и пока начальник мне не скажет   с вещами на выход   на выход  блять    ты как всегда опечалишь барыгу  взяв то  что толкает англичанин с очами навыкат  мне подождать и поднажать или уйти  чтоб это дальше продолжали подражатели   несу хуйню  и они безропотно вторят  но в итоге  кто учёный  а кто подопытный кролик   мне нужен врач  лучше на дом и немедленно  ватсон  фрейд  курпатов  пеппер  горизонт заволокло  и где-то хлопнуло окно  похуй кто  но дай мне свет  бог или оптоволокно    мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни  мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни    я много лет не выпускал стафф  ждал  пока он станет жирным  как фальстаф  видать  ты думал  что пора  что ты   five star    ты поспешил  насмешил  это фальстарт  я проповедовал щеглам  как франциск ассизский  видел смерть  она сказала   распишись на сиськах   мы разной породы  вы   безопасной дорогой  а я   то желтою кирпичной  то красной ковровой  коль судишь меня  удостоверься  не мажор ли ты  я был голодным mc  съел других и стал прожорливым  ты пугал и свой понос исподтишка толкал  ну а пока у тебя даже толстая кишка тонка  у твоей мамы сестра  что мокра  как водоём  её дыра настолько мала  что я наноёб  мне пишут mc   нихуя себе  брат  как классно    и вам спасибо  биомасса и протоплазма    мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни  мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни    если ночью все кошки серы  что же это за чёрный кот  что ведёт со мной каждую ночь беседы   диалоги о чём угодно  про  закат европы  и ложь системы  если б это был чёрт  его бы выдавало зловоние  точно из бочки серы  но ничего подобного  он глядит за окошко в темень  дождь со снегом ебошат с неба  мы все в отрыве от почвы слепы  а на ней будто тощий стебель  и в том числе ты  одиноки в ячейках и сотах  как мощи в склепах  всё равно вешать нос щас нехуй  против всех  и  бря-я   круговая порука  тот  кто деньги скопил  завещает их внукам  но  мои сбережения равны совокупности мной издаваемых звуков  ставлю вас в угол  коль ты   супер mc  я   лекс лютор  на концертах лес рук  но я по-прежнему тут на ветру  как флюгер  сколько лет я был где-то в подземке  сколько зим проебал  сколько сил потратил  демо-тейпы  онлайн-баттлы  горе-лейблы  хватит   и хоть мы из пробирки  но мы не готовы жить на галерке  в давке  как на ходынке  я джинн из бутылки или чёртик из табакерки  я один с лампой  от red bull а зрачки   манга  депривация сна и неебаться устал  под глазами круги   панда  всё  что есть в этой ёбаной песне  я для вас оторвал от груди   данко  и я бы вышел из этой игры  но я боюсь повредить ей гланды    мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни  мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни    мне не передать  что в моей черепной коробке телепередач  я вернулся  держись  мир  вы заказали мою голову   я подаю вам признаки жизни    </s>    <s>    это всё правда  я те зуб даю  бля буду  выведи на чистую воду  как обряд вуду  правду  не вымысел  только факты без примесей  вывезешь  детектор лжи  это всё правда  я те зуб даю  бля буду  выведи на чистую воду  как обряд вуду  правду  не вымысел  только факты без примесей  вывезешь  детектор лжи    я не лгу  я   элита  да  я дворянских кровей  вот мой герб   на нём утка и липа  но при том я гастарбайтер почти что  ведь я гострайтер для кравца с батиштой  я отыграл сотни пьес  в том числе  венецианский купец   я принял ислам   вот те крест  я опять курить крэк стал  я прошёл кастинг на вестерн  ожидайте трансфер на black star  владею тайнами оккульта и карманной катапультой  я писал это под пальмой в акапулько  тебя снова обманут все  кеннинг таун   мюнхгаузен  я живу у дяди в пентхаусе  примерный семьянин  стопроцентный славянин  под известным псевдонимом был до рэпа знаменит  ты занял деньги в ломбарде  зачем   1239   pi -код к моей дебетной карте    это всё правда  я те зуб даю  бля буду  выведи на чистую воду  как обряд вуду  правду  не вымысел  только факты без примесей  вывезешь  детектор лжи  это всё правда  я те зуб даю  бля буду  выведи на чистую воду  как обряд вуду  правду  не вымысел  только факты без примесей  вывезешь  детектор лжи    я помирил когда-то крипла с сд  это не ради рифмы  но за мной следит фсб  дипломированный сомелье  и вы узрите ваших прародителей  в моей квартире  в дезабилье  е    однажды я пытался мышь есть  на первом курсе меня вербовала mi6  я на любых выборах культурно плевал в урну  мал урбан  впереди телеэкран   иван ургант  rap ru пугали меня после румы бандюками  луперкаль изъясняется рунами да матюками  грубой праворадикальной руганью  и  кстати  да  я не блефую  никогда не вру даже в малых деталях  я от скуки решил отдать в руки booki g machi e  права на все мои треки  я   мудрый харун ар-рашид  эта инфа как лавина  в ней вранья половина  но ты никогда не узнаешь  какая именно    </s>    <s>    твоя тёлка   босфор  самый узкий пролив  я ее драл на спор  на полную врубив дрейка  плюс ушёл  всё на блузку пролив   мазал тов  теперь она зовётся пролив дрейка    маргинал  ты манерный фигляр  постмодерн   бодрийяр  я кладу хер  как приап  он для твоей благоверной как кляп  ты не гремлин  как я  тут инферно   ты тленный  как для тля  ведь я  рэп-патриарх  репатриант  чей ареал  от спб до uk  где меридиан  эмигрант  как эльфы в белерианд  и из нета в реал  ты не ретро  как пин-ап  а плюсквамперфект  старина  григорианский на земле календарь   не вариант  времени мало  мой рэп   грей дориан  твой   мемориал  да  я изувер и ересиарх   ортофен  нозепам  в голове идей  будто бы я мегацефал  идей  негоциант  антипод  как дезодорант  пиздец не за горами  чёрный флоу  хоть не загорал  твой   лесоповал  я жид и целюсь из обреза по вам  свинец наповал  словесный smith & wesso  всем  кто на прогресс уповал  в жизни   мил  в бэттле   агрессор и хам  ювентус-реал  исход известен  ну так бейся теперь здесь и сейчас  плюс и дева   это не зодиак  не эрих ремарк  но тоже с женским неймом и фейк  будто де габриак  фэйдер вверх на преамп  присесть  как эм на re-up  всем пиар  кпд  как реактор  дел миллиард  был до gree park a  как die a twoord в юар  ты дал крен  как фрегат  я твоей маме дал хрен   агрегат    я начал бацать рэп  едва ходить стал  а не в 13 лет  будто бар-мицва  ещё не мацал дев  не пал совдеп  был мал совсем  врубаться в тему захотел  по красоте рубить стафф  я начал бацать рэп  едва ходить стал  а не в 13 лет  будто бар-мицва  ещё не мацал дев  не пал совдеп  был мал совсем  врубаться в тему захотел  по красоте рубить стафф    ты за деньги в карман на заднем сиденье лабал  фристайлы и стреляешь мелочь   брейвик ламар  и хоть рэп   не первый канал  немало лейблов  команд  в которых каждый первый педиковат  как гелиогабал  да  я звучу  будто я гелий глотал неделю подряд  но мой флоу кошерен   халяль  сколько ни перебивай в гневе меня  идёшь на бэттл   бери смену белья  я подкатил к твоей маме шары  как хренов бильярд  империя  скоро везде филиал  хоть не сити-банк  как успеть всё  здесь десятикратен стресс и дела  в бороде седина  всё  чтоб достичь суметь идеал  а не чтоб вместо дома   крепость и вал десять гектар  но рэп   целина  поднять сложней  чем для блядей целибат  чем понять акцент твой  если не декан и не телепат   катаюсь по стране  хоть не делегат   спб  байконур  тюмень и екб   tele club  эдем или ад  где бы я ни был   dis eyla d и гулаг  бэттл-рэп   это флаг  легенда тут big l  а не pac  стадо просит хлеба и плед   получит хлев или кляп  запомни  я не делаю трэп  локтями  я делаю hop    я врубаю кровосток  стреляю словами как робокоп    я начал бацать рэп  едва ходить стал  а не в 13 лет  будто бар-мицва  еще не мацал дев  не пал совдеп  был мал совсем  врубаться в тему захотел  по красоте рубить стафф  я начал бацать рэп  едва ходить стал  а не в 13 лет  будто бар-мицва  еще не мацал дев  не пал совдеп  был мал совсем  врубаться в тему захотел  по красоте рубить стафф    </s>    <s>    по асфальту  мимо цемента  избегая зевак под аплодисменты  обитателей спальных аррондисманов  социального дна  класс- и нац  элементов  мимо зданий муниципального центра  и статуи вице-мэра  насвистывая концерты  я спускаюсь  беспрецедентно оправданный  лицемерно помилованный  тридцатилетний  бля  меня явно любит вселенная  не знай меня все  я вряд ли бы уцелел там  но  видимо  мэру надо улице бедной  продать было милосердие да правосудие щедрое   хер знает  я живой  спасибо фортуне  и балансирую через пропасти на ходулях  иду  сутулясь и подпрыгивая  как дурень  сквозь судьбы и бури к неуловимой ultima thule  я думал  время вышло  вымя выдоено  на дороге рытвины и выбоины  валуны и глыбы  на моей тропе меж мира  войны  одни считают  что я сильно хитровыебанный  другие видят во мне наивный мир игр и книг  ты пойми  я   гибрид  я вырос и таким  и таким  я не был задуман для света софитов  интриг  и адреналина  выбор линии судьбины хитрит  я просто годами писал и смотрел в окно  зачёркивал  стирал  неустанно толстел блокнот  хрупкие миры распадались во тьме на стол  покуда мёртвые кумиры взирали со стен во двор  я был один  мироздание по краю вело  теперь из каждого киоска смотрит моё ебло  но что изменилось  ничего внутри  а с виду зело  ведь закрутили в узелок сильные мира сего  до того  что стресс  кипиш  бег  квиддич  раньше я думал  что в 30 лет   финиш  но я здесь  видишь  gle fiddich  они куксились  дулись   хули ты не сгинешь    они всё чё-то просят и портят воздух и нервы  судмедэксперт   то ли кровь  то ли сперму  суки   руку и сердце  издателям   букера  сделки  читателям   чучело в клетке  эй  я видел цирк ваш с виселицы  забудьте сунь-цзы и лао-цзы  ведь в этом цирке лишь два пути   суицид или стоицизм  и если выбрал не суицид  тогда терпи  хватит ныть  дай вовсю идти  и да  мы ссым  каждый ссыт  страх и солипсизм  но назло миру мы взлетим среди суеты    мой город вне времени  вне территории  племени  рода  империи  троя  помпеи  рим  мой город   морок и видение  что во тьме видит бедуин  мой город на горе руин  мой город   лабиринт  я по нему слепой и неумелый гид  и мой город не верит им  его правление внутри  но ни под горою  ни в мэрии    я стоик  будто луций сенека  спускаюсь от палаццо элиты к улицам гетто  раз уцелел  то надо жить и глубже дышать  и девочка пиздец ушла  предав  но я переживу и это  ты ответь на такой вопрос мне  может ли творец жить в башне из слоновой кости   вхожим быть во дворец или яро против вельмож  или сохранять свой нейтралитет рядом с    звук выстрела     ты ещё не дома  странно  слушай  ну  что я могу тебе сказать  кроме того  что ты идиот  да  я очень рада  что тебя отпустили  мы тут все чуть с ума не сошли  в общем  ты возвращайся  а я пока прочитаю  где нас нет   кстати  название   говно   целую    </s>    <s>    опять ебёт мозги  пресса  озноб и джетлэг  засунь себе в жопу бейдж и вопросы  шерлок  даём рэпу часть себя  словно тосин   дженту  косим кэш  не вытерев слёз  на надгробии год начертан  мы просим тщетно   дай нам всем тут не просто ночлег   и что кочевник  так ждёт  мигнёт огонёк харчевни  мы те  кто зевакам всё здесь приносит в жертву  те  кто на шее кольцо проносит и бросит в жерло  а что взамен-то  дружок  а что взамен-то  что   народной любови горький хлеб  стёб на шоу вечернем   когда ты бедный  ты ждёшь всех грёз от большого чека  но куда пойдёшь  когда чек не спасёт  как сок лечебный      окс  под чем ты   у тебя же всё  о чём только мог и мечтать рэперок подземный  ты возрос из черни  вместо того  чтобы спокойно радоваться взлётам   вновь плачевный  тексток ты чекни  как мы живём  у тебя так не живёт  небось  никто вообще  а у нас алкоголь дешевле  чем учебник  так что  если ты ноешь  ты обсос  как чендлер  это больше  чем ты  больше  чем твоё самокопание никчёмного клоуна  чел  ты вспомни  мы с тобою в ногу сто лет шли  бок о бок сто лет шли по дороге до мечты  из депо до конечной  чем залечь на дно  ещё на пару лет лучше прочь эту хрень шли   слышь  жидок-то  по ходу  не вечный    а  ну-ка  соберись  ты чего такой нежный      правда ваша  но вы  по ходу  рановато выводы сами из неё сделали  пацаны  я за полтора года потерял себя  друзья   двое мертвы  третья в реанимации  каждый мой шаг под микроскопом  и real talk  если хоть раз оплошал   всё  я ждал год  чтобы отдать трон  шапка мономаха тяжела  я подустал всех ебать в рот  возраст  когда стартуешь    всё просто   всё ловишь из воздуха  быкуешь на взрослых  помнишь  мой дом был дрейфующий остров   из имущества только будущее и воздух  нечего писать  окси  отрасти волосы    я за три года сми облысел полностью  и я хотел бы уйти  но мне друзей подвести  не дают ошмётки чести да совести    а из ротовой полости надо бы новый стих  и чтоб флоу   пластид   вроде поп-артист  но и до сих пор mc  big boss  c e o   что кормит team  добрый сын  свой в доску   тот же тип  оксфордский выпускник   всем до пизды  ты над пропастью во ржи  над пропастью поржи  на руке кроули девиз   бог простит  мс дружат против  кружат  бродят  уже хороводы вокруг моей туши водят  но я труп живой  меня душит плоть  и если ты меня пырнёшь   надорву животик  так лейся  песня  цыганочка  стал неясно чем  был школяр в очках  но я не грущу нихуя  начхал  ведь меня любит моя биполярочка    </s>    <s>     obody loves me  loves me  loves me   obody loves me  loves me  loves me   obody loves me  loves me  loves me   obody loves me  loves me  loves me    это сказка для взрослых  утро после холостой свистопляски  накалившийся космос  как тостер  и в нём уплотнившийся воздух стал вязким  у меня было предчувствие квеста  и ты вошла в это пространство  как в масло  но ты не думай  будто я жду тебя с детства  а если честно  то жду  хули  здравствуй  я дитя бетонной коробки  с лифтом  где нужно тянуться до кнопки  а ты уверенно ходишь по моему солнечному сплетению лунной походкой  и ты будто вообще не с лестничной клетки  не потому что иногородние шмотки  а потому что такие твари тут редкие  да что там редкие  не годятся в подмётки    каждый ёбаный день  в этом зиккурате пролитых слёз  в вавилонской башне мёртвых идей  от которых череда одних чёрных полос  в колизее нереализованных грёз  в клубе одиноких сердец  как я тебя заебался ждать и нашёл  моя девочка пиздец     obody loves me  loves me  loves me   obody loves me  loves me  loves me    всё предельно серьёзно  и ты из тех  что смертельно опасны  в сердце больше борозд  чем от оспы  оставишь  последствия злей  чем от астмы  у меня было предчувствие теста  ты подошла  и всё мироздание погасло  ведь я не знал  что тебя найду наконец-то  я в этой бездне тону  хули  здравствуй  ты дитя холодного фронта  с камнем  где обычно орган с аортой  а я всё так же не понимаю  на что тебе  в лаборатории на правах подножного корма  и почему эта боль от эксперимента  у меня даже нет воли быть непокорным  я не пойму отчего накрыло конкретно  но накрыло конкретно и  по ходу  надолго    каждый ёбаный день  в этом зиккурате пролитых слёз  в вавилонской башне мёртвых идей  от которых череда одних чёрных полос  я пока тебя-то знаю пару часов  но усёк одно  ты мой крест  хоть и понимаю  ты однажды уйдёшь  моя девочка пиздец     obody loves me  loves me  loves me   obody loves me  loves me  loves me   obody loves me  loves me  loves me   obody loves me  loves me  loves me    </s>    <s>    можете величать меня исчадьем ада  можете линчевать меня  мыча как стадо  но на мне нет печати зла  сгущать не надо  краски  я счастлив  что я не раб мещанских взглядов  можете обличать меня  крича с экрана  можете исключать меня из ваших кланов  но вы ж сами не без греха  признай  что я был  к несчастью таким же  как ты  ша  молчать и на пол     сложней всего было найти тротил и запал  и пронести на бал  фитиль был подозрительно мал  актовый зал  ак достал  с предохранителя снял  как удивится директриса  лишь увидев меня  11-ый  а   не хотите мира   выйдет война  нет  я не маньяк  при чём здесь чикатило  битцевский парк   терроризмом признанный акт   для других единственный шанс  естественный шаг   объявить обидчикам личный джихад  я терпеливо ждал  но что делать  мой класс   педики  жертвы маркетинга  масс-медий и косметики  вы все в ответе за то несчастье  что щас светит вам  последний звонок  в шейный позвонок мой глаз метит вам  захожу без шума и спецэффектов   им деться некуда  палю известной в школе сердцеедке в сердце метко  прости золотая  но врачи тебя не залатают  нехуй рвать мои письма  толпа бежит из зала  тая    можете величать меня исчадьем ада  можете линчевать меня  мыча как стадо  но на мне нет печати зла  сгущать не надо  краски  я счастлив  что я не раб мещанских взглядов  можете обличать меня  крича с экрана  можете исключать меня из ваших кланов  но вы ж сами не без греха  признай  что я был  к несчастью таким же  как ты  ша  молчать и на пол     шарики  ленты  рядышком двоечник и отличник  стонущий от боли обидчик   что может быть ещё мелодичней   те  кто свинцом не напичкан забаррикадировались в коридоре  я перезарядил  проверяю затвор  и стреляю в упор  иду гулять по школе  я не сатанист  не фанат металла  влом  быть таким  их стволы   металлолом  не псих  не фрик  не играл давно за компом  так что не верь ментам  народ  жизнь  как игра в домино  всем важно одно   забить отпущенья козла  вы мне мученье доставили  но ваш час быть мишенью настал   время замедляет свой бег  я не понимаю  кто стреляет по мне  где-то падает дверь  à la guerre comme à la guerre  снайперы мелькают в окне  из меня что-то начинает течь  везде томатный кетчуп  мне обеспечена вечность  в руке детонатор  я просыпаюсь рывком  покрыт испариной лоб  кошмар  а не сон  в кровати жарко   я шагаю во двор  спускаюсь в погреб  отпираю ржавый медный замок  и собираю ствол ак   завтра последний звонок    можете величать меня исчадьем ада  можете линчевать меня  мыча как стадо  но на мне нет печати зла  сгущать не надо  краски  я счастлив  что я не раб мещанских взглядов  можете обличать меня  крича с экрана  можете исключать меня из ваших кланов  но вы ж сами не без греха  признай  что я был  к несчастью таким же  как ты  ша  молчать и на пол     спасибо kass у  так что не смейте обвинять меня в плагиате  все сегодня на одноклассниках  вконтакте  но далеко не все одноклассники в контакте  понимаете  о чем идёт речь   йе  оксимирон  в лондоне наконец-то солнце  альбом twi$terbeatz  2009-ый    </s>    <s>    o e   </s>    <s>    я как пародия на  тающий апрель   чтобы ты спотыкалась об меня с первых слов  в левую грудь  нарисованная дверь  нарисованный удар  выбивал с оков  и тебя всю идеально похищал  ограбление души   всё бросай в пакет  сейчас уйду  всем ворованным лежать  всем ворованным молчать  вас здесь больше нет    и если слышишь ты  давай вернёмся в наш дом  вернёмся в наш дом  дом  дом  или опять молчи   контрольный выстрел в лицо  как выстрел в моё лицо  и  как всегда  в моих куплетах кто-то умрёт  кого-то убью опять  ха-ха   это так легко-оу уо-о    привет со дна  ты где-то там  где  свет и электрофанк  смех  левый электорат  обед во фраках  светский раут  там нет меня  здесь приём стеклотары  пакет отрав  едких  как винегрет с утра  нет контакта и недотрах  сто лет назад  летом нам так хотелось весь свет дотла на хуй сжечь всю планету  став прахом  пеплом и ветром  как мне к тебе взять билет обратно без предоплат   стеклопакет не греет  отъехал трап и поблек экран  детектор прав   я себе нередко врал  я  наверное  вертопрах  в деле потерпят крах те  кто без цели и вектора стелят куплеты поверх октав  наверняка рады те  кто нам это предрекал  но я непререкаемо верил  что нет преград и в их реплики не вникал  не-а   теперь плутать неприкаянно в дебрях материка  выпал жребий нам  где петляет во тьме река  рэп   это кал  всюду гипербола да синекдоха  ежели без них  что тебя не хватает   конкретный факт  мне зеркало говорит    пей до дна  пей до дна  твоя жизнь ещё дней полна  кругом летний гам  а не герника  тебе с ней никак  скорей технократом стал ретроград  чем ты таким  как ей надо  детский сад и взрослеть пора  грех пытаться тень догнать  бред  как буклеты гербалайф  нет на свете примерных пар  время делать карьерный план   а я в ответ киваю   респект за совет  братан  если так  убей меня  как дерек негра  поребриком     и если слышишь ты  давай вернёмся в наш дом  вернёмся в наш дом  дом  дом  или опять молчи   контрольный выстрел в лицо  как выстрел в моё лицо  и  как всегда  в моих куплетах кто-то умрёт  кого-то убью опять  ха-ха   это так легко-оу уо-о    я думаю  рэп меня спрятал от проблем  заполнил в башке пробел  что жизнь это не побег  за брендом  свали с моей квартиры  я бедный  плюс литр коньяка опять подменит обед мой  но где ты  что учила танцевать на рождество   и наш вечно голодный стол  и таял твой гардероб  бежали со всех работ  ты трахалась на износ  я видел всё это  но нам нужно платить за дом было  так что терпел реалии мира  мы своровали одежду   вау  и ты чертовски красивый  мой криминал  врубайте свои камеры  живо  два персонажа из фильма  мы помолчим   аплодируй  блять       и если слышишь ты  давай вернёмся в наш дом  вернёмся в наш дом  дом  дом  или опять молчи   контрольный выстрел в лицо  как выстрел в моё лицо    я мог построить большую карьеру  ау   подъебать всю систему  и бросить жопу в каен  и 200 миль в час  в руках jack da iels  я готов сейчас въебаться в премиум твой  рулю на встречную  увидев тебя  сука транслирует улыбку с лобового окна  ну что  пока  нога раздавила педаль  такой вот дерзкий наш последний поцелуй  а жаль       и если слышишь ты  давай вернёмся в наш дом  вернёмся в наш дом  дом  дом  или опять молчи   контрольный выстрел в лицо  как выстрел в моё лицо  и  как всегда  в моих куплетах кто-то умрёт  кого-то убью опять  ха-ха   это так легко-оу уо-о    и мы бы могли остаться в живых  но это скучно здесь быть живым  и если опять захочешь войны  ты намекай   мы всё повторим  а если не так  то смейся в лицо  мы всё равно вернёмся в наш дом  дом  дом  дом       </s>    <s>    o e   </s>    <s>    и так каждый божий день   больше денег  больше дел  больше дев  больше демонов  но как из кожи ни лезь  то же небо  то же кредо  больше бена  porchy демо дал  нам тут  похоже  пиздец  толще снег и тоньше стены  больше стрелок  ложь и декаданс  и так каждый божий день   больше денег  больше дел  больше дев  больше демонов    раньше было как  логика войны   мы  либо они  вот чужие  вот свои  фронты поделены  всё просто кристально ясно  всё  что было вчера   это вовсе детсад и ясли  щас   нет  ведь чем дальше  тем больше сраных подтекстов  на обе лопатки  в песочнице стало тесно  здесь так  в каждой шкатулке двойное дно  в каждом доме есть чёрный ход и в трюмо потайная дверца  власть   это шестерёнки под ринопластикой  пальцы на джойстике  и ты уже часть её  а дальше  как ты тонко ни тролль  там то ли наркокартель  то ли наркоконтроль  а девы наблюдают оком йоко оно  коконом локонов оплетая  как лаокоона  сотни головоломок  какой тут мордор  шир   кругом одни судоку-перевёртыши    и так каждый божий день   больше денег  больше дел  больше дев  больше демонов  но как из кожи ни лезь  тоже небо  тоже кредо  больше бена  porchy демо дал  нам тут  похоже  пиздец  толще снег и тоньше стены  больше стрелок  ложь и декаданс  и так каждый божий день   больше денег  больше дел  больше дев  больше демонов    что ты видишь за моей гримасой   за гримом  капюшоном  чёрно-белой маской   за псевдонимом  за мимом  за мемом   бэк-мс  не отличимого от серой массы   или кривой оскал из ниоткуда  в поиской сырья на тебя точащий 32 зуба  горестную лыбу бытия  ухмылку авгура  происки чеширского бродяги от уха  до уха  да  все это я  любознательный отпрыск  да  я персонаж  собирательный образ  да  и ты меня подсознательно создал  хватит с тебя  теперь я самостоятельный монстр  охра   редкая птица  будто бог ра  меткий  как традиции у вохра  и я   дитя первозданного хаоса  этот фаустоподобный мирок мне по фаллосу    и так каждый божий день   больше денег  больше дел  больше дев  больше демонов  но как из кожи ни лезь  тоже небо  тоже кредо  больше бена  porchy демо дал  нам тут  похоже  пиздец  толще снег и тоньше стены  больше стрелок  ложь и декаданс  и так каждый божий день   больше денег  больше дел  больше дев  больше демонов    герою   слава  вору   горы злата  консерваторам   однополый брак и голый завтрак  а мне б только знахарь  были врачи  но из спины что-то торчит  я то ли ангел  то ли бронтозавр  вы в тронных залах  но  как баронов и вассалов валтасара  вас пустят на голубое сало  всю кодлу за борт  я не европа с нато  но пятый год как я веду русский хип-хоп на запад  сказали    придержать удила   но я  в чём мать родила  умудряюсь разъебать кадиллак  зачитавшись в нём александром дюма  выйти  как ни в чём не бывало  и сказать   как дела    фомы пусть от неверия скулят  нахуй мне ваши лейблы  мы строим империю с нуля и на века  пока небо фенрира с поводка  не спустит  обрушив светила  как лепнину с потолка    </s>    <s>    люди проживают в поместье  прожигают наследство  доживают в подъезде  и люди проживают вместе  но это последовательность машинальных действий  люди так мечтают вначале  что пожирают очами и поджидают ночами  но люди пожинают печаль и  причал сжигают  отчалив  и пожимают плечами  люди что-то находят в словах  ходят к психотерапевтам  заводят собак  люди снова знакомятся  как  будто ни в чём не бывало они сходят с ума  люди верят в гороскопы с зодиаками  но  незаменимых нет – разъёмы одинаковы  а город утопает в зелени  кто-то выплывает  кто-то утопает – c est la vie    и мы никогда не знаем  что потеряли  впереди водоворот  я ранимый  не снимай с меня хитиновый покров  но кто же твоего лица теперь коснётся руками   впереди водоворот  не щади меня  сними с меня хитиновый покров    люди проливают на стойку  сидя в баре  настойку  выпивают  поскольку  здесь не всякий принимает стойко  то  что ему никто никогда не скажет   постой-ка   и  да  тут ценится гораздо  больше всё  что целостно и целесообразно  а если чем-то делятся – напрасно  все целиться горазды  вцепятся на раз-два  люди маются  выбирают пятьсот  разных одеяний  когда теряют лицо  люди умирают и всё  в доме есть хозяин – он выселяет жильцов  и люди всюду видят знаки и знамения  но сами не уверены  где страхи и сомнения  а облака по небу шастают  абсолютно немые и тотально безучастные    и мы никогда не знаем  что потеряли  впереди водоворот  я ранимый  не снимай с меня хитиновый покров  но кто же твоего лица теперь коснётся руками   впереди водоворот  не щади меня  сними с меня хитиновый покров  и мы никогда не знаем  что потеряли  впереди водоворот  я ранимый  не снимай с меня хитиновый покров  но кто же твоего лица теперь коснётся руками   впереди водоворот  не щади меня  сними с меня хитиновый покров    </s>    <s>    какие-то шуты на потешном столбе висят  тут нечего ловить  не задерживай беглый взгляд  ты лучше посмотри  как там реет победный стяг  весёлый кинофильм просто великолепно снят  тут лести нет  как пятна на белой стене  а если видишь бедность и гнев  то дело в тебе  тело в тепле  мы сильнее и целостнее  винить систему стало теперь уделом свиней  у таких прицел на спине  ну же  начни с себя  докажи  что ты не свинья  укажи  кто змея  накажи за себя  за других  община  семья  за тебя  земляк  отомсти  если мнишь мужчиной себя  вперёд  силёнки в кулак  верёвку берём  на приговорённых тряпьё  народ во всю глотку орёт  эшафот и мешок  неизбежной победы стяг  какие-то шуты на потешном столбе висят    горький дым  чей-то полигон  сколько от балды в этом ущелье полегло  нам никогда не будет места тут  помни  братан  горгород  горгород   дом  но капкан  ayo  motherfucker  bou ce  не думай о плохом  ты всё это впитал  как наркоту и с молоком  и ты вернёшься  даже если стал полным карман  горгород  горгород   дом  но капкан    на небе дым  под ним бетон  ты бы уплыл  да моветон  ты бы уплыл и далеко  мимо витрин  мимо икон  твой город   это быль  не фельетон  ты бы уплыл  но там не то  и ты  по ходу  по полной попал  горгород  горгород   дом  но капкан    нелепые лохи недовольны всегда и всем  по кухням непрерывно идёт череда бесед  очкарик очерняет нас всех под чай и десерт  а значит получает извне на чай и себе  пора про них рассказать  ибо рак проник  повсюду  отравляя родник  рукава в крови  врага боготворит и врата б отворил  им всякий  кто хает мир  подозрительно грамотно говорит  провокаторы  главари   будь начеку  почуяв недуг  они принесут беду к твоему очагу  бунт  войну и чуму  смуту  пули  мы так негодуем  но умников не линчуем  а почему   вперёд  силёнки в кулак  верёвку берём  на приговорённых тряпьё  народ во всю глотку орёт  эшафот и мешок   замолчит череда бесед  нелепые лохи недовольны всегда и всем    горький дым  чей-то полигон  сколько от балды в этом ущелье полегло  нам никогда не будет места тут  помни  братан  горгород  горгород   дом  но капкан  ayo  motherfucker  bou ce  не думай о плохом  ты всё это впитал  как наркоту и с молоком  и ты вернёшься  даже если стал полным карман  горгород  горгород   дом  но капкан    на небе дым  под ним бетон  ты бы уплыл  да моветон  ты бы уплыл и далеко  мимо витрин  мимо икон  твой город   это быль  не фельетон  ты бы уплыл  но там не то  и ты  по ходу  по полной попал  горгород  горгород   дом  но капкан  ayo  motherfucker  bou ce    во-первых  спасибо  что посидел с ником  серьёзно  второе   прочитала  полигон   всё круто  конечно  но тебе не кажется  что ты за два дня как-то нереально политизировался  я тебя не узнала  если честно  я  конечно  понимаю  что ты теперь делишь с алисой постель  но не обязательно же делить друзей и мировоззрение  тем более  что мы до сих пор не знаем  кто она такая  я начинаю сильно за тебя переживать    </s>    <s>    я свой диагноз знал заранее –  спонтанное самовозгорание  врач сказал   маниакальная депрессия  нахуй те валиум  не донимай меня  убейся     всё ещё бабла нет  всё ещё с долгами канитель  все ещё в подвале  всё ещё parliame t на бите  и я вернусь на трек  твой хуй как тулуз-лотрек  если русский рэп в гробу сто лет  то я ебу скелет  и я построил альбом на костях  этому не видно конца  как будто он голый толстяк  каждый просит фит  каждый пишет   денег дам   вас миллион  но мой кумир – гриша перельман  кеннинг таун  где  get dow   значит  хватай инвалида   тут не читают галимо  не почитают madlib а  хули мне пиздеть   тёмные аллеи  бунина  куни в десять лет – вот мой университет  не обрезан  предъявляю папарацци хер  моя мать – тереза  папа – ратцингер  последний в очередь к майк-чеку кровью харкал  зови меня солнечным зайчиком – донни дарко    ты не хастлер  браток  а подобострастный холоп  после нас хоть потоп  vgb   рэп поднялся с кортов  bitch  мой паспорт готов  после нас хоть потоп  ты не хастлер  браток  а подобострастный холоп  после нас хоть потоп  vgb   рэп поднялся с кортов  bitch  мой паспорт готов  после нас хоть потоп    бутиград  петербург  репербан  такой тур не видал даже тур хейердал  я тут  как гук – лимита  vagabu d  кеннинг таун  гаплогруппа j2  чики тут  чики там  знаю  девы приведут к проблемам  но твою ебу под феном и ибупрофеном  когда кончил  заплакал навзрыд  и  моё детство  как пустырь за корейской лавочкой  вот где собака зарыта  я не дигидитальный  мой стиль как винтики и детали  берегись и тикай  я дикий  как рикки-тикки-тави  а у тебя из польши кеды и поло  и ты  как хэзер миллс  – у тебя больше нету пола  ты призёр на фестивале по десятибалльной  системе в разделе  женский рэп в тьмутаракани   bitch  ты слухом покинут  какие  сука  плагины  звук из вагины    ты не хастлер  браток  а подобострастный холоп  после нас хоть потоп  рэп поднялся с кортов  bitch  мой паспорт готов  после нас хоть потоп  ты не хастлер  браток  а подобострастный холоп  после нас хоть потоп  рэп поднялся с кортов  bitch  мой паспорт готов  после нас хоть потоп    </s>    <s>    кем ты стал  где ты гнев потерял   ты был лев для телят  теперь это не для тебя  кем ты стал  тут на деньги деляг  и зачем тебе лям  на деле ты бедный демьян  кем ты стал  для богемных стиляг  тебе внемлет земля  ты нем  те  кто медлят   темнят  кем ты стал  ты был всем для меня  но на еблю и яд легенды билет променял    я помню  как я узнал о тебе  ты звезда теперь  а тогда никто не знал  хоть убей  ведь ты рано  как далай-лама тибет  оставил наш край  а теперь был готов летать  как стрела на тетиве  я с тринадцати лет знал  ты мой старший брат  хоть и мне не родня и даже не седьмая вода на киселе  я вживался в каждый твой текст  меня поражал интеллект  тебя ждал успех в жанре   знай  я желал тебе сделать всех  всех тогдашних коллег  писак  однажды навек  променявших свой шанс  талант  как торгаш отдавши за хлеб  а ты клал  что дальше некуда  хер  не жаждал монет  жил так же  как все мы средь многоэтажных фавел  а теперь кураж исчез  эпатаж   немалый гешефт  буржуа для этажерок тираж скупают уже  хрустальных фужеров звон там  где раньше гнали взашей  или званый фуршет  омары  буше  в карманах бюджет  скажи мне  эй  как же так  друг  ты же дважды неправ  ты как же сам не подумал  как ты в ступор вгоняешь меня   подай нам знак или звук  что не потух  не сторчался  не сдал  осознал ли сам ты  что за рупор сжимаешь в руках     кем ты стал  где ты гнев потерял   ты был лев для телят  теперь это не для тебя  кем ты стал  тут на деньги деляг  и зачем тебе лям  на деле ты бедный демьян  кем ты стал  для богемных стиляг  тебе внемлет земля  ты нем  те  кто медлят   темнят  кем ты стал  ты был всем для меня  но на еблю и яд легенды билет променял    глаза у нимф пусты  все бухают  эго набухает  как лимфоузлы  вот за этим ты шёл к олимпу  стыд  но через тебя бог не вызволит нас из лап египтян  подпалив кусты  там ты был  ух  смелым и вслух высмеивал культ сделок  хруст денег  плюс как система лузгает дух с телом  луч света  был из тех  за кем все идут следом  но едва вернулся  как тут же сузил свой круг тем до  тус и девок  сюжетов устье опустело  ты литературной фигурой собственный пуп сделал  пусть  но если вокруг взят курс на войну с изменой  то не проповедовать бунт   хули  кощунственно  эй  почему пока тут растут стены  мрут зеки  судят за пару карикатур с мэром  врут слепо  для тебя табу сделать вдруг слепок  с общества  ты трус или просто сдулся нехуйственно   эй  кем ты стал  склеив себе пьедестал  из фэнов  что были преданы делу    ты предал свой стайл  хрена-с с два  ты теперь не звезда  раз не самиздат  но кем ты стал  раз моя вера в ненависть переросла     кем ты стал  где ты гнев потерял   ты был лев для телят  теперь это не для тебя  кем ты стал  тут на деньги деляг  и зачем тебе лям  на деле ты бедный демьян  кем ты стал  для богемных стиляг  тебе внемлет земля  ты нем  те  кто медлят   темнят  кем ты стал  ты был всем для меня  но на еблю и яд легенды билет променял    привет  это снова я  ну  что    я так поняла  этот всё-таки до тебя дозвонился  сочувствую    м-м-м  чё я звоню  да  сегодня очередная попойка у фон глиена по поводу переизбрания нашего  горячо любимого  мэра  я твоё отношение к этому знаю  но сходить стоит  только полегче с гором  а то сам знаешь       </s>    <s>    здравствуйте  дети  сегодня мы изучаем британский английский  liste   i  ll give you three gra d  ow  always o  a hype ti g   a d call me from  ext ma   s chick  bu  that  ma   you ca  go  ma   fuck you   hey  blud  i bet you bell me back  yeah  safe  do i k ow you  cuz   we  re havi g a little chat  you get me   what  you thi k you  re a bad ma    sta dard  blud  you get it  i told you   bout that girl  ma   she  s a sket    говоря  на нашем русском кокни  без словаря  ты как этруски сдохнешь    твой район   это e dz или bits  слово sket   для панельных девиц  и поверьте мне  food   это не только еда  её в tesco и sai tsbury негде купить  как и lemo &cheese  хозяин   la dlord  если ты взял апартамент в ренту  не верь ему  знай  ты не fam и не mate ему  далеко не все ma dem   bredri    брря    тебя ночью ждут здесь  под капюшоном корча screwface  в общем  тут есть  те  кто нахальнее  те  кто на хайпе и  в клочья рвут весь lo do   ты  наверное  badma   говоря вместо привета  wagwa    но ты wastema   косишь под местных  сам думая   мне бы поскорей обратно в брайтон и плимут   вряд ли англия примет  если не знаешь  что quid   это фунт  а косарь   это gra d  ствол   это tool  нож   это sha k  не правда ли  i  it   off ur face или wasted   синий  в говно  если tipsy   бухой слегка  проездной   это oyster card  хз  почему так повелось  it  s some  ext ti g  в кокни к чёрту падеж  телка у нас   jaezzy  bird или gash  она может быть buff или butterz   пиздатой  на пафосе  страшной и все  что промеж  oi   уличный   gully и gutter  знай  ты попался  эстет  затирая про dub- и twostep  но не зная  что панельный блок   это cou cil estate   utter    utter   utter      накосячил  тебе скажут   fix up    если   ice o e   то все заебись  так  дисс   это par  респект   это big up  и safe  а для zoot  a тут красная rizzla  и для каждого дабстеп-туриста   ты на острове  cuz  берегись тут  пусть у вас ты опасный и гангстер  у нас тут на брикстоне с пластоу ты   хипстер    говоря  на нашем русском кокни  без словаря  ты как этруски сдохнешь    </s>    <s>    o e   </s>    <s>    сомнения слетаются с утра  моя бритая башка   это их лысая гора  сколько ещё лет будет мясо и рубилово  и сколько ещё куплетов про себя любимого   я здоров  как бык  но смертельно болен  ведь от священных коров отвык  иалдабаоф  он же иегова  он же князь мира сего суёт перо в кадык  за окном под видом города гниёт нарыв  я режу вас турбиною во снах  не реже  чем под аль-джазиру  мастурбирую на снафф  желаю смерти паре редколлегий  паре рэп-коллег  и  я не дейл карнеги  мне всё это на руку  как брейгель  мне снится то глава  ампира „в“   то в эфире тв живая голова эльвиры т  у меня кризис жанра  не индиана  но всё из последних сил  как прыжок в дилижанс из дирижабля  что  слышь  ногами на коврик  якобы  ты говорил  что я сдулся   но  видимо  я ипохондрик  если держу свою руку на пульсе  и в этом году я смогу либо переродиться  либо загнусь от конвульсий  ведь нахуй жить  если новой не сложить песни  и к чему вычурная полисиллабика  коль из нового ничего  летит моль из динамиков   мой второй альбом будет эталон  либо в этанол с головой  ты говно  либо гений   среднего не дано    твоя мигрень   балаган  тут больше четверти века набекрень голова  мы идём на глубине путями тёмными  под веществами ещё не изобретёнными  твоя мигрень   балаган  тут больше четверти века набекрень голова  мы идём на глубине путями тёмными  под веществами ещё не изобретёнными    i m fucki g o  o e   your   i fly above you ducks like aerial pursuit  oh   i left the dark side hurdli g the fe ce  yeah   by  ow you might have heard of me  i murder with i te t  every word is blurted  ow assertive a d i te se  go     i m cutti g through this track like a hacksaw  ragh   i m a fucki g rottweiler  you re a lap dog   ow you ca  tell  em who the best is  me   shit  who would ve guessed it that i could spit asbestos   old school  like playi g a game of grabber hockey  i m hard to u dersta d like blabberi g with a jabberwocky  um   try to fight it  but i guara tee you ll die to ight  yeah   k ighted  i am dy amite  mighty mouse behi d the mike  grapple you a d smash your adam s apple with a s apple bottle  squash you like a  apricot  sir la celot is back a d battle  catch a felo   i m a  elderly dege erate  se time ts are ve omous that melt you o ce i pe etrate  kill you the  i ll cook you the  i ll put you o  my di  er plate  yeah   poppi g vicodi  u til the psycho hyperve tilates  getti g wild boy teari g off the roof  to battle me is death  get buried i  a suit    твоя мигрень   балаган  тут больше четверти века набекрень голова  мы идём на глубине путями тёмными  под веществами ещё не изобретёнными    i m fucki g o  o e   your   i fly above you ducks like aerial pursuit  oh   i left the dark side hurdli g the fe ce  yeah   by  ow you might have heard of me  i murder with i te t  every word is blurted  ow assertive a d i te se    </s>    <s>    я помню в детстве миг  как девственник  заняв неизвестный ник  из протеста вник  в русрэп  сказав   я буду здесь вместо них   и полез в архив  бездна книг  я верил   ликбез велик  но  проведя десять лет в пыли  понял  что себя искать бесполезно в них  и я вышвырнул том  вышел из дома  взяв лишь пассатижи и клей с собой  двинул в отель savoy  в пизженном volvo по городу  пей со мной  сука паршивая  пей со мной   живо в номер и дверь закрой   потом клей в замок  если хочешь  теперь завой  фейсом в пол   я такое с твоей сестрой сотворю  что  по сравнению  джейсон   лох   мне снится с кровью tetra pak  я кончу плохо   это факт  готовьте гроб и катафалк  средневековый карнавал  над эшафотом стаи ворон  от славы пророка  полшага до врага народа   савонарола  но мой рэп   как самооборона  вас много  вы   стадо баранов  стандартное  как фоторобот   а нас мало  мы трое бродяг и не знаем дорогу  от восточного мордора до изумрудного города  прочь из бетонных коробок  где мы в роли дойных коров   замурованы  в стойлах и ждём кислорода  мой альбом по драйву всех макнёт в дерьмо морально  кеды  поло sto e isla d  vgb живет за гранью  и пусть мирона райдер не купит ему новый maybach  но  я за своё и против чужого   винона райдер    и я не дорожу вашим мнением  кина не будет  ведь я в вашем рэпе   жук в муравейнике  это  ёб твою мать  среди вашей дружной полемики  я дышу с колыбели тем  что я жук в муравейнике   муравей  беги   да  я жук  и разворошил им разом весь муравейник  хоть здесь не америка  ванька   мой dre   я   маршалл  как жуков и ворошилов   как энтомолог  провожу параллели  к тому  что каждый из нас в вашем доме   жук в муравейнике  муравей  беги     мне мало места  я сбежал  покинув дом свой и люксус  и если в deutschla d  е жизнь   мёд  в дождливом лондоне   уксус  но я привык делать  что я хочу  мне по хую суд ваш   не мой характер смотреть в пол  печально охать и слушать  я вагабунд  блядь  и мне дождь в самый раз   в кармане пусто и простроченный бас  злость во мне  значит  в тебе мой нож   я беден  ты кинут  грехи   мой смысл в жизни  что ж  ложите крест мне на спину   и если кто-то   хайп и имидж  то я   псих-похуист  моя мораль вам непонятна  как воры полицистам  у жизни панчи шлют в нокаут  это не рэп  нигга  пару раз упал и больше на ноги не встанешь ты  как рэм дигга  а у нас троих  как прежде  то густо  то пусто  мои карманы   огород  и в них то хрен  то капуста  но если ты с голода полез в мой дом  то смотри   я не жадный  я кормлю своих врагов до смерти     и я не дорожу вашим мнением  кина не будет  ведь я в вашем рэпе   жук в муравейнике  это  ёб твою мать  среди вашей дружной полемики  я дышу с колыбели тем  что я жук в муравейнике   муравей  беги   да  я жук  и разворошил им разом весь муравейник  хоть здесь не америка  ванька   мой dre   я   маршалл  как жуков и ворошилов   как энтомолог  провожу параллели  к тому  что каждый из нас в вашем доме   жук в муравейнике  муравей  беги   и я не дорожу вашим мнением  кина не будет  ведь я в вашем рэпе   жук в муравейнике  это  ёб твою мать  среди вашей дружной полемики  я дышу с колыбели тем  что я жук в муравейнике   муравей  беги   да  я жук  и разворошил им разом весь муравейник  хоть здесь не америка  ванька   мой dre   я   маршалл  как жуков и ворошилов   как энтомолог  провожу параллели  к тому  что каждый из нас в вашем доме   жук в муравейнике  муравей  беги     </s>    <s>    бр-р    раунд три  они мне мозги ебут  а   мол  барни вскрыть   дело нескольких минут  сколько  сколько    ты u2005автор u2005книг  окей  я u2005в них  хоть лень  нырну  хоть u2005лень  нырну   в реальном мире пройдет несколько минут  слышь  твой параллельный мир   мартышкин труд  труд   в эфире радио ностальжи на хип-хоп ру  на хип-хоп ру   ты плодовит  увы  я проник к тебе в литру  ч-ч-чё    барни  муравьед  бондарев олег  ты труп  олег  ты труп     ты писатель от бога  спроси любого  он будет сразу заколдован увлекательным слогом  кто б еще  кроме тебя  свой написал некролог  сходу закопав своим третьим раундом свой же шанс андердога  от души  беллетрист  сколько миров ни создашь  ты и сам лишь  pc   неигровой персонаж  герои карикатурны  сколько томов ни продашь  в глаза бросается халтурно прорисованный пейзаж  лох   рэпчик не твое  бывает  просто не дано  проза   постное говно  хоть строчишь простыни давно  вопрос ребром    каково жить  когда ты во всем посредственность   отец семьи из-за фэнтези выглядит как девственник  олег  смекни   все  прекращай  кринж пиздец  трипл экс  едва заслышав  верещат   рифмабес  трижды крест тебе в судействе по очкам   гришка лепс  лишний вес в игре  я съел на ней собак   гришковец  сеттинг   постапокалипсис  твой стайл   катастрофа  синий ебальник   аватар был наспех взят с фотостока  где профсоюз писак  это помощь неимущим  сдай макулатуру свою сам  устройся ночным грузчиком  слышь  ты обломал всем третий раунд  сдуру мне  думалось  попал муравей   был бы хотя б уровень  но ты не добрый маг  копрофаг  компромат говном пропах  много так про говно  ты прав    каков мой враг  таков мой раунд  суровый факт  твой клип называется  развод   в нем ты горько причитаешь  собираясь на завод  мол  спит с любовником жена   ауч  ты с ним бухаешь алкоголь  предрассудки патриархата  всё нормально    ты альфач  а не куколд  я не хотел о личном  скандалы   баг  а не фича  но ты же сам это слил  твой трек автобиографичен  плюс  уточню  для тех  кто не стал вникать в твои вирши  твоей нынешней жене   респект  я здесь о бывшей  тем любовником был я  звал прогуляться до дома  она била фонтаном  словно она пиаццо навона  знаешь    я проник в твой мир  будто тебя помацал проктолог  когда пришвартовался к ней  как у палаццо гондола  гондольер   я  ты все тянешь в пасть   лангольер  хватит жрать  олег  ты уже с виду шар   монгольфьер  на книжной ярмарке ты б  как швейцар  ждал в фойе  о-о-о   веду муравьеда на поводке будто браконьер  бич  я купил твой трэш   серия  кремль 2222   роман  тушино   потушен едва был лишь мой пердак  дабы передать для слушателя весь ужас тебя  я погружусь в роман на пару минут и шустро назад  летс гоу      идет пепельный дождь  где я  зачем на мне доспехи  где зонт   по сравнению с тем  как быстро время течет здесь  оно во внешнем мире   густой кремовый торт  шаг вперёд     в небе светила не сияют  вместо них впотьмах сюжетные дыры зияют  от косноязычия описаний ландшафта на всем остался радиоактивный осадок  я ползу по-пластунски  избегая скользкие отростки  в поисках хоть тусклой смысловой нагрузки  фух-фух   под сюжетной аркой пусто  саспенс не ходит вверх-вниз  как будто хвост у трясогузки  мутант-трясогузка  да  эту хуету я рифмы ради выдумал  ибо тут названия не лучше  плюс чем оправдать ту же  крысособаку   ей же не нужно даже рифмоваться  барни  это буллшит  жанр кибергот а-ля рюс  тонкий вкус  я пойму его  как только напьюсь  меч  микрочип  иконка  герои в речи на полном серьезе юзают  не кручинься  и  томко   бля   лубочный стиль  кто тут главный по пошлости   что с тобой  что это за сталкер в кокошнике   господи  я достал счетчик гейгера  он показал  что у тебя щечки геймера  от заброшки бегом до звезд кремля  сюжетная ветка приводит в метро глуховского  прямиком  автору снятся нобели с оскаром  но он пока едва обоссал надгробье тарковского   жестокий мир   это постап или пинап  неонуар  фоллаут  киберпанк или пикап   ведь вся любовная линия бо и нейроманта  настолько плоха  насколько трэшак остальной роман  существа зовутся нео  кио  био  надеюсь  автор накопил на киа рио  герой стреляет на уровне  пиу  пиу    писатель гордо вписывает фио в био  сын  тебе незнакомы прозы азы  выбор слов злит и вызывает рвотный позыв  вот те четкий посыл  даже в постельной сцене не используй оборот  дошел до вожделенной щели   сука  возвращаться пора к аппарату  скоро начнет закрываться портал  пока я вис уже целый месяц тут  вне чтива это было дело нескольких минут  аллес гут      я уже баттлил графомана  уже диссил олега  ты мой дауншифтинг дня  я   твоя миссия века  наблюдение  пока трек не отдали на сведение   в твоих мирах тебя б за шпек отдали на съедение  бой  не ценят в мордоре кривой сторителлинг  бондарев точно не феликс   ноль хитов  где бестселлер  пока тут  пишут такие  как ты  акунин в бегах  зови маркул меня  раз ты думал  ты акула пера    раунд три  они мне мозги ебут  а   мол  барни вскрыть   дело нескольких минут  сколько  сколько    ты автор книг  окей  я в них  хоть лень  нырну  хоть лень  нырну   в реальном мире прошло несколько минут  слышь   слышь  твой параллельный мир   мартышкин труд  мир  труд   в эфире радио ностальжи на хип-хоп ру  на хип-хоп ру   ты плодовит  увы  я проник к тебе в литру  да  да   барни  муравьед  бондарев олег  ты труп    rest i  peace  э    </s>    <s>    это ноль-ноль-восемь  помни памятную дату  йе   мы вас разносим  будто в цирке сахарную вату  без меня русский рэп загнётся  как усы дали  ага   я женюсь на твоей маме  чтоб тебя усыновить  боже мой  ты умрёшь от раны ножевой  а я сойду с эстрады вниз усталый  но живой  здесь всё разрешено  один бэттл xa0  ты решето  я режу то  что ты зовёшь рэпом и сплю с твоей женой  я несу свет в массы  разнесу всех вас  и детсад русрэпа  моя crew   спецназ  йе   ты нелепо бэттлишь  я уничтожаю фэйков непроизвольным движением левого мизинца  ваши лица слишком даже для картины босха  флоу малокалорийный  как диета новодворской  русский рэп   это горстка геев  фэйков и подростков  с задротской смесью флоу французского с нижегородским  пту  поэты с этой псевдоглубиною  сэмплеры длинною в 100 минут со всей голубизною  и сэмплы из одной и той же скрипки морриконе  с рифмами под антикона проглотил  как мантикора  йе   где mc те  что якобы рэпа надежда   будь что будет  но я был и буду синонимом fresh ess  ага   измени внешность или прояви нежность ко мне  но я прилечу к тебе и пну тебя в промежность  йе  ха-ха-ха-ха     лондон против всех  он против всех  иду по крови ногами  как роберт мугабе  йоу  лондон против всех  он против всех  я в рэпе иди амин  твой рэп xa0  идиотизм  лондон против всех  он против всех  иди дрочи на че  но я в рэпе пиночет  лондон против всех  он против всех  он против  он против всех  йе     твой рэп высокодуховный  словно далай лама  мне насрать  поскольку я гулаг  а ты варлам шаламов  просто знай  брат  ты ни то  ни сё  как нос тапира  я трахал твою мать  а ты смотрел и мастурбировал  это грязь и кровь  как операция в ираке  вы все на одно лицо  как мастурбация в буккаке  мы отрываем mc руки  к вашим воплям глухи  и сопли в духе  опле раз  достойны оплеухи  я люблю токо хитовый звук  а не кухонный  взять хоть  йе   r&b  а не моно на кухне  как в 80-е  но вы скалите зубы и хвалите друг друга  варитесь в одном соку  все перед guf ом лебезят  пидорасы   ты пьёшь литр пива  я   литр электролита  твоя любимая просит меня подписать ей клитор  нет проблем  я мигом  вы для рэпа вериги  как и всё  что выходило на rap recordz  кроме лиги  ты против меня   капустница против мангуста  узница против прокруста  устрица против лангуста  если в зале пусто  значит на сцене русрэп  но лишь я в зале в углу встал  тотчас на сцене пусто  ведь я   рэп-наследник  mc первый и последний  чтобы изучить мой флоу нужно 20 лет исследований  у вас есть шанс не кануть бесследно  пишите диссы на меня  о вас узнают посмертно  йе     лондон против всех  он против всех  ты хочешь флоу как мой  лучше свой флоу отмой  лондон против всех  он против всех  но времени в обрез  ведь у темени обрез  лондон против всех  он против всех  что вы не геи  надеюсь  но вас наденут на пенис  лондон против всех  он против всех  он против  он против  он против всех    </s>    <s>    зачем я тебе звоню  я же знаю  что тебя нет дома  я выхожу на улицу   все только и говорят о том  что ты замешан в заговоре против мэра  я обзвонила всех   никто не знает где ты  прибежала к тебе   тут дверь открыта  я хотя бы рукопись забрала  пожалуйста  перезвони сразу же  как только ты услышишь это сообщение    ну что сказать  я вижу кто-то наступил на грабли  ты разочаровал меня  ты был натравлен  гора как на ладони под нами  смог с дымом над ней  может  вина  у меня вполне сносный виноградник  ты  главное  так не шугайся  я   не изверг  я даже листал твой фолиант  что издан    браво  бис  ты дальше б сочинял  смешил  писал  природу  пасторали  ладно  не серчай  мои орлы чуток перестарались  не надо тебя было им волочить за рубаху до виселицы  но что ж тебе  гаду  неймётся никак  отдыхай  веселись и не ссы  а такие  как ты  всегда видят лишь негатив  положив на весы  но я переживаю за всех горожан  словно каждый из них мой единственный сын  я знаю  враги утверждают  что  якобы  многих боюсь  что  якобы  я целый край оплетаю  как головоногий моллюск  допустим  всё так  но что будет  если уйду   города по соседству  убрав своих деспотов  бедствуя  мрут  для сравнения тут    горный воздух  спорт и здоровье  курорт  игорный дом  двор торговый  фуд-корты  добро пожаловать в горгород  эталон комфортного отдыха  гольф  аквадром и кёрлинг  добро пожаловать в горгород  мировой гандбольный рекорд  ипподром и соборы  боулинг  добро пожаловать в горгород  мой народ не хочет реформы  когда повторно накормлен  добро пожаловать в горгород    ты собрал только половину пазла  картина маслом  социальный лифт в пирамиде маслоу  толпа многоголова  как гидра и цербер  но она не делает погоду  как гидрометцентр  она  хоть я не макиавелли никколо  благоговеет влекомо к плахе  петле или колу  страху  елею  иконам  хаки  игле и оковам  ты умнее намного  нафиг плебеи такому  а   тебе промыл мозги идиот под горой   всё переплетено    который год анекдот с бородой  конспиролог-изгой   мы с гуру знакомы с тех пор  как этот горе-воин был чиновник  чьё слово   закон  допустим  враги утверждают  что  якобы  многих боюсь  допустим  я мир оплетаю  как будто я головоногий моллюск  допустим  всё так  но что будет  если уйду   города по соседству  убрав своих деспотов  бедствуя  мрут  для сравнения тут    горный воздух  спорт и здоровье  курорт  игорный дом  двор торговый  фуд-корты  добро пожаловать в горгород  эталон комфортного отдыха  гольф  аквадром и кёрлинг  добро пожаловать в горгород  мировой гандбольный рекорд  ипподром и соборы  боулинг  добро пожаловать в горгород  мой народ не хочет реформы  когда повторно накормлен  добро пожаловать в горгород    пойми  писатель  ты хороший парень  но с плохой компанией связался  не нарочно твари  бросили тебя  едва запахло гарью  глянь  на горизонте расцветает    у нас даже солнце под ногами  а   учти  тебе на сей раз повезло  карьера  здоровье  свобода   всё цело  но вот те слово мэра    второго шанса нет  короче  берегись  если снова дотронешься до моей дочери алисы    </s>    <s>    устал и истощился вечер  ты спишь  беспечен  вычислив количество овечек  город   как пирамидка из колечек  каждый человечек в нём наполовинку искалечен  молчит диспетчер  пуст автоответчик  и не стоит свеч игра в  любит   не любит   в  чёт и нечет   одни долечиваются  либо они далече  у других девиз   дивиться нечему  делиться нечем   пока родители кутят и тратят  знай  тебя укладывает спать популярный писатель  но тебе на эти статусы плевать  ведь  пока тебя впечатляет найденный оловянный солдатик  где-то лунатик крутит радио  оттуда голос мэра призывает взять и покарать их  кого конкретно   без понятия  в городе казни  власть и плутократия переплетаются в объятиях    утомлённые днём  мы поём колыбельные для тёмных времён  что ещё остаётся нам   смысл бороться  сила тьмы восстаёт со дна  спи спокойно  район  мы поём колыбельные для тёмных времён  чем ещё заниматься тут   сопротивляться глупо  мрак   водолаз да спрут  утомлённые днём  мы поём колыбельные для тёмных времён  что ещё остаётся нам   смысл бороться  сила тьмы восстаёт со дна    чернила накрывают крыши  неслышно проникая в швы  и заполняют ниши  чем ближе кипиш с патрулями слышен  тем ты хуже спишь  во сне бежишь куда-то  чаще дышишь  за окнами холодный макрокосмос  масса мокрых спин  промозглый дым  встаёт  гора наростом  под ней руда  камней обвал  сверху кварталов гроздья  банды  наркота  жандармы варварски винтят подростков  ты ещё мал и не подозреваешь  как подозреваемых снимают сотни скрытых камер  я   не пассионарий  чтобы в каземате прозябать  но то  что назревает  называется  концлагерь   сгущаю краски  завтра новый бой за  бабки  территорию  контроль  и каждый в роли войска  вокруг тебя недобрый мир  его террор и боль вся  с головой укройся  крепко спи и ничего не бойся    утомлённые днём  мы поём колыбельные для тёмных времён  что ещё остаётся нам   смысл бороться  сила тьмы восстаёт со дна  спи спокойно  район  мы поём колыбельные для тёмных времён  чем ещё заниматься тут   сопротивляться глупо  мрак   водолаз да спрут    </s>    <s>    o e   </s>    <s>    думаете  его эго не гниёт изнутри   моя матрица   рэп    все девы крутят динамо  ждут идеала  я не такой  но и твоя подруга нюх потеряла  услышав меня  ведь я так плотно стелю  будто ко сну ей подоткнул одеяло  я будто бы наверху фудзиямы  а она внизу и полна грусти и трепета  я называю её hip-hop ru   на ней перебывали уже все русские рэперы  и что мне делать с тем  что не помню  где сейчас мой дом  если он международный   чем раньше жил  хотя и вам о таком не  говорил  чем быт был годами наполнен   моя голова  как каменоломня  оправдаю ли долгами в этом храме торговлю   ещё пару лет  и будет столько спален и комнат  дома каменный пол  наступай ногами на коврик  эй  слышь  ногами на коврик  годами накопленный  сваленный в погреб  антиквариат  я с деньгами помолвлен  моногамен  но менять не успеваю любовниц  но это в будущем  и я не ебу  зачем я себя заранее считаю виновным  ведь даже если я и обнищаю духовно  мне помогут пиздюлями ga z  дамани и gobli     mic check o e  two  let me start this   o va  gogh  but i m a  artist  i m a demo  straight a little somethi g  i ll be cutti g a d shutti g dow  a ybody that i feel is tryi g to ru  this  real talk here  o assumptio s  what i do here mo ey ca  t buy a d  talk is cheap  so mo ey ca ’t lie  if you k ow somebody your buddy ca  try  let me drop by sayi g that i do ’t care   coz whe  i spit it i kill it you k ow   it’s so clear  how could i be so far yet so  ear   coz dem a stamp o  your life if you show fear  i ma drop a little flow to show that i ma ru  the show  a d i ma keep it goi g see  i m  ever slowi g my roll  a d with the pedal i  ma ha d i ma row the boat  a d if you wa t me to fail better k ow that i wo ’t  i’m o  fire   fe ix  fuck with your sti ki g attitude   clea  it  let it rip skip like a child with a rope  if you really thi k i really give a shit i do ’t  i keep my shit tight  i k ow  ha d me the beat   i flow  ca ’t call this a race ca ’t keep with my pase  fuck the gree  flag i go    mc’s are like calories bur  them like i’m ru  i g o  a treadmill  i will  ever sell out eve  whe  i get the best deal  it’s a car crash whe  you liste  to me spitti g  because i’m over the limit before i start writi g lyrics  a d i k ow i  ever write  so whe  it’s time  i pick up a bottle a d ha d my soul to the spirits  my tracks are viruses  so whe  you play them o  your hard drive we leave gaps i  the system  cheetah o  a missio  a d they are sayi g  i’m slow  because they made of gold  still i do ’t give a fuck for those  i’m a gargoyle a d i got tur ed to sto e by medusa  before i was a producer  i was a myth i  the shadows a d they hated my humor   ow i’m ready to blow out the ca dles make a wish  i just wa  a blaze piff like harold a d kumar  i’m a killer whe  i e ter the rhythm i’m at the top  there is  o other divisio   o other shot  wa  a lear   better liste   whe  i drop bombs head for the bu kers  a d do  t try to stop me whe  i’m aimi g for your boss  life is a bitch these days   i k ow  so i fuck her like a slut i  a private show  dark beats from be eath  whe  i step up o  the sce e  give everybody seasick ess from the flow  bleah    </s>    <s>    все детство я был где-то там внизу  на концертах срать на звук - я знал все тексты наизусть  но сам к такому заранее не готовился -  безымянный подросток с окраины мегаполиса  как происходит так  что всего лишь за эти годы  дети хором подсели  и множатся эпигоны   а мне неловко  будто в первый раз на лёд встал  оказывается  я делал  мамоёбство      у родителей паника  лбы покрыты испариной   дети следуют за мной  как за флейтистом из гаммельна  запрети меня  сдай меня и называй меня неандертальцем  а коль обо мне нельзя заикаться  показывай пальцем   вон он   я стал звездой  ты по полной бы одурел  видя  как я живу с женой в однокомнатной конуре  я новый питер пэн  и тебя злоба берёт  полный вперёд - это второй детский крестовый поход   йе     хочешь кайф  острый  как баттерфляй   а   налетай  покупай  потребляй  чё    и дети ждут моих песен  как первый снег  йе   зовите меня йети  я - снежный человек  оу   хочешь кайф  острый  как баттерфляй   налетай  покупай  потребляй  и дети ждут моих песен  как первый снег  зовите меня йети  снежный человек    ты знаешь  что всё так и происходит  да  метафора стара  но рэп - аудио-наркотик  это не прежняя школа - зелье сладкого дыма  мой рэп - снежная дорога в зеркале заднего вида  хиппари не любят меня  зная  что я мизантроп  а попсари за то  что распадаются  как изотоп  певцы собираются  рыдают - новый дисс готов  но мне стыдно за полк  если он бросается вниз в окоп  стоять   я знаю  наверное  хватит бэттлиться  но эти красны девицы ветреные  как мельницы  я только за  вы меня  как пресса  попиарьте  но в финале меня ждет мой профессор мориарти  у дисса должен быть повод  я предлагаю мир   да   возьмемся за руки  друзья  и за высоковольтный провод  вам никуда от пения ни деться   мои треки для детей  этот куплет как избиение младенцев       хочешь кайф  острый  как баттерфляй   налетай  покупай  по-потребляй  и дети ждут моих песен  как первый снег  зовите меня йети  я - снежный человек  хочешь кайф  острый  как баттерфляй   налетай  покупай  потребляй  и дети ждут моих песен  как первый снег  зовите меня йети  я - снежный человек  а     </s>    <s>    е  ага… не  стоп  стоп  убери бит  короче  и верни потом его  xa0е     мне говорят остатки знакомых    загадка природы  зачем от грин-парка до xa0дома  если есть в рамке диплом  через давку в вагонах  бежать на посадку в автобус   жить без бабок  нехватка которых  поставит тебя в двадцать пять за прилавком  макдональдс  к палаткам торговым  ради чего  ради жалких фантомов того  что ты отобьешь бабки альбомом     хм  мне эти нападки знакомы  я к схваткам готов  будто бы повивальные бабки с роддомом  что  мне  вкалывать в банке как робот   снова засовывать в частные школы арбатских  мажоров  кусок пирога просто так не дарован  а ворован украдкою в барских хоромах  тик-так  килотонны взрывчатки готовы  фингал  гематома  нихуя  стиль  как яд белладонна  с летальным исходом  ты падкий на xa0промо  поставь трижды крест на себе  как фанатки мирона  и пусть я гадкий утенок  и рэп для детей  будто шапки с помпоном  но я в роли тёмной лошадки подкован  это не как кинопробы   ты правда в крови  как прокладки  тампоны  все тявкают хором в сети  но увидев  мне тянут ладони  мол   братка  здорова    не-а  вон с шаткого трона   ты нужен мне  как зажигалка дракону  я вас породил ненароком  прости  ради бога  видать  неполадки с xa0гондоном    ты жертва  даже если в самом деле не xa0виновен  тебе докажут это на всем теле переломы  ты денег нажил и был в хастла переименован  но не продажами   семья шлет деньги переводом    oi  мой менталитет   лондон против всех  oi  oi  oi  мой менталитет   лондон против всех  oi  oi  мой менталитет   лондон против всех  мой менталитет  мой менталитет    нечто иное   вечный феномен  вечно готовый задеть за живое  как ели живой   значит  вечнозеленый  под шестиконечной звездою  речь о мироне  что вечно с пеленок не xa0понят  будто на литовском словечки симоны  да  он вечно виновен  будто бы над темечком венчик терновый  xa0а   как по встречной зимою  через населенный пункт  двести спидометр  нечего спорить   тут рэпа синоним  плюс нечисти бесчеловечней чернобыля  больше  чем у него  нету ни члена  ни xa0шнобеля   он сиречь квазимодо  стеречь казино бы ему с таким фэйсом  а не девкам течь в xa0пищеводы  du  o   я полежал на печке немного с xa0тех пор  как решением старейшин синода святейшего  мне суждено было лечь на дно  ведь я не мог нежно петь  как шинода  пусть  всё равно без меня пусто  будто в вещмешке нищеброда  и нет ничего  но я детище лондона  ты хотел сделать меня решетом   осечка  хуёво…    ты жертва  даже если в самом деле не xa0виновен  тебе докажут это на всем теле переломы  ты денег нажил и был в хастла переименован  но не продажами   семья шлет деньги переводом    oi  мой менталитет   лондон против всех  oi  oi  oi  мой менталитет   лондон против всех  oi  oi  мой менталитет   лондон против всех  мой менталитет  мой менталитет  oi  мой менталитет   лондон против всех  oi  ooi  oi   oi  oi   o-o-o-oi  oi  мой менталитет   лондон против всех  мой менталитет  мой менталитет    </s>    <s>    прошло три года и ничего не изменилось     в лондоне по-прежнему дожди  oxxxymiro     всё ещё покидаю дом в семь часов утра  в одном кармане диплом  а в другом дыра  я сижу  курю на детской площадке  я в детстве о счастье мечтал   надежды так шатки  синоптик   паразит  вместо солнца дождик моросит  люди говорят мне   окси  подрасти    но я бы скорее ногти отрастил  и с газпром-сити сиганул бы в костюме hello kitty  бог видит  что так пытался подстраиваться  быть как все  совсем как однокурсник  друг или сосед  но во мне что-то не так   мне 26  но я развиваюсь с трудом  будто заштопанный флаг  меня растили мать с бабушкой в парнике  а судьба кидала  будто камушки по реке  и оставляя за собой  как они  эти круги ада  говорил себе   вперёд  тебе больше других надо     и я продолжаю жить в говне  ты хоть бы день сумел прожить на дне   так что не втирай мне про духовные ценности  лучше подскажи дорогу из бедности  но не такую как у всех вокруг  мне  срать на мнение друзей  подруг  yeah  и ты ещё спроси зачем я бэттлю    тут уж либо бэттл-рэп  либо на шею петлю    я прихожу домой в девять вечера  тире дождь зпт ветер тчк  мне здесь не место  как немецкой овчарке на детской площадке  моим пальцам не согреться в перчатках  тысячу раз прожить мне не хватит  чтоб понять  как надо  верил в бога  но он вышел покурить  йоу  а деньги манят  будто женские коленки  я  как пленный инопланетянин  не в своей тарелке  можете говорить  что я позёр  что я позор для рэпа  что нужен шире диапазон и глубина во всём  окей  я грубиян  и что  пойми  что я лишь тот  кто должен за тебя сказать всё то  что для тебя грешно  ведь я пережил в эмиграции из россии на запад  больше дерьма  чем в канализации ассенизатор  и даже если мои треки ни о чём  я жму на  record   майк включён   помолясь  начнём    и я продолжаю жить в говне  ты хоть бы день сумел прожить на дне   так что не втирай мне про духовные ценности  лучше подскажи дорогу из бедности  но не такую как у всех вокруг  мне  срать на мнение друзей  подруг  yeah  и ты ещё спроси зачем я бэттлю    тут уж либо бэттл-рэп  либо на шею петлю    и пусть порой слова не те  что вам нравятся  но это мой менталитет  я поднимаюсь сам и помаленьку    это всё ещё сага об орлах и канарейках  у тебя только грусть и сопли  а я вырос так  как вырос   да  я русский кокни  и пока каждый третий эмси продаёт зад  дети приходят и уходят  йети остаётся  ха     </s>    <s>    наша фортуна перед зеркальцем вертится  примерив часовой пояс верности смертницы  помнишь  никто не верил  что мы вместе и женаты  но нам карты спутал неумелый престидижитатор  знаешь  мне снилось  будто мы снова спали вдвоём  спальный мешок  спальный вагон или спальный район  соря в спальне бельём и кутаясь спальным бельём  я столько раз сказал  спальня   что это двуспальный альбом  мирон   это не псевдоним  опять ночник  над моей головой круглый неон   это мой псевдонимб  ты мой серотонин  а не амброзия с нектаром  я сказал железно   всё     но ты коррозия металла     я всё сижу и смотрю из окна  как гаснет небо  хаслят малолетки с барсетками на велосипедах  виноват или нет  я   неважно  мы с тобой сироты  которым провели в интернат интернет    я знаю   это крокодиловы слёзы  я знаю   всё необратимо  но всё же  я знаю   нет альтернативы  и всё же  скажи   зачем у крокодила есть слёзы   я знаю   это крокодиловы слёзы  я знаю   всё необратимо  но всё же  я знаю   нет альтернативы  и всё же  скажи   зачем у крокодила есть слёзы     я ночевал по сквотам  кочевал автостопом  сворачивал с дорог и поворачивал на тропы  и пока мои треки вовсю до невы гоняли  увольнялся отовсюду  откуда не выгоняли     и может это не комильфо или моветон  и кто-то экономил на дом  а я на микро  скажут   какой хуёвый флоу    но это в горле ком  на полке кроули  открытка с котом и некрономикон  другие пишут альбомы быстро  я этот выстрадал  вырвал  вызвал и выдавил  выдрал  высрал и выблевал  выжрал  вынюхал  выкурил   не говори  что на  вы  стали мы с тобой   я похуистом был  но я не пиздобол  напротив гастроном  тут всё ровно в основном  хоть островной костолом носит телескоп  как астроном     и каждый вес мутит  а я исчез в студии  тренировочные штаны  ирокез  худи    я знаю   это крокодиловы слёзы  я знаю   всё необратимо  но всё же  я знаю   нет альтернативы  и всё же  скажи   зачем у крокодила есть слёзы   я знаю   это крокодиловы слёзы  я знаю   всё необратимо  но всё же  я знаю   нет альтернативы  и всё же  скажи   зачем у крокодила есть слёзы     </s>    <s>    незаметно поправь её одеяло  за это себя предавая анафеме  она вышла из пены  худой отпечаток плеча оставляя на кафеле  и хана тебе  доигрался ты  старый дурак    вот и вся эпитафия  на город падает тьма   засыпает шпана  просыпается мафия  там  под нами копошась  муравейник разевает пасть  как ротвейлер  и учит выживать параллельно  к тому  как в это время тех  кто наверху  учат решать уравнения  и пока город вертикально поделен  она хватает машинально в постели  даже не просыпаясь  мой член  а я улыбаюсь ей и понимаю  что фатально потерян  а лучше рассказать постепенно  всё это началось с того звонка в понедельник    марк  привет  это кира  марк  ты вообще хоть что-то написал  тебе рукопись через месяц сдавать  тут ещё какой-то повёрнутый фанат названивал с упрёками  я даже не знаю  как он нашёл мой телефон  но  видимо  он может и до тебя дозвониться    </s>    <s>    сколько было всего  сколько вынесено  ultima thule  сколько было всего  ultima thule  но я путь к ней найду ли  к своей неуловимой ultima thule   ultima thule    погода за окном ништяк  в восторге маринист  земля   это плоский диск под колпаком дождя  по комнатам волны шумят и мой смоют ночлег  но мне наперёд щас ясно  не заберёт нас ваш ноев ковчег  check  на этих атоллах ветер и холод  на них давно забил синоптик  запил метеоролог  повсюду море  я вряд ли перейду его вброд  но я беру собственную слабость и ебу её в рот  ведь чайки мяукают в небе  как будто бы души кошек  без суши тяжело  но мы жить в этой луже сможем  это как голливудский фильм про водный мир  который снял очередной занудный мужеложец  тут сухопутный опыт непригоден  что-то многовато круговорота воды в природе  а город пришвартован  плавает на длинном тросе  в открытом море кракен хавает авианосец  но мне ещё дальше  туда вот  к эпицентру бури  ведь глаза гипербореи на лице лемурии  вон первые касатки  опа   мимо окна  такие кратковременные атмосферные осадки    на плоту  на плаву  на борту  но я путь к ней найду ли  ultima thule  я плыву  как в бреду  я тону и бреду на ходулях  ultima thule  через судьбы и бури  но я путь к ней найду ли  к своей неуловимой ultima thule   ultima thule    я за билет  за ойкумену  отдал капитал  тут море немо  как одноименный некий капитан  грустный этап  заглянем в плеер   только трики там  не прут другие фрики так  мир держат три кита  не надо клянчить там умышленно любви у фанов  в порту среди громад промышленных левиафанов  искал плывущих за фронтир  в одну из атлантид  а разум  будто ламантин  дрейфует между книжных фабул  нас на крюки нанижет фатум  перемкнёт нейроны  сердце сгорит  как рим нерона  пусть ритм неровный  биться с природой  осмелев  сделать шаг из кустов   я как жак-ив кусто  ведь предпочел давно пирсы перронам  холод стикса с хароном  горло стиснет гарротой  кто-то скиснет в пороке  время с них скинет короны  с той же стрижкой короткой  в трюме меж сеток  коробок  севшим на паром укажет путь лоцман седобородый  ведь курс лежит туда  на край материков  за горы  где море  выгибая горб  шлёт моряков к дагону  кляня погоду  средь останков каравелл торговых  меня встретит та  с лицом мадонны и взглядом горгоны    на плоту  на плаву  на борту  но я путь к ней найду ли  ultima thule  я плыву  как в бреду  я тону и бреду на ходулях  ultima thule  через судьбы и бури  но я путь к ней найду ли  к своей неуловимой ultima thule   ultima thule  на плоту  на плаву  на борту  но я путь к ней найду ли  ultima thule  я плыву  как в бреду  я тону и бреду на ходулях  ultima thule  через судьбы и бури  но я путь к ней найду ли  к своей неуловимой ultima thule   ultima thule    </s>    <s>    это oxxxymiro  из лондона  специально для optik russia  слушайте рифмы  йе     рэперы грозятся бифами и диссами  рифмами и skills ами  клипами  релизами  облизывая марки  как филателисты  вы нарки  я вас разнесу  а вы лишь спид и сифилис  я стакан  ты   стопка  мы не одна тусовка  твой рэп   подтасовка  мой рэп   потасовка  твоя crew не матерится  не пьёт и не курит  но выглядит  как вицин  моргунов и никулин  я пуленепробиваемый  в буре не потомляемый  дурень  тебя пинали мы  хули вы залупались  блин   даже кулио не так ультрапримитивен  мы вам руки превентивно к диджей-пультам привинтили  у меня есть эта страсть к декадентским  излишествам  дайте-ка мне настю каменских   или шубу из вымирающих видов  и кукол вуду рэперов  заражающих спидом  ты читаешь про богов  про саги и легенды  перестань выпендриваться  сын  ты сосал на weeke d ах  и остался никем  твой лейбл не матёрый  cut this shit  я зависаю с мародёрами на кладбищах  пешком по лондону с мешком полония  русский рэп без эмиграции   ничто  колония  ты считал что  код да винчи  был котом этого гения италии  твой iq   как твои гениталии  сын    ты   мой читатель  каждой ночью дрочишь в чате  не иначе твоя дочь от непорочного зачатия  оксимирон в рэпе   как красная нить  как сказал убийца   разрешите вас перебить      я хейтер  я ненавижу ваш рэп  ваш трек  ваш текст  мягкий как паштет   homie  это наш век  каждый лошпед  осознает  что у него продаж нет   я хейтер  я ненавижу ваш флоу  ваш слог  ваш хоп  не важно за что   homie  это наш год  до тебя дошло   oxxxymiro  и optik russia   в ваш рот     ты вошёл в вагон к нам   я дёрнул стопкран  как 100 грамм  тебе конец  как флагам соц  стран  вагон встал   ты выбираться на перрон стал  саппорт звал  но он подкачал  будто спортзал  йе   вам некуда деться  хватаетесь за сердце  будто бы вас немцы везут в освенцим  в детстве я в плеере круглосуточно гонял savas а  ты смеялся   я на optik russia  что ты щас скажешь   твоя подруга из-за красной занавески  ко мне идёт  и на ней лишь повязки и подвески  я  как собаке  ей кидаю палки  конкуренты отсосали  как князья при калке  йе    homie  ты склеил ласты  не помог лейкопластырь  мой флоу огнеопасный  твой флоу из пенопласта  ты полужвачка  как табак с ментолом  у меня синица в руке   зовите меня орнитологом    я хейтер  я ненавижу ваш рэп  ваш трек  ваш текст  мягкий как паштет   homie  это наш век  каждый лошпед  осознает  что у него продаж нет   я хейтер  я ненавижу ваш флоу  ваш слог  ваш хоп  не важно за что   homie  это наш год  до тебя дошло   oxxxymiro  и optik russia   в ваш рот   йе    я хейтер  я ненавижу ваш рэп  ваш трек  ваш текст  мягкий как паштет   homie  это наш век  каждый лошпед  осознает  что у него продаж нет   я хейтер  я ненавижу ваш флоу  ваш слог  ваш хоп  не важно за что   homie  это наш год  до тебя дошло   oxxxymiro  и optik russia   в ваш рот     </s>    <s>    сначала средь нехоженых троп  группа туристов потревожила заброшенный ров  через год  недалеко  сгинули сторож и плот  потом у коров пошёл ни на что не похожий приплод  то ли птицы в лесах молчат  то ли всем кажется так  в колодцах яркие цвета  не слышно даже цикад  толстый дьяк видел неслыханных масштабов следы  ему не поверили  сказав   кто ж их оставил  не ты    сельского лекаря смущает рост количества гнид  пьяный геолог не засек ход тектонических плит  рыбацкий катер достаёт что-то из моря  со дна  поэты с художниками корчатся и сходят с ума  озеро илом бурлит  шериф  покрутив пальцем у виска  выставил патрули  но немыслимое внутри  небо цвета пепла  ветхозаветной пыли  клубы  в десять лет как забытой церкви  вьется дым из трубы  вот тогда-то и мэру доставили два странных письма  что были частично нечитабельны  весьма и весьма  мэрия бы все к обману с маскарадом свела  но там были как подписи замов  так и старост села  все  что удалось разобрать  с трудом графолог нашел    не присылайте подкрепление  отдуб всё хорошо  и всё то  что раньше было не в порядке  ил дярв прошло  приезжайте к нам  йомод титсупто сав ил дярв акыдалв йывон шан     </s>    <s>    землянка проводами нашпигована  десятки мониторов  как сетчатка насекомого  и на экранах копошатся незнакомые  букашки-пешеходы  безучастные  как роботы  он входит  не включая свет  он начальник здесь  он ставит чай  предпочитает для начала сесть  ведь он уже немолод  опыт за плечами есть  и с каждым годом он всё чаще ощущает смерть  зато перед его очами вся людская сеть  вся людская суть   камеры вещают и вращается земная твердь  бликов в зрачках не счесть   он верховный наблюдатель  а это большая честь  и он сидит перед светящейся стеной из дисплеев  застывший  будто ящерица в зной  и глазеет  другому было б страшно и темно  а он  уставившись  чудно таращится в бездонный телек  на мониторах лабиринты  коридоры  содомиты  переломы  вечеринки  передозы  должники и кредиторы  от элиты до низов  за многоликую толпою он следит и мониторит  где-то трое лбов избивают больного  надев на него жабо  размалёванного  как клоун  где-то хоронят детей образцового детдома  но за ними дед в оба глядит из телескопа  и лишь когда свет фонарей за шторами погаснет  он поправляет на рукаве жёлтую повязку  и  заперев свою землянку крепко на ночь  вдаль  уходит  стуча по асфальту белой палочкой    </s>    <s>    слышь  поэт  твоя муза  походу  зверь неведомый  она копия trippie redd’а  но  увы  не дредами   малая  даже u2005не u2005скажешь  в плечах u2005косая сажень  и  глаза  как узники в u2005зиндане  глубоко посажены  эй   твой снаряд  малец  не бронебоен  хочу  чтобы меня судили deep-ex-se s’ы с joh yboy’ем  майню рэп  а не биткоин  как лиор коэн  десять лет прошло  ты всё никто  я стал звездой  ami a koyim  эй   я спускаюсь в андер забрать всё у господ  возвращаю онлайн-баттлам их проёбанный spot  пусть дворовый  но спорт  но так и быть  я в долгий путь  время пыль с винтовки сдуть  им положить цветок на грудь  как в девятом  отвернулся   все хором пиздят  вернулся   воры в гостях  обернулся   вторят и льстят  паскудные флейвы запутаны в олдовый костяк  и два скупленных тейпа  два культовых альбома спустя  та-на-ня-на    </s>    <s>      куда  кстати  идёт бабангида     нахуй       короче  чуваки  одесса  я слышал  что изначально город  где очень много евреев  правда или нет  я не знаю    да       короче  кто знает такую еврейскую песню   hava  agila  hava  agila  не-не  но мы с вами по-другому  мы с вами по-другому  бабан в могилу  бабан в могилу  бабан в могилу  смерть всем лохам     перед нами картина  спящая в канаве скотина  бабангида  давай завязывай с ca  abis sativa  в калининграде стыдно не знать кантов императив  но  ты позабыл о нём и залупался гиперактивно  тебе подфартило  ведь тебя диссить было противно  но ты  взяв имя футболиста  вдруг заныл неспортивно  что тебе всё похуй  но в моих темах  ханжа  наследил  если ты правдив  я буду звать тебя ходжа насреддин  пиздёж  ты лицемерен  и врёшь  будто сивый мерин  кто ж революционерам в ложь с небылицей верит   тебя подначивали   наконец-то диссуй его   и ты  как подлежащее  остался предсказуемым  йеа   всё так закономерно и просто  наверно  босс твоей газеты не дал карьерного роста  теперь уже поздно в рэпе лезть через тернии к звёздам  вне интернета   опёздал  но в интернете   апостол     умер тиджани бабангида  день памяти физкультурника     ты  как pimp schwab  любой видит в тебе дурака  ага   но тот был студентом психологии академгородка  а ты   журналистом  и я не удивился  если б узнал  что ты ради денег удавился за передовицу  лох   был король калининграда  но он шибко сдулся  под тобой не крепкий трон   это жидкий стул  сань  ты под ацетоном и под марками  в optik есть парламент и царь  значит  это конституционная монархия  yes  sir   ты должен быть тише травы  не шевелиться  с тех пор  как выдал за свои статьи от миши вербицкого  хочешь  он подтвердит сам  он уже наслышан про фана  дури профанов  за бабана пишут климов  проханов   газета  завтра  давно про жидов уже навоняла  у тебя нет ничего своего  даже погоняла  ты бездарен  и тебе в этих тяжелых условиях  дважды пизда  как телепередаче  школа злословия      умер тиджани бабангида  день памяти физкультурника     я рифмовал годами  так что моих рифм на вас хватит  и шевеля губами в такт  будто старик-маразматик  кассиром в магазе  пришив каждой из штанин по заплате  что ты мне хочешь рассказать  что я попсовик на зарплате    и хоть тебе и под 30  но твой детский лепет   веский аргумент в пользу того  кто здесь плебей и патриций  кто подлее полиции   журналист  его низко ставят все  как новые элементы менделеев в таблице  я знаю много кого   правых фанатов и леваков  оба фланга тебя бы вон гнали матом и кулаком  в украине  как видишь  тебя шлют красиво на идиш  после твоих слов жидом быть круто  шмок  спасибо за имидж  мой рэп  как порох  слово   сера  рифмы   калия нитрат  такие как ты  превратили гору короля в калининград  не хочешь давать кёнигсбергскую корону мирону   зови вымышленных друзей  привет baro  у vo  dro  у     умер тиджани бабангида  день памяти физкультурника     ты лишь в одном  как будто гитлер или бонапарт  а скорей саакашвили   ты не на того напал  бабан  у кучки твоих фанов походу в ушах не чисто  кого ты воспитал  блядь  мафона и шахматиста   это оксимирон  и ты снова стал сырою материей   назад к корням  значит  что мы тебя в грин парке зароем под деревом  я тоже схавал грибов и намутил наркоты  накурился в дым  и теперь читаю также быстро  как ты  я тебе кипу тку  на плите кипятку  нагреваю  заливаю в рот тебе  keep it cool   ссср   верхняя вольта с ракетами  даже пидоры из-за тебя становятся гетеро  снова лето пролетело  так что время думать  из тебя не вышло ни летова  ни mf doom а  ты   аноним  анонимы половину мира заполонили  но мы их анонимно похороним     умер тиджани бабангида  день памяти физкультурника     еврейский нос  мне всё равно  я   сирано де бержерак  ты   пиноккио  украшай уши лапшою  доширак   паре-тройке зевак  тем более  их словарный запас  ограничен фразой   homie  бля  какой угарный напас    и не удивительно  что рядом с ними ты эрудициею блещешь  видно  круто средь нулей побыть единицею  но рано или поздно вас ждёт обязательно крах  всё перевернётся  как испанский восклицательный знак  делай песни   отморозки грузят килобайты  но твой сельский юмор плоский  будто кира найтли  йеа   ты  как группа каста  где твои  трёхмерные рифмы    м    хоть убей  мне не видно  ты пишешь  наверно  верлибром  лёгкие атлеты все в слезах слезают с турника  плачет первая ракетка вместе с анной курниковой  рыдает публика   где же взять нам дубликат   умер бабангида  справим день памяти физкультурника    ха-ха-ха  йе  йе  do e  motherfucker  йе  я выражаю уважение своему сопернику по раунду  который выступал у нас на разогреве в одессе  а господин журналист  вы  наконец  довольны  вы  наконец  получили свой  блядь  инфоповод  свой  блядь  медиа вброс  четыре раунда подряд нужно было упоминать моё имя  чтобы я наконец снизошёл и ответил  ну что  поздравляю  господин  блядь  журналист и одновременно с этим футбольный фанат  ты уже совсем заврался  если кто не знает  журналисты в иерархии стоят даже ниже мусоров  тебя просто к стадиону не подпустили бы  зачем врать  бабангида  зачем ты врёшь везде  ты   аноним  тебя никто никогда не видел  я   реальный человек с реальными недостатками  кто ты  скажи мне  тебя никто не знает даже в калининграде  ёпт  я могу сделать ещё 14 и 88 строчек тебе  но тебе нассы в глаза   божья роса  блядь  пошёл нахуй  лох     </s>    <s>     он успел спросить у раджабова  с кем бы тот хотел записать совместный трек  раджабов ответил  u2005что u2005с оксимироном     какой политактивист  u2005я таджик и самбист  был подписан на u2005рип  клеил чик  делал биз  обычный тип стал демоном и чуть ли не декабристом  за испуг   до пяти лет  почти как за убийство  как нас там  в новостях   хвалят  а толку   погулял малость от кормяка к шконкам  ха   матросская тишина  москва видна в щёлку  самариддин и самаритянин  запись на плёнку  дело выжимает  идея выживает  тело лишь  бывает  злит  что раз в неделю душевая  продольный  обновите мне в мини-баре ром-колу  сэр   снится дом  в котором всё не приварено к полу  если б мне каждый третий рэпер  например  написал  я б четвертый месяц не сидел  лишь когда каждый политзек получит двести писем в день  над крышами везде подует ветер перемен  яу     буриме  отправляйся вдаль в письме  нацарапанная на стене  пара фраз про будни здесь   свобода   то  что в сердце  остальное детали   и буриме  допиши мои стихи  как в игре  чтоб моё имя было слышно и вне  этих стен  где люди есть   свобода   то  что в сердце  остальное детали      остальное детали    остальное де      не нарушал закон  не трогал людей в погонах  дело заказное  они сажают любого  суп тут та еще стряпня  словно варили носки  а рыба воняет так  будто сдохла от тоски  студенту   ивс  прокурору   медали  свобода   то  что в сердце  остальное детали     </s>    <s>    что такое империя   сходу приходят в голову то ли   звёздные войны  плюс mmorpg  то ли тот  забытый школьный урок истории древнего мира  чей-то лавровый венок - вот более-менее всё     а  нет  ещё  всё громче слово  империя  слышно  когда идут разговоры  споры и прения  бой  за территорию  войны  ссоры и трения  в слове так много пафоса  понта  крови и тления  но  у меня с латынью ещё со школы не клеилось   вроде как слово значило  власть  - берия  тиберий  марк аврелий  кто-нибудь  что такое империя   молчание     мёртвым не до полемики  где сегодня наполеон  монголы  шумеры   а тысячелетний рейх  стоял всего-то мгновение  я называю нечто иное словом  империя    то  во что мечтатель и предприниматель верит  растит  и она растёт из идеи  корень и стебель  накрыт бетонной плитой  но цветок на волю из тени  ползёт  и сколько подобных историй - столько империй  вот мы  не стали вторить и следовать  в хоре петь и  по шаблонам  напротив  хотели нос утереть им  по проторенным тропам на всём готовом легко  но империю не обрести  история не заметит  мы selfmade  вот от чего они нам сулят  важна не империя  важно то  что она с нуля  по принципу diy  и не было ни черта  мой стартовый капитал - это те  с кем я начинал  не каждый строит империю  важно строить империю  но  свою иначе за вас построят империю  кто  чужая власть или местный князь и по мере  того  как эти стратегии нас на  конго  либерию  на сферы влияния делят ловко  уверенно плюй   и начинай свою собственную империю    </s>    <s>    я рос один  только  два капитана    три мушкетёра    четыре танкиста  делали картину путёвой  мне было пять  я был гадкий лебединый утёнок  от шести до семи я возненавидел учёбу  на колесе восьмёрка  во дворе спалили  девятку    девять с половиной недель  всё объяснили мне внятно  досчитал до десяти   на хуй прятки  каждую осень  одиннадцатого пускал в дом напротив бумажный самолётик  говорят   ты ободрал двенадцать колен   чтобы взобраться наверх  я  пятницу 13  смотрел  потом эмиграция  плен  и меня в нём носит и носит  в 14 ощущал себя на 88  погодите  я сбился  номера в мобиле визжат   38 попугаев  44 чижа  кому-то 1001 ночь с тобой  а мне ни одной  привет на дно   двадцать тысяч лье под водой    эти цифры и цвета  тысячи цитат  визы  паспорта  всё растворится без следа  и всё что испытал  все лица и скитания  блеск и нищета  сохранится лишь в моих текстах  эти цифры и цвета  тысячи цитат  визы  паспорта  всё растворится без следа  и всё что испытал  все лица и скитания  блеск и нищета  сохранится лишь в моих текстах    я был зелёным  когда чёрным налом звались наличные  когда белый дом был то жёлтым  то красно-коричневым   бумер    чёрным  саня   белым и жёлтою   пресса  когда  мегаполис-экспресс  писал про чёрные мессы  когда пиджак должен был быть малиновым  а я бредил  белым солнцем пустыни  и  жёлтою субмариною   покинул дом  был что-либо сделать бессилен  и каждый красный день календаря я с серым был синим  зеленый змий и красный глаз от purple haze  мне 26  ещё один год  и я kurt cobai   и я столько дней был белой вороной  прохожим  среди голубой крови  белой кости  золотой молодёжи  и вот я   не желторотый  но лет мало ещё  не чернорабочий  но и не белый воротничок  но мой мир серый  ведь не в моей постели она  привет со дна  где нет места пастельным тонам    эти цифры и цвета  тысячи цитат  визы  паспорта  всё растворится без следа  и всё что испытал  все лица и скитания  блеск и нищета  сохранится лишь в моих текстах  эти цифры и цвета  тысячи цитат  визы  паспорта  всё растворится без следа  и всё что испытал  все лица и скитания  блеск и нищета  сохранится лишь в моих текстах    </s>    <s>    перелатанный узелок  иду походкой придурковатою босиком  для дезертира военкоматы и узи в лоб   хуй те в рот  я кропаю каракули до сих пор  спальники и подъезды  площади и таверны  сами-то мы не местные  всё в кредит и в аренду  протягивайте руки на паперти  но пока я не протянул ноги  кладу их на скатерти  а   на дорогах надо бороться  прописка бомжа  страховка канатоходца  нас тошнит  когда ты рассуждаешь про hustla shit  насмешил  посмотри на меня в профиль   ростовщик  и раз так жил  то всё с детства делал не так  в кармане ломаный грош да неразменный пятак  видя вдалеке чужака  горожане в панике  всё своё с собой   бродяга шагает налегке    вечный жид  я умру  но в этих песнях буду вечно жить  помни об этом  если меня задеть решил  ведь я вечный жид  я скитаюсь  мне не суждено прилечь в тиши  вечный жид  я умру  но в этих песнях буду вечно жить  помни об этом  если меня задеть решил  ведь я вечный жид  я скитаюсь  мне не суждено прилечь в тиши  вечный жид    мне не ближе дети иакова  я изгой везде  ненавижу всех одинаково  национальность   это слишком человеческое  нечего лечить мне за отчизну и отечество  мы проходимцы  таким надо родиться  махинатор спекулирует номерами ритца  все карты мечены  ярмарки  балаганы  тут   карлики  великаны и бородатые женщины  я   еретик  быть такими   не поле перейти  трахну твою деву спереди  харкну в аперитив  на шикарный камзол   потасканная заплата  паспорт выдан послом хазарского каганата  бастарды  пасынки  гастарбайтеры   мастаки  воровать у как раз таких  как вы  где твой удар с ноги   напёрстки  скрипочки  но если хочешь быть одним из нас  навёрстывай   просто протри очки    вечный жид  я умру  но в этих песнях буду вечно жить  помни об этом  если меня задеть решил  ведь я вечный жид  я скитаюсь  мне не суждено прилечь в тиши  вечный жид  я умру  но в этих песнях буду вечно жить  помни об этом  если меня задеть решил  ведь я вечный жид  я скитаюсь  мне не суждено прилечь в тиши  вечный жид    </s>    <s>    oxxxymiro   я вам всё доказал  да  мой рэп для вас бельмо  ведь он растет на глазах  вавилондон  у моих пацанов работы нет  ворованные кеды lacoste и футболка motorhead  мода - бред  имидж - ничто  но голод всё  вчера кассир и полотёр - нынче в поло shirt  пусть и самую малость  но я постоянно пытаюсь  из кварталов  где хаос  вылезть из подвала в пентахус  агрессии много  так что в этом месте херово  если две хромосомы x и нету тестостерона  холодный  озлобленный лондон кувырком по наклонной  неблагосклонный небосклон на высотках  словно колоннах  тренировочные штаны  airmax  ножик  здесь чужаков любой подросток поиметь может  и если не хватит яиц  ты будешь следующим  какая жизнь - такие песни в стране мужчин    быть мужиком - это состояние души  широкой словно расстояния в глуши  и если ты в состоянии понять  как с шалавой промотать состояние - пляши  но коль ты сам не в состоянии решить  кто ты - баба  размазня или мужик  то я дам тебе задание  смени пол заранее и сигануть со здания спеши    в россии - всё наоборот  хотя еще тяжелей  живётся  но рэп беззубый  будто вы жуёте желе  и льете елей  ваш рэп в конечном счете милей  даже когда врёте ради промо  что даёте люлей  полстраны погибает  но читать о них не с руки  ведь у вас более популярны маменькины сынки  ряженые в мужчин  но я таким верю ни на йоту  русский рэп настолько женственный  что у меня встает   ты продолжал писать  но я не фанатик этого стайла  смотри  как смоки мо украл этот бит у хозяина  будто в вериги и петли вас заковали в интриги и сплетни  женская логика  так что в итоге ваш рэп безликий и средний  в жюри везде девицы сидят  коль я писклявый  почему нету яиц у тебя   вся ваша сцена в мыльном пузыре  хотя вокруг жести не меньше  какая жизнь - такие песни  в стране женщин    быть мужиком - это состояние души  широкой словно расстояния в глуши  и если ты в состоянии понять  как с шалавой промотать состояние - пляши  но коль ты сам не в состоянии решить  кто ты - баба  размазня или мужик  то я дам тебе задание  смени пол заранее и сигануть со здания спеши    не надо ехать за сто морей  ведь царство красных фонарей прямо у вас во дворе  рэперы продают себя  как будто жрицы любви  добавь им к единице нули  или жди целлюлит  этот баттл бордель  я отдохнул отлично в нём  меня учит терпимости этот публичный дом  будто страна  не требующая при въезде виз  ведь я ебу её гражданок  будто секс-турист  азиаток  мулаток снять предлагают вам тут  но адик вылетел  попытавшись снять панамку   короб тоже пас  ведь он больно добрый  и обращается с путанами  как холден колфилд   но я верну вам патриархат  как мормоны  на заду рэпа моя пятерня - i got five o  it   и весь этот текст написан за последний день  какая жизнь - такие песни в стране блядей    быть мужиком - это состояние души   широкой словно расстояния в глуши  и если ты в состоянии понять  как с шалавой промотать состояние - пляши  но коль ты сам не в состоянии решить  кто ты - баба  размазня или мужик  то я дам тебе задание  смени пол заранее и сигануть со здания спеши    </s>  \n"
     ]
    }
   ],
   "source": [
    "print(padded_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break our text into separate sentences\n",
    "sentences = nltk.tokenize.sent_tokenize(padded_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split our sentences into separate words\n",
    "words = []\n",
    "for i in sentences:\n",
    "    words.append(i.split())\n",
    "words = list(flatten(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'дон', 'ли', 'волга', 'ли', 'течёт', 'котомку', 'на', 'плечо', 'боль', 'в', 'груди', 'там', 'тайничок', 'открытый', 'фомкой', 'не', 'ключом', 'сколько', 'миль', 'ещё', 'перелет', 'короткий', 'был', 'не', 'в', 'счёт', 'долгий', 'пыльный', 'чёс']\n"
     ]
    }
   ],
   "source": [
    "print(words[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break into bigrams\n",
    "bigrams = list(bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'дон'), ('дон', 'ли'), ('ли', 'волга'), ('волга', 'ли'), ('ли', 'течёт')]\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break into trigrams\n",
    "trigrams = list(trigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'дон', 'ли'), ('дон', 'ли', 'волга'), ('ли', 'волга', 'ли'), ('волга', 'ли', 'течёт'), ('ли', 'течёт', 'котомку')]\n"
     ]
    }
   ],
   "source": [
    "print(trigrams[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating train / test ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_train_size = int(len(bigrams) * 0.8)\n",
    "bi_test_size = int(len(bigrams) * 0.2)\n",
    "\n",
    "train_bigrams = bigrams[:bi_train_size]\n",
    "test_bigrams = bigrams[bi_test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_train_size = int(len(trigrams) * 0.8)\n",
    "tri_test_size = int(len(trigrams) * 0.2)\n",
    "\n",
    "train_trigrams = bigrams[:tri_train_size]\n",
    "test_trigrams = bigrams[tri_test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make bigram counts and vocabulary\n",
    "bi_train, bi_vocab = padded_everygram_pipeline(2, train_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make trigram counts and vocabulary\n",
    "tri_train, tri_vocab = padded_everygram_pipeline(3, train_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimator (MLE) models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to detokenize our final text\n",
    "detok = TreebankWordDetokenizer().detokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to generate a random text and clear it from <s>...</s>\n",
    "def generator(model, words, random_seed=20):\n",
    "    rap = []\n",
    "    for word in model.generate(words, random_seed=random_seed):\n",
    "        if word == \"<s>\" or word == \"</s>\":\n",
    "            continue\n",
    "        rap.append(word)\n",
    "    return detok(rap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Max Likelihood Estimator model for bigrams\n",
    "bi_lm = MLE(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the bigram model\n",
    "bi_lm.fit(bi_train, bi_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ты пиросмани конечно братства вес в вашем доме быть buff или сохранять тебя вы лев и так но город беден ты'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a text\n",
    "generator(bi_lm, 50, random_seed=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, the bigram MLE model generated the verse, which perfectly reflects the quality of this model.\n",
    "\n",
    "Unfortunately, I did find the way to stop predicting padding sings, that is why the number of predicted words sometimes is different from the one that was indicated. Changing padding from verses to songs did not help much.\n",
    "\n",
    "Nevertheless, the model does its work by finding neigbouring words throug the bigrams.\n",
    "Probably, the model performance could be improved by better processing of input data and larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating model entropy\n",
    "bi_lm.entropy(test_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating model perplexity\n",
    "bi_lm.perplexity(test_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly, model entropy and perplexity for some reason tend to infinity on the test bigrams. \n",
    "\n",
    "If not because of some code or math problem, probably the model encountered such pairs of words (or even words) that has never seen before. That is why, it fails to generalize this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6913928053562075"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.entropy(train_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.918734121883936"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_lm.perplexity(train_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, using the data on which the model was trained, we receive more realistic result (not suprising). Though, it is may be incorrent to use training data, now we may guess that the problem can be in input data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Max Likelihood Estimator model for trigrams\n",
    "tri_lm = MLE(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the trigram model\n",
    "tri_lm.fit(tri_train, tri_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'мне подождать'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate a text\n",
    "generator(tri_lm, 20, random_seed=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_lm.entropy(test_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_lm.perplexity(test_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7110383225123598"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_lm.entropy(train_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.09585476787456"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_lm.perplexity(train_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In trigram model we have quite the same problem, though entropy and perplexity are a bit higher on training sample. If it was test sample, we might conclude that this model is worse on text generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further on, I decided to construct RNN LSTM model, which will give us some hot rapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_lyrics = lyrics\n",
    "\n",
    "for i in clear_lyrics:\n",
    "    for char in bad_chars:\n",
    "        clear_lyrics = str(clear_lyrics).replace(char,' ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a function to tokenize & filter input data\n",
    "def tokenize_words(input): \n",
    "    #Initiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "    \n",
    "    #Filter some words if they are in nltk stop list\n",
    "    filtered = filter(lambda token: token not in stopwords.words('russian'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess our input data and make tokens\n",
    "processed_inputs = tokenize_words(clear_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "дон волга течёт котомку плечо боль груди тайничок открытый фомкой ключом сколько миль ещё перелет короткий счёт долгий пыльный чёс фургон набит коробками мерчём верим подфартит наши постели портативны менестрелю пути корпоратив квартирник схемы однотипны mc смену породив достигли смены парадигмы рэп\n"
     ]
    }
   ],
   "source": [
    "print(processed_inputs[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since NN requires numbers, but not text characters, convert the characters in our input to numbers\n",
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 89852\n",
      "Total vocab: 74\n"
     ]
    }
   ],
   "source": [
    "#For futher data preparation, calculate the length of our input and vocabulary\n",
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define how long an individual sequence (one complete mapping of inputs characters as integers) to be\n",
    "seq_length = 100\n",
    "\n",
    "#Make empty lists to store our input and output data\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop through inputs - start at the beginning and go until we hit the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    \n",
    "    #Define input and output sequences\n",
    "    #Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    #Output sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    #Convert list of characters to integers and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 89752\n"
     ]
    }
   ],
   "source": [
    "#Total number of sequences\n",
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform it to numpy array for our NN\n",
    "X = np.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "\n",
    "#Make the numbers float for activation function being able to interpret them as probabilities\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-hot encode our data\n",
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Construct stacked LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(LSTM(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For our callback create a file with RNN weights\n",
    "filename = \"RNN_weights.hdf5\"\n",
    "filepath = \"RNN_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set callback function to save the best weights\n",
    "callbacks = [ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile RNN model on default adam optimizer and track categorical cross-entropy\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 3.3559\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.35589, saving model to RNN_weights.hdf5\n",
      "Epoch 2/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.9859\n",
      "\n",
      "Epoch 00002: loss improved from 3.35589 to 2.98593, saving model to RNN_weights.hdf5\n",
      "Epoch 3/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.9063\n",
      "\n",
      "Epoch 00003: loss improved from 2.98593 to 2.90632, saving model to RNN_weights.hdf5\n",
      "Epoch 4/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.8594\n",
      "\n",
      "Epoch 00004: loss improved from 2.90632 to 2.85941, saving model to RNN_weights.hdf5\n",
      "Epoch 5/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 2.8246\n",
      "\n",
      "Epoch 00005: loss improved from 2.85941 to 2.82461, saving model to RNN_weights.hdf5\n",
      "Epoch 6/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.7810\n",
      "\n",
      "Epoch 00006: loss improved from 2.82461 to 2.78101, saving model to RNN_weights.hdf5\n",
      "Epoch 7/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 2.7302\n",
      "\n",
      "Epoch 00007: loss improved from 2.78101 to 2.73018, saving model to RNN_weights.hdf5\n",
      "Epoch 8/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.6783\n",
      "\n",
      "Epoch 00008: loss improved from 2.73018 to 2.67834, saving model to RNN_weights.hdf5\n",
      "Epoch 9/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.6397\n",
      "\n",
      "Epoch 00009: loss improved from 2.67834 to 2.63967, saving model to RNN_weights.hdf5\n",
      "Epoch 10/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.6086\n",
      "\n",
      "Epoch 00010: loss improved from 2.63967 to 2.60864, saving model to RNN_weights.hdf5\n",
      "Epoch 11/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.5784\n",
      "\n",
      "Epoch 00011: loss improved from 2.60864 to 2.57844, saving model to RNN_weights.hdf5\n",
      "Epoch 12/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.5456\n",
      "\n",
      "Epoch 00012: loss improved from 2.57844 to 2.54558, saving model to RNN_weights.hdf5\n",
      "Epoch 13/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.5163\n",
      "\n",
      "Epoch 00013: loss improved from 2.54558 to 2.51631, saving model to RNN_weights.hdf5\n",
      "Epoch 14/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.4970\n",
      "\n",
      "Epoch 00014: loss improved from 2.51631 to 2.49702, saving model to RNN_weights.hdf5\n",
      "Epoch 15/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.4690\n",
      "\n",
      "Epoch 00015: loss improved from 2.49702 to 2.46900, saving model to RNN_weights.hdf5\n",
      "Epoch 16/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.4501\n",
      "\n",
      "Epoch 00016: loss improved from 2.46900 to 2.45005, saving model to RNN_weights.hdf5\n",
      "Epoch 17/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.4274\n",
      "\n",
      "Epoch 00017: loss improved from 2.45005 to 2.42737, saving model to RNN_weights.hdf5\n",
      "Epoch 18/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.4063\n",
      "\n",
      "Epoch 00018: loss improved from 2.42737 to 2.40626, saving model to RNN_weights.hdf5\n",
      "Epoch 19/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.3880\n",
      "\n",
      "Epoch 00019: loss improved from 2.40626 to 2.38798, saving model to RNN_weights.hdf5\n",
      "Epoch 20/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.3701\n",
      "\n",
      "Epoch 00020: loss improved from 2.38798 to 2.37015, saving model to RNN_weights.hdf5\n",
      "Epoch 21/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.3495\n",
      "\n",
      "Epoch 00021: loss improved from 2.37015 to 2.34954, saving model to RNN_weights.hdf5\n",
      "Epoch 22/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.3296\n",
      "\n",
      "Epoch 00022: loss improved from 2.34954 to 2.32956, saving model to RNN_weights.hdf5\n",
      "Epoch 23/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.3084\n",
      "\n",
      "Epoch 00023: loss improved from 2.32956 to 2.30844, saving model to RNN_weights.hdf5\n",
      "Epoch 24/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.2925\n",
      "\n",
      "Epoch 00024: loss improved from 2.30844 to 2.29249, saving model to RNN_weights.hdf5\n",
      "Epoch 25/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.2760\n",
      "\n",
      "Epoch 00025: loss improved from 2.29249 to 2.27599, saving model to RNN_weights.hdf5\n",
      "Epoch 26/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.2599\n",
      "\n",
      "Epoch 00026: loss improved from 2.27599 to 2.25991, saving model to RNN_weights.hdf5\n",
      "Epoch 27/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 2.2391\n",
      "\n",
      "Epoch 00027: loss improved from 2.25991 to 2.23912, saving model to RNN_weights.hdf5\n",
      "Epoch 28/700\n",
      "89752/89752 [==============================] - 117s 1ms/step - loss: 2.2264\n",
      "\n",
      "Epoch 00028: loss improved from 2.23912 to 2.22643, saving model to RNN_weights.hdf5\n",
      "Epoch 29/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 2.2122\n",
      "\n",
      "Epoch 00029: loss improved from 2.22643 to 2.21224, saving model to RNN_weights.hdf5\n",
      "Epoch 30/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1910\n",
      "\n",
      "Epoch 00030: loss improved from 2.21224 to 2.19098, saving model to RNN_weights.hdf5\n",
      "Epoch 31/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1813\n",
      "\n",
      "Epoch 00031: loss improved from 2.19098 to 2.18126, saving model to RNN_weights.hdf5\n",
      "Epoch 32/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1667\n",
      "\n",
      "Epoch 00032: loss improved from 2.18126 to 2.16672, saving model to RNN_weights.hdf5\n",
      "Epoch 33/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1518\n",
      "\n",
      "Epoch 00033: loss improved from 2.16672 to 2.15176, saving model to RNN_weights.hdf5\n",
      "Epoch 34/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1351\n",
      "\n",
      "Epoch 00034: loss improved from 2.15176 to 2.13509, saving model to RNN_weights.hdf5\n",
      "Epoch 35/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1212\n",
      "\n",
      "Epoch 00035: loss improved from 2.13509 to 2.12119, saving model to RNN_weights.hdf5\n",
      "Epoch 36/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.1100\n",
      "\n",
      "Epoch 00036: loss improved from 2.12119 to 2.10998, saving model to RNN_weights.hdf5\n",
      "Epoch 37/700\n",
      "89752/89752 [==============================] - 117s 1ms/step - loss: 2.0987\n",
      "\n",
      "Epoch 00037: loss improved from 2.10998 to 2.09866, saving model to RNN_weights.hdf5\n",
      "Epoch 38/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 2.0843\n",
      "\n",
      "Epoch 00038: loss improved from 2.09866 to 2.08432, saving model to RNN_weights.hdf5\n",
      "Epoch 39/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 2.0698\n",
      "\n",
      "Epoch 00039: loss improved from 2.08432 to 2.06982, saving model to RNN_weights.hdf5\n",
      "Epoch 40/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 2.0580\n",
      "\n",
      "Epoch 00040: loss improved from 2.06982 to 2.05801, saving model to RNN_weights.hdf5\n",
      "Epoch 41/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 2.0432\n",
      "\n",
      "Epoch 00041: loss improved from 2.05801 to 2.04324, saving model to RNN_weights.hdf5\n",
      "Epoch 42/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 2.0313\n",
      "\n",
      "Epoch 00042: loss improved from 2.04324 to 2.03125, saving model to RNN_weights.hdf5\n",
      "Epoch 43/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.0348\n",
      "\n",
      "Epoch 00043: loss did not improve from 2.03125\n",
      "Epoch 44/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 2.0161\n",
      "\n",
      "Epoch 00044: loss improved from 2.03125 to 2.01611, saving model to RNN_weights.hdf5\n",
      "Epoch 45/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 2.0049\n",
      "\n",
      "Epoch 00045: loss improved from 2.01611 to 2.00492, saving model to RNN_weights.hdf5\n",
      "Epoch 46/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.9865\n",
      "\n",
      "Epoch 00046: loss improved from 2.00492 to 1.98645, saving model to RNN_weights.hdf5\n",
      "Epoch 47/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9746\n",
      "\n",
      "Epoch 00047: loss improved from 1.98645 to 1.97464, saving model to RNN_weights.hdf5\n",
      "Epoch 48/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.9609\n",
      "\n",
      "Epoch 00048: loss improved from 1.97464 to 1.96094, saving model to RNN_weights.hdf5\n",
      "Epoch 49/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9518\n",
      "\n",
      "Epoch 00049: loss improved from 1.96094 to 1.95180, saving model to RNN_weights.hdf5\n",
      "Epoch 50/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9462\n",
      "\n",
      "Epoch 00050: loss improved from 1.95180 to 1.94619, saving model to RNN_weights.hdf5\n",
      "Epoch 51/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9438\n",
      "\n",
      "Epoch 00051: loss improved from 1.94619 to 1.94377, saving model to RNN_weights.hdf5\n",
      "Epoch 52/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9231\n",
      "\n",
      "Epoch 00052: loss improved from 1.94377 to 1.92314, saving model to RNN_weights.hdf5\n",
      "Epoch 53/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.9138\n",
      "\n",
      "Epoch 00053: loss improved from 1.92314 to 1.91384, saving model to RNN_weights.hdf5\n",
      "Epoch 54/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8994\n",
      "\n",
      "Epoch 00054: loss improved from 1.91384 to 1.89936, saving model to RNN_weights.hdf5\n",
      "Epoch 55/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8900\n",
      "\n",
      "Epoch 00055: loss improved from 1.89936 to 1.89003, saving model to RNN_weights.hdf5\n",
      "Epoch 56/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.8819\n",
      "\n",
      "Epoch 00056: loss improved from 1.89003 to 1.88185, saving model to RNN_weights.hdf5\n",
      "Epoch 57/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.8636\n",
      "\n",
      "Epoch 00057: loss improved from 1.88185 to 1.86359, saving model to RNN_weights.hdf5\n",
      "Epoch 58/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.8589\n",
      "\n",
      "Epoch 00058: loss improved from 1.86359 to 1.85886, saving model to RNN_weights.hdf5\n",
      "Epoch 59/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8521\n",
      "\n",
      "Epoch 00059: loss improved from 1.85886 to 1.85208, saving model to RNN_weights.hdf5\n",
      "Epoch 60/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8333\n",
      "\n",
      "Epoch 00060: loss improved from 1.85208 to 1.83331, saving model to RNN_weights.hdf5\n",
      "Epoch 61/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8291\n",
      "\n",
      "Epoch 00061: loss improved from 1.83331 to 1.82912, saving model to RNN_weights.hdf5\n",
      "Epoch 62/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8137\n",
      "\n",
      "Epoch 00062: loss improved from 1.82912 to 1.81373, saving model to RNN_weights.hdf5\n",
      "Epoch 63/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8150\n",
      "\n",
      "Epoch 00063: loss did not improve from 1.81373\n",
      "Epoch 64/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.8054\n",
      "\n",
      "Epoch 00064: loss improved from 1.81373 to 1.80544, saving model to RNN_weights.hdf5\n",
      "Epoch 65/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7975\n",
      "\n",
      "Epoch 00065: loss improved from 1.80544 to 1.79747, saving model to RNN_weights.hdf5\n",
      "Epoch 66/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7866\n",
      "\n",
      "Epoch 00066: loss improved from 1.79747 to 1.78662, saving model to RNN_weights.hdf5\n",
      "Epoch 67/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7767\n",
      "\n",
      "Epoch 00067: loss improved from 1.78662 to 1.77674, saving model to RNN_weights.hdf5\n",
      "Epoch 68/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.7714\n",
      "\n",
      "Epoch 00068: loss improved from 1.77674 to 1.77141, saving model to RNN_weights.hdf5\n",
      "Epoch 69/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7576\n",
      "\n",
      "Epoch 00069: loss improved from 1.77141 to 1.75765, saving model to RNN_weights.hdf5\n",
      "Epoch 70/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.7501\n",
      "\n",
      "Epoch 00070: loss improved from 1.75765 to 1.75008, saving model to RNN_weights.hdf5\n",
      "Epoch 71/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7394\n",
      "\n",
      "Epoch 00071: loss improved from 1.75008 to 1.73945, saving model to RNN_weights.hdf5\n",
      "Epoch 72/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7320\n",
      "\n",
      "Epoch 00072: loss improved from 1.73945 to 1.73199, saving model to RNN_weights.hdf5\n",
      "Epoch 73/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7194\n",
      "\n",
      "Epoch 00073: loss improved from 1.73199 to 1.71939, saving model to RNN_weights.hdf5\n",
      "Epoch 74/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7175\n",
      "\n",
      "Epoch 00074: loss improved from 1.71939 to 1.71746, saving model to RNN_weights.hdf5\n",
      "Epoch 75/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7050\n",
      "\n",
      "Epoch 00075: loss improved from 1.71746 to 1.70502, saving model to RNN_weights.hdf5\n",
      "Epoch 76/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.7026\n",
      "\n",
      "Epoch 00076: loss improved from 1.70502 to 1.70265, saving model to RNN_weights.hdf5\n",
      "Epoch 77/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.6953\n",
      "\n",
      "Epoch 00077: loss improved from 1.70265 to 1.69526, saving model to RNN_weights.hdf5\n",
      "Epoch 78/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6834\n",
      "\n",
      "Epoch 00078: loss improved from 1.69526 to 1.68342, saving model to RNN_weights.hdf5\n",
      "Epoch 79/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6808\n",
      "\n",
      "Epoch 00079: loss improved from 1.68342 to 1.68080, saving model to RNN_weights.hdf5\n",
      "Epoch 80/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6721\n",
      "\n",
      "Epoch 00080: loss improved from 1.68080 to 1.67213, saving model to RNN_weights.hdf5\n",
      "Epoch 81/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6682\n",
      "\n",
      "Epoch 00081: loss improved from 1.67213 to 1.66815, saving model to RNN_weights.hdf5\n",
      "Epoch 82/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6585\n",
      "\n",
      "Epoch 00082: loss improved from 1.66815 to 1.65849, saving model to RNN_weights.hdf5\n",
      "Epoch 83/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6549\n",
      "\n",
      "Epoch 00083: loss improved from 1.65849 to 1.65489, saving model to RNN_weights.hdf5\n",
      "Epoch 84/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6365\n",
      "\n",
      "Epoch 00084: loss improved from 1.65489 to 1.63649, saving model to RNN_weights.hdf5\n",
      "Epoch 85/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6363\n",
      "\n",
      "Epoch 00085: loss improved from 1.63649 to 1.63627, saving model to RNN_weights.hdf5\n",
      "Epoch 86/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6284\n",
      "\n",
      "Epoch 00086: loss improved from 1.63627 to 1.62837, saving model to RNN_weights.hdf5\n",
      "Epoch 87/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6241\n",
      "\n",
      "Epoch 00087: loss improved from 1.62837 to 1.62409, saving model to RNN_weights.hdf5\n",
      "Epoch 88/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6188\n",
      "\n",
      "Epoch 00088: loss improved from 1.62409 to 1.61878, saving model to RNN_weights.hdf5\n",
      "Epoch 89/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.6053\n",
      "\n",
      "Epoch 00089: loss improved from 1.61878 to 1.60533, saving model to RNN_weights.hdf5\n",
      "Epoch 90/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.6073\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.60533\n",
      "Epoch 91/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5948\n",
      "\n",
      "Epoch 00091: loss improved from 1.60533 to 1.59483, saving model to RNN_weights.hdf5\n",
      "Epoch 92/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5951\n",
      "\n",
      "Epoch 00092: loss did not improve from 1.59483\n",
      "Epoch 93/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5848\n",
      "\n",
      "Epoch 00093: loss improved from 1.59483 to 1.58481, saving model to RNN_weights.hdf5\n",
      "Epoch 94/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5735\n",
      "\n",
      "Epoch 00094: loss improved from 1.58481 to 1.57352, saving model to RNN_weights.hdf5\n",
      "Epoch 95/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5665\n",
      "\n",
      "Epoch 00095: loss improved from 1.57352 to 1.56647, saving model to RNN_weights.hdf5\n",
      "Epoch 96/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5639\n",
      "\n",
      "Epoch 00096: loss improved from 1.56647 to 1.56394, saving model to RNN_weights.hdf5\n",
      "Epoch 97/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5622\n",
      "\n",
      "Epoch 00097: loss improved from 1.56394 to 1.56218, saving model to RNN_weights.hdf5\n",
      "Epoch 98/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5517\n",
      "\n",
      "Epoch 00098: loss improved from 1.56218 to 1.55169, saving model to RNN_weights.hdf5\n",
      "Epoch 99/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5397\n",
      "\n",
      "Epoch 00099: loss improved from 1.55169 to 1.53968, saving model to RNN_weights.hdf5\n",
      "Epoch 100/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.5400\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.53968\n",
      "Epoch 101/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5329\n",
      "\n",
      "Epoch 00101: loss improved from 1.53968 to 1.53292, saving model to RNN_weights.hdf5\n",
      "Epoch 102/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5315\n",
      "\n",
      "Epoch 00102: loss improved from 1.53292 to 1.53148, saving model to RNN_weights.hdf5\n",
      "Epoch 103/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5259\n",
      "\n",
      "Epoch 00103: loss improved from 1.53148 to 1.52588, saving model to RNN_weights.hdf5\n",
      "Epoch 104/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5171\n",
      "\n",
      "Epoch 00104: loss improved from 1.52588 to 1.51711, saving model to RNN_weights.hdf5\n",
      "Epoch 105/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5119\n",
      "\n",
      "Epoch 00105: loss improved from 1.51711 to 1.51190, saving model to RNN_weights.hdf5\n",
      "Epoch 106/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5063\n",
      "\n",
      "Epoch 00106: loss improved from 1.51190 to 1.50634, saving model to RNN_weights.hdf5\n",
      "Epoch 107/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5034\n",
      "\n",
      "Epoch 00107: loss improved from 1.50634 to 1.50338, saving model to RNN_weights.hdf5\n",
      "Epoch 108/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.5002\n",
      "\n",
      "Epoch 00108: loss improved from 1.50338 to 1.50019, saving model to RNN_weights.hdf5\n",
      "Epoch 109/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4938\n",
      "\n",
      "Epoch 00109: loss improved from 1.50019 to 1.49380, saving model to RNN_weights.hdf5\n",
      "Epoch 110/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.4850\n",
      "\n",
      "Epoch 00110: loss improved from 1.49380 to 1.48503, saving model to RNN_weights.hdf5\n",
      "Epoch 111/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4848\n",
      "\n",
      "Epoch 00111: loss improved from 1.48503 to 1.48484, saving model to RNN_weights.hdf5\n",
      "Epoch 112/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4792\n",
      "\n",
      "Epoch 00112: loss improved from 1.48484 to 1.47923, saving model to RNN_weights.hdf5\n",
      "Epoch 113/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4736\n",
      "\n",
      "Epoch 00113: loss improved from 1.47923 to 1.47363, saving model to RNN_weights.hdf5\n",
      "Epoch 114/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4671\n",
      "\n",
      "Epoch 00114: loss improved from 1.47363 to 1.46713, saving model to RNN_weights.hdf5\n",
      "Epoch 115/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4564\n",
      "\n",
      "Epoch 00115: loss improved from 1.46713 to 1.45640, saving model to RNN_weights.hdf5\n",
      "Epoch 116/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4536\n",
      "\n",
      "Epoch 00116: loss improved from 1.45640 to 1.45362, saving model to RNN_weights.hdf5\n",
      "Epoch 117/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4570\n",
      "\n",
      "Epoch 00117: loss did not improve from 1.45362\n",
      "Epoch 118/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4511\n",
      "\n",
      "Epoch 00118: loss improved from 1.45362 to 1.45111, saving model to RNN_weights.hdf5\n",
      "Epoch 119/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4426\n",
      "\n",
      "Epoch 00119: loss improved from 1.45111 to 1.44265, saving model to RNN_weights.hdf5\n",
      "Epoch 120/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.4392\n",
      "\n",
      "Epoch 00120: loss improved from 1.44265 to 1.43922, saving model to RNN_weights.hdf5\n",
      "Epoch 121/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.4329\n",
      "\n",
      "Epoch 00121: loss improved from 1.43922 to 1.43289, saving model to RNN_weights.hdf5\n",
      "Epoch 122/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.4333\n",
      "\n",
      "Epoch 00122: loss did not improve from 1.43289\n",
      "Epoch 123/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4272\n",
      "\n",
      "Epoch 00123: loss improved from 1.43289 to 1.42718, saving model to RNN_weights.hdf5\n",
      "Epoch 124/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4223\n",
      "\n",
      "Epoch 00124: loss improved from 1.42718 to 1.42225, saving model to RNN_weights.hdf5\n",
      "Epoch 125/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4180\n",
      "\n",
      "Epoch 00125: loss improved from 1.42225 to 1.41795, saving model to RNN_weights.hdf5\n",
      "Epoch 126/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4161\n",
      "\n",
      "Epoch 00126: loss improved from 1.41795 to 1.41610, saving model to RNN_weights.hdf5\n",
      "Epoch 127/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4118\n",
      "\n",
      "Epoch 00127: loss improved from 1.41610 to 1.41177, saving model to RNN_weights.hdf5\n",
      "Epoch 128/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.4008\n",
      "\n",
      "Epoch 00128: loss improved from 1.41177 to 1.40081, saving model to RNN_weights.hdf5\n",
      "Epoch 129/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.3966\n",
      "\n",
      "Epoch 00129: loss improved from 1.40081 to 1.39664, saving model to RNN_weights.hdf5\n",
      "Epoch 130/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3997\n",
      "\n",
      "Epoch 00130: loss did not improve from 1.39664\n",
      "Epoch 131/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3922\n",
      "\n",
      "Epoch 00131: loss improved from 1.39664 to 1.39223, saving model to RNN_weights.hdf5\n",
      "Epoch 132/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.3908\n",
      "\n",
      "Epoch 00132: loss improved from 1.39223 to 1.39076, saving model to RNN_weights.hdf5\n",
      "Epoch 133/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3811\n",
      "\n",
      "Epoch 00133: loss improved from 1.39076 to 1.38108, saving model to RNN_weights.hdf5\n",
      "Epoch 134/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3802\n",
      "\n",
      "Epoch 00134: loss improved from 1.38108 to 1.38023, saving model to RNN_weights.hdf5\n",
      "Epoch 135/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3748\n",
      "\n",
      "Epoch 00135: loss improved from 1.38023 to 1.37484, saving model to RNN_weights.hdf5\n",
      "Epoch 136/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3774\n",
      "\n",
      "Epoch 00136: loss did not improve from 1.37484\n",
      "Epoch 137/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3625\n",
      "\n",
      "Epoch 00137: loss improved from 1.37484 to 1.36249, saving model to RNN_weights.hdf5\n",
      "Epoch 138/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3664\n",
      "\n",
      "Epoch 00138: loss did not improve from 1.36249\n",
      "Epoch 139/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3588\n",
      "\n",
      "Epoch 00139: loss improved from 1.36249 to 1.35878, saving model to RNN_weights.hdf5\n",
      "Epoch 140/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3531\n",
      "\n",
      "Epoch 00140: loss improved from 1.35878 to 1.35311, saving model to RNN_weights.hdf5\n",
      "Epoch 141/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3501\n",
      "\n",
      "Epoch 00141: loss improved from 1.35311 to 1.35014, saving model to RNN_weights.hdf5\n",
      "Epoch 142/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3423\n",
      "\n",
      "Epoch 00142: loss improved from 1.35014 to 1.34233, saving model to RNN_weights.hdf5\n",
      "Epoch 143/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 1.3406\n",
      "\n",
      "Epoch 00143: loss improved from 1.34233 to 1.34062, saving model to RNN_weights.hdf5\n",
      "Epoch 144/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.3415\n",
      "\n",
      "Epoch 00144: loss did not improve from 1.34062\n",
      "Epoch 145/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3355\n",
      "\n",
      "Epoch 00145: loss improved from 1.34062 to 1.33548, saving model to RNN_weights.hdf5\n",
      "Epoch 146/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3284\n",
      "\n",
      "Epoch 00146: loss improved from 1.33548 to 1.32838, saving model to RNN_weights.hdf5\n",
      "Epoch 147/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3320\n",
      "\n",
      "Epoch 00147: loss did not improve from 1.32838\n",
      "Epoch 148/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3237\n",
      "\n",
      "Epoch 00148: loss improved from 1.32838 to 1.32367, saving model to RNN_weights.hdf5\n",
      "Epoch 149/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3220\n",
      "\n",
      "Epoch 00149: loss improved from 1.32367 to 1.32199, saving model to RNN_weights.hdf5\n",
      "Epoch 150/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3211\n",
      "\n",
      "Epoch 00150: loss improved from 1.32199 to 1.32113, saving model to RNN_weights.hdf5\n",
      "Epoch 151/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3165\n",
      "\n",
      "Epoch 00151: loss improved from 1.32113 to 1.31649, saving model to RNN_weights.hdf5\n",
      "Epoch 152/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3153\n",
      "\n",
      "Epoch 00152: loss improved from 1.31649 to 1.31529, saving model to RNN_weights.hdf5\n",
      "Epoch 153/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3058\n",
      "\n",
      "Epoch 00153: loss improved from 1.31529 to 1.30582, saving model to RNN_weights.hdf5\n",
      "Epoch 154/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.3012\n",
      "\n",
      "Epoch 00154: loss improved from 1.30582 to 1.30124, saving model to RNN_weights.hdf5\n",
      "Epoch 155/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.3062\n",
      "\n",
      "Epoch 00155: loss did not improve from 1.30124\n",
      "Epoch 156/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2961\n",
      "\n",
      "Epoch 00156: loss improved from 1.30124 to 1.29613, saving model to RNN_weights.hdf5\n",
      "Epoch 157/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2936\n",
      "\n",
      "Epoch 00157: loss improved from 1.29613 to 1.29362, saving model to RNN_weights.hdf5\n",
      "Epoch 158/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2911\n",
      "\n",
      "Epoch 00158: loss improved from 1.29362 to 1.29110, saving model to RNN_weights.hdf5\n",
      "Epoch 159/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2879\n",
      "\n",
      "Epoch 00159: loss improved from 1.29110 to 1.28793, saving model to RNN_weights.hdf5\n",
      "Epoch 160/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2832\n",
      "\n",
      "Epoch 00160: loss improved from 1.28793 to 1.28320, saving model to RNN_weights.hdf5\n",
      "Epoch 161/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2817\n",
      "\n",
      "Epoch 00161: loss improved from 1.28320 to 1.28167, saving model to RNN_weights.hdf5\n",
      "Epoch 162/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2799\n",
      "\n",
      "Epoch 00162: loss improved from 1.28167 to 1.27987, saving model to RNN_weights.hdf5\n",
      "Epoch 163/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2799\n",
      "\n",
      "Epoch 00163: loss improved from 1.27987 to 1.27986, saving model to RNN_weights.hdf5\n",
      "Epoch 164/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.2720\n",
      "\n",
      "Epoch 00164: loss improved from 1.27986 to 1.27196, saving model to RNN_weights.hdf5\n",
      "Epoch 165/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2737\n",
      "\n",
      "Epoch 00165: loss did not improve from 1.27196\n",
      "Epoch 166/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2698\n",
      "\n",
      "Epoch 00166: loss improved from 1.27196 to 1.26981, saving model to RNN_weights.hdf5\n",
      "Epoch 167/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2630\n",
      "\n",
      "Epoch 00167: loss improved from 1.26981 to 1.26298, saving model to RNN_weights.hdf5\n",
      "Epoch 168/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2630\n",
      "\n",
      "Epoch 00168: loss did not improve from 1.26298\n",
      "Epoch 169/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2550\n",
      "\n",
      "Epoch 00169: loss improved from 1.26298 to 1.25498, saving model to RNN_weights.hdf5\n",
      "Epoch 170/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2500\n",
      "\n",
      "Epoch 00170: loss improved from 1.25498 to 1.24997, saving model to RNN_weights.hdf5\n",
      "Epoch 171/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2523\n",
      "\n",
      "Epoch 00171: loss did not improve from 1.24997\n",
      "Epoch 172/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2474\n",
      "\n",
      "Epoch 00172: loss improved from 1.24997 to 1.24741, saving model to RNN_weights.hdf5\n",
      "Epoch 173/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2456\n",
      "\n",
      "Epoch 00173: loss improved from 1.24741 to 1.24560, saving model to RNN_weights.hdf5\n",
      "Epoch 174/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2440\n",
      "\n",
      "Epoch 00174: loss improved from 1.24560 to 1.24405, saving model to RNN_weights.hdf5\n",
      "Epoch 175/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2354\n",
      "\n",
      "Epoch 00175: loss improved from 1.24405 to 1.23541, saving model to RNN_weights.hdf5\n",
      "Epoch 176/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.2409\n",
      "\n",
      "Epoch 00176: loss did not improve from 1.23541\n",
      "Epoch 177/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2372\n",
      "\n",
      "Epoch 00177: loss did not improve from 1.23541\n",
      "Epoch 178/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2254\n",
      "\n",
      "Epoch 00178: loss improved from 1.23541 to 1.22536, saving model to RNN_weights.hdf5\n",
      "Epoch 179/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2337\n",
      "\n",
      "Epoch 00179: loss did not improve from 1.22536\n",
      "Epoch 180/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2246\n",
      "\n",
      "Epoch 00180: loss improved from 1.22536 to 1.22459, saving model to RNN_weights.hdf5\n",
      "Epoch 181/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2201\n",
      "\n",
      "Epoch 00181: loss improved from 1.22459 to 1.22014, saving model to RNN_weights.hdf5\n",
      "Epoch 182/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2254\n",
      "\n",
      "Epoch 00182: loss did not improve from 1.22014\n",
      "Epoch 183/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2161\n",
      "\n",
      "Epoch 00183: loss improved from 1.22014 to 1.21606, saving model to RNN_weights.hdf5\n",
      "Epoch 184/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.2130\n",
      "\n",
      "Epoch 00184: loss improved from 1.21606 to 1.21295, saving model to RNN_weights.hdf5\n",
      "Epoch 185/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2128\n",
      "\n",
      "Epoch 00185: loss improved from 1.21295 to 1.21281, saving model to RNN_weights.hdf5\n",
      "Epoch 186/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2084\n",
      "\n",
      "Epoch 00186: loss improved from 1.21281 to 1.20845, saving model to RNN_weights.hdf5\n",
      "Epoch 187/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2081\n",
      "\n",
      "Epoch 00187: loss improved from 1.20845 to 1.20810, saving model to RNN_weights.hdf5\n",
      "Epoch 188/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2140\n",
      "\n",
      "Epoch 00188: loss did not improve from 1.20810\n",
      "Epoch 189/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2051\n",
      "\n",
      "Epoch 00189: loss improved from 1.20810 to 1.20510, saving model to RNN_weights.hdf5\n",
      "Epoch 190/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2089\n",
      "\n",
      "Epoch 00190: loss did not improve from 1.20510\n",
      "Epoch 191/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1961\n",
      "\n",
      "Epoch 00191: loss improved from 1.20510 to 1.19614, saving model to RNN_weights.hdf5\n",
      "Epoch 192/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.2022\n",
      "\n",
      "Epoch 00192: loss did not improve from 1.19614\n",
      "Epoch 193/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1934\n",
      "\n",
      "Epoch 00193: loss improved from 1.19614 to 1.19337, saving model to RNN_weights.hdf5\n",
      "Epoch 194/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1906\n",
      "\n",
      "Epoch 00194: loss improved from 1.19337 to 1.19061, saving model to RNN_weights.hdf5\n",
      "Epoch 195/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.1921\n",
      "\n",
      "Epoch 00195: loss did not improve from 1.19061\n",
      "Epoch 196/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1891\n",
      "\n",
      "Epoch 00196: loss improved from 1.19061 to 1.18911, saving model to RNN_weights.hdf5\n",
      "Epoch 197/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1835\n",
      "\n",
      "Epoch 00197: loss improved from 1.18911 to 1.18346, saving model to RNN_weights.hdf5\n",
      "Epoch 198/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1801\n",
      "\n",
      "Epoch 00198: loss improved from 1.18346 to 1.18013, saving model to RNN_weights.hdf5\n",
      "Epoch 199/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1824\n",
      "\n",
      "Epoch 00199: loss did not improve from 1.18013\n",
      "Epoch 200/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1733\n",
      "\n",
      "Epoch 00200: loss improved from 1.18013 to 1.17325, saving model to RNN_weights.hdf5\n",
      "Epoch 201/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1729\n",
      "\n",
      "Epoch 00201: loss improved from 1.17325 to 1.17292, saving model to RNN_weights.hdf5\n",
      "Epoch 202/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1789\n",
      "\n",
      "Epoch 00202: loss did not improve from 1.17292\n",
      "Epoch 203/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1666\n",
      "\n",
      "Epoch 00203: loss improved from 1.17292 to 1.16663, saving model to RNN_weights.hdf5\n",
      "Epoch 204/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1697\n",
      "\n",
      "Epoch 00204: loss did not improve from 1.16663\n",
      "Epoch 205/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1743\n",
      "\n",
      "Epoch 00205: loss did not improve from 1.16663\n",
      "Epoch 206/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1683\n",
      "\n",
      "Epoch 00206: loss did not improve from 1.16663\n",
      "Epoch 207/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1589\n",
      "\n",
      "Epoch 00207: loss improved from 1.16663 to 1.15890, saving model to RNN_weights.hdf5\n",
      "Epoch 208/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1608\n",
      "\n",
      "Epoch 00208: loss did not improve from 1.15890\n",
      "Epoch 209/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1592\n",
      "\n",
      "Epoch 00209: loss did not improve from 1.15890\n",
      "Epoch 210/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.1629\n",
      "\n",
      "Epoch 00210: loss did not improve from 1.15890\n",
      "Epoch 211/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1571\n",
      "\n",
      "Epoch 00211: loss improved from 1.15890 to 1.15711, saving model to RNN_weights.hdf5\n",
      "Epoch 212/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1561\n",
      "\n",
      "Epoch 00212: loss improved from 1.15711 to 1.15606, saving model to RNN_weights.hdf5\n",
      "Epoch 213/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1493\n",
      "\n",
      "Epoch 00213: loss improved from 1.15606 to 1.14934, saving model to RNN_weights.hdf5\n",
      "Epoch 214/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1451\n",
      "\n",
      "Epoch 00214: loss improved from 1.14934 to 1.14505, saving model to RNN_weights.hdf5\n",
      "Epoch 215/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1440\n",
      "\n",
      "Epoch 00215: loss improved from 1.14505 to 1.14400, saving model to RNN_weights.hdf5\n",
      "Epoch 216/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1406\n",
      "\n",
      "Epoch 00216: loss improved from 1.14400 to 1.14057, saving model to RNN_weights.hdf5\n",
      "Epoch 217/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.1388\n",
      "\n",
      "Epoch 00217: loss improved from 1.14057 to 1.13885, saving model to RNN_weights.hdf5\n",
      "Epoch 218/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1391\n",
      "\n",
      "Epoch 00218: loss did not improve from 1.13885\n",
      "Epoch 219/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1404\n",
      "\n",
      "Epoch 00219: loss did not improve from 1.13885\n",
      "Epoch 220/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1351\n",
      "\n",
      "Epoch 00220: loss improved from 1.13885 to 1.13505, saving model to RNN_weights.hdf5\n",
      "Epoch 221/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1385\n",
      "\n",
      "Epoch 00221: loss did not improve from 1.13505\n",
      "Epoch 222/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1359\n",
      "\n",
      "Epoch 00222: loss did not improve from 1.13505\n",
      "Epoch 223/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1280\n",
      "\n",
      "Epoch 00223: loss improved from 1.13505 to 1.12802, saving model to RNN_weights.hdf5\n",
      "Epoch 224/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.1251\n",
      "\n",
      "Epoch 00224: loss improved from 1.12802 to 1.12508, saving model to RNN_weights.hdf5\n",
      "Epoch 225/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1255\n",
      "\n",
      "Epoch 00225: loss did not improve from 1.12508\n",
      "Epoch 226/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1257\n",
      "\n",
      "Epoch 00226: loss did not improve from 1.12508\n",
      "Epoch 227/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.1187\n",
      "\n",
      "Epoch 00227: loss improved from 1.12508 to 1.11873, saving model to RNN_weights.hdf5\n",
      "Epoch 228/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1139\n",
      "\n",
      "Epoch 00228: loss improved from 1.11873 to 1.11389, saving model to RNN_weights.hdf5\n",
      "Epoch 229/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1151\n",
      "\n",
      "Epoch 00229: loss did not improve from 1.11389\n",
      "Epoch 230/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1207\n",
      "\n",
      "Epoch 00230: loss did not improve from 1.11389\n",
      "Epoch 231/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1122\n",
      "\n",
      "Epoch 00231: loss improved from 1.11389 to 1.11222, saving model to RNN_weights.hdf5\n",
      "Epoch 232/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1159\n",
      "\n",
      "Epoch 00232: loss did not improve from 1.11222\n",
      "Epoch 233/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1149\n",
      "\n",
      "Epoch 00233: loss did not improve from 1.11222\n",
      "Epoch 234/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1129\n",
      "\n",
      "Epoch 00234: loss did not improve from 1.11222\n",
      "Epoch 235/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1103\n",
      "\n",
      "Epoch 00235: loss improved from 1.11222 to 1.11028, saving model to RNN_weights.hdf5\n",
      "Epoch 236/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1020\n",
      "\n",
      "Epoch 00236: loss improved from 1.11028 to 1.10203, saving model to RNN_weights.hdf5\n",
      "Epoch 237/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1091\n",
      "\n",
      "Epoch 00237: loss did not improve from 1.10203\n",
      "Epoch 238/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1045\n",
      "\n",
      "Epoch 00238: loss did not improve from 1.10203\n",
      "Epoch 239/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0995\n",
      "\n",
      "Epoch 00239: loss improved from 1.10203 to 1.09952, saving model to RNN_weights.hdf5\n",
      "Epoch 240/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0975\n",
      "\n",
      "Epoch 00240: loss improved from 1.09952 to 1.09752, saving model to RNN_weights.hdf5\n",
      "Epoch 241/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.1047\n",
      "\n",
      "Epoch 00241: loss did not improve from 1.09752\n",
      "Epoch 242/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0943\n",
      "\n",
      "Epoch 00242: loss improved from 1.09752 to 1.09433, saving model to RNN_weights.hdf5\n",
      "Epoch 243/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0981\n",
      "\n",
      "Epoch 00243: loss did not improve from 1.09433\n",
      "Epoch 244/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0895\n",
      "\n",
      "Epoch 00244: loss improved from 1.09433 to 1.08955, saving model to RNN_weights.hdf5\n",
      "Epoch 245/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0949\n",
      "\n",
      "Epoch 00245: loss did not improve from 1.08955\n",
      "Epoch 246/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0893\n",
      "\n",
      "Epoch 00246: loss improved from 1.08955 to 1.08926, saving model to RNN_weights.hdf5\n",
      "Epoch 247/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0904\n",
      "\n",
      "Epoch 00247: loss did not improve from 1.08926\n",
      "Epoch 248/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0828\n",
      "\n",
      "Epoch 00248: loss improved from 1.08926 to 1.08282, saving model to RNN_weights.hdf5\n",
      "Epoch 249/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0829\n",
      "\n",
      "Epoch 00249: loss did not improve from 1.08282\n",
      "Epoch 250/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0815\n",
      "\n",
      "Epoch 00250: loss improved from 1.08282 to 1.08145, saving model to RNN_weights.hdf5\n",
      "Epoch 251/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0822\n",
      "\n",
      "Epoch 00251: loss did not improve from 1.08145\n",
      "Epoch 252/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0831\n",
      "\n",
      "Epoch 00252: loss did not improve from 1.08145\n",
      "Epoch 253/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0753\n",
      "\n",
      "Epoch 00253: loss improved from 1.08145 to 1.07534, saving model to RNN_weights.hdf5\n",
      "Epoch 254/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0732\n",
      "\n",
      "Epoch 00254: loss improved from 1.07534 to 1.07318, saving model to RNN_weights.hdf5\n",
      "Epoch 255/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0831\n",
      "\n",
      "Epoch 00255: loss did not improve from 1.07318\n",
      "Epoch 256/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0705\n",
      "\n",
      "Epoch 00256: loss improved from 1.07318 to 1.07055, saving model to RNN_weights.hdf5\n",
      "Epoch 257/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0742\n",
      "\n",
      "Epoch 00257: loss did not improve from 1.07055\n",
      "Epoch 258/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0704\n",
      "\n",
      "Epoch 00258: loss improved from 1.07055 to 1.07043, saving model to RNN_weights.hdf5\n",
      "Epoch 259/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0736\n",
      "\n",
      "Epoch 00259: loss did not improve from 1.07043\n",
      "Epoch 260/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0698\n",
      "\n",
      "Epoch 00260: loss improved from 1.07043 to 1.06983, saving model to RNN_weights.hdf5\n",
      "Epoch 261/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0743\n",
      "\n",
      "Epoch 00261: loss did not improve from 1.06983\n",
      "Epoch 262/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0684\n",
      "\n",
      "Epoch 00262: loss improved from 1.06983 to 1.06842, saving model to RNN_weights.hdf5\n",
      "Epoch 263/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0624\n",
      "\n",
      "Epoch 00263: loss improved from 1.06842 to 1.06242, saving model to RNN_weights.hdf5\n",
      "Epoch 264/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0573\n",
      "\n",
      "Epoch 00264: loss improved from 1.06242 to 1.05730, saving model to RNN_weights.hdf5\n",
      "Epoch 265/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0587\n",
      "\n",
      "Epoch 00265: loss did not improve from 1.05730\n",
      "Epoch 266/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0647\n",
      "\n",
      "Epoch 00266: loss did not improve from 1.05730\n",
      "Epoch 267/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0605\n",
      "\n",
      "Epoch 00267: loss did not improve from 1.05730\n",
      "Epoch 268/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0591\n",
      "\n",
      "Epoch 00268: loss did not improve from 1.05730\n",
      "Epoch 269/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0562\n",
      "\n",
      "Epoch 00269: loss improved from 1.05730 to 1.05621, saving model to RNN_weights.hdf5\n",
      "Epoch 270/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0496\n",
      "\n",
      "Epoch 00270: loss improved from 1.05621 to 1.04959, saving model to RNN_weights.hdf5\n",
      "Epoch 271/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0472\n",
      "\n",
      "Epoch 00271: loss improved from 1.04959 to 1.04718, saving model to RNN_weights.hdf5\n",
      "Epoch 272/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0512\n",
      "\n",
      "Epoch 00272: loss did not improve from 1.04718\n",
      "Epoch 273/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0549\n",
      "\n",
      "Epoch 00273: loss did not improve from 1.04718\n",
      "Epoch 274/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0483\n",
      "\n",
      "Epoch 00274: loss did not improve from 1.04718\n",
      "Epoch 275/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0425\n",
      "\n",
      "Epoch 00275: loss improved from 1.04718 to 1.04246, saving model to RNN_weights.hdf5\n",
      "Epoch 276/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0467\n",
      "\n",
      "Epoch 00276: loss did not improve from 1.04246\n",
      "Epoch 277/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0339\n",
      "\n",
      "Epoch 00277: loss improved from 1.04246 to 1.03391, saving model to RNN_weights.hdf5\n",
      "Epoch 278/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0482\n",
      "\n",
      "Epoch 00278: loss did not improve from 1.03391\n",
      "Epoch 279/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0473\n",
      "\n",
      "Epoch 00279: loss did not improve from 1.03391\n",
      "Epoch 280/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0381\n",
      "\n",
      "Epoch 00280: loss did not improve from 1.03391\n",
      "Epoch 281/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0345\n",
      "\n",
      "Epoch 00281: loss did not improve from 1.03391\n",
      "Epoch 282/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0363\n",
      "\n",
      "Epoch 00282: loss did not improve from 1.03391\n",
      "Epoch 283/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0393\n",
      "\n",
      "Epoch 00283: loss did not improve from 1.03391\n",
      "Epoch 284/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0366\n",
      "\n",
      "Epoch 00284: loss did not improve from 1.03391\n",
      "Epoch 285/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0321\n",
      "\n",
      "Epoch 00285: loss improved from 1.03391 to 1.03205, saving model to RNN_weights.hdf5\n",
      "Epoch 286/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0364\n",
      "\n",
      "Epoch 00286: loss did not improve from 1.03205\n",
      "Epoch 287/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0263\n",
      "\n",
      "Epoch 00287: loss improved from 1.03205 to 1.02629, saving model to RNN_weights.hdf5\n",
      "Epoch 288/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0236\n",
      "\n",
      "Epoch 00288: loss improved from 1.02629 to 1.02358, saving model to RNN_weights.hdf5\n",
      "Epoch 289/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0306\n",
      "\n",
      "Epoch 00289: loss did not improve from 1.02358\n",
      "Epoch 290/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0301\n",
      "\n",
      "Epoch 00290: loss did not improve from 1.02358\n",
      "Epoch 291/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0313\n",
      "\n",
      "Epoch 00291: loss did not improve from 1.02358\n",
      "Epoch 292/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0217\n",
      "\n",
      "Epoch 00292: loss improved from 1.02358 to 1.02172, saving model to RNN_weights.hdf5\n",
      "Epoch 293/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0232\n",
      "\n",
      "Epoch 00293: loss did not improve from 1.02172\n",
      "Epoch 294/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0181\n",
      "\n",
      "Epoch 00294: loss improved from 1.02172 to 1.01814, saving model to RNN_weights.hdf5\n",
      "Epoch 295/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0264\n",
      "\n",
      "Epoch 00295: loss did not improve from 1.01814\n",
      "Epoch 296/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0188\n",
      "\n",
      "Epoch 00296: loss did not improve from 1.01814\n",
      "Epoch 297/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0198\n",
      "\n",
      "Epoch 00297: loss did not improve from 1.01814\n",
      "Epoch 298/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0186\n",
      "\n",
      "Epoch 00298: loss did not improve from 1.01814\n",
      "Epoch 299/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0193\n",
      "\n",
      "Epoch 00299: loss did not improve from 1.01814\n",
      "Epoch 300/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0162\n",
      "\n",
      "Epoch 00300: loss improved from 1.01814 to 1.01624, saving model to RNN_weights.hdf5\n",
      "Epoch 301/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0089\n",
      "\n",
      "Epoch 00301: loss improved from 1.01624 to 1.00885, saving model to RNN_weights.hdf5\n",
      "Epoch 302/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0046\n",
      "\n",
      "Epoch 00302: loss improved from 1.00885 to 1.00462, saving model to RNN_weights.hdf5\n",
      "Epoch 303/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0181\n",
      "\n",
      "Epoch 00303: loss did not improve from 1.00462\n",
      "Epoch 304/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0133\n",
      "\n",
      "Epoch 00304: loss did not improve from 1.00462\n",
      "Epoch 305/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0079\n",
      "\n",
      "Epoch 00305: loss did not improve from 1.00462\n",
      "Epoch 306/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0084\n",
      "\n",
      "Epoch 00306: loss did not improve from 1.00462\n",
      "Epoch 307/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0085\n",
      "\n",
      "Epoch 00307: loss did not improve from 1.00462\n",
      "Epoch 308/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 1.0097\n",
      "\n",
      "Epoch 00308: loss did not improve from 1.00462\n",
      "Epoch 309/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0081\n",
      "\n",
      "Epoch 00309: loss did not improve from 1.00462\n",
      "Epoch 310/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0029\n",
      "\n",
      "Epoch 00310: loss improved from 1.00462 to 1.00285, saving model to RNN_weights.hdf5\n",
      "Epoch 311/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9992\n",
      "\n",
      "Epoch 00311: loss improved from 1.00285 to 0.99916, saving model to RNN_weights.hdf5\n",
      "Epoch 312/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0031\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.99916\n",
      "Epoch 313/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 1.0010\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.99916\n",
      "Epoch 314/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0008\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.99916\n",
      "Epoch 315/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9912\n",
      "\n",
      "Epoch 00315: loss improved from 0.99916 to 0.99125, saving model to RNN_weights.hdf5\n",
      "Epoch 316/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9926\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.99125\n",
      "Epoch 317/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9896\n",
      "\n",
      "Epoch 00317: loss improved from 0.99125 to 0.98958, saving model to RNN_weights.hdf5\n",
      "Epoch 318/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9991\n",
      "\n",
      "Epoch 00318: loss did not improve from 0.98958\n",
      "Epoch 319/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9940\n",
      "\n",
      "Epoch 00319: loss did not improve from 0.98958\n",
      "Epoch 320/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 1.0050\n",
      "\n",
      "Epoch 00320: loss did not improve from 0.98958\n",
      "Epoch 321/700\n",
      "89752/89752 [==============================] - 113s 1ms/step - loss: 0.9926\n",
      "\n",
      "Epoch 00321: loss did not improve from 0.98958\n",
      "Epoch 322/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9914\n",
      "\n",
      "Epoch 00322: loss did not improve from 0.98958\n",
      "Epoch 323/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9964\n",
      "\n",
      "Epoch 00323: loss did not improve from 0.98958\n",
      "Epoch 324/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9898\n",
      "\n",
      "Epoch 00324: loss did not improve from 0.98958\n",
      "Epoch 325/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9858\n",
      "\n",
      "Epoch 00325: loss improved from 0.98958 to 0.98576, saving model to RNN_weights.hdf5\n",
      "Epoch 326/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9908\n",
      "\n",
      "Epoch 00326: loss did not improve from 0.98576\n",
      "Epoch 327/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9872\n",
      "\n",
      "Epoch 00327: loss did not improve from 0.98576\n",
      "Epoch 328/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9796\n",
      "\n",
      "Epoch 00328: loss improved from 0.98576 to 0.97956, saving model to RNN_weights.hdf5\n",
      "Epoch 329/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9843\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.97956\n",
      "Epoch 330/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9821\n",
      "\n",
      "Epoch 00330: loss did not improve from 0.97956\n",
      "Epoch 331/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9853\n",
      "\n",
      "Epoch 00331: loss did not improve from 0.97956\n",
      "Epoch 332/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9858\n",
      "\n",
      "Epoch 00332: loss did not improve from 0.97956\n",
      "Epoch 333/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9812\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.97956\n",
      "Epoch 334/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9831\n",
      "\n",
      "Epoch 00334: loss did not improve from 0.97956\n",
      "Epoch 335/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9777\n",
      "\n",
      "Epoch 00335: loss improved from 0.97956 to 0.97774, saving model to RNN_weights.hdf5\n",
      "Epoch 336/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9760\n",
      "\n",
      "Epoch 00336: loss improved from 0.97774 to 0.97604, saving model to RNN_weights.hdf5\n",
      "Epoch 337/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9721\n",
      "\n",
      "Epoch 00337: loss improved from 0.97604 to 0.97212, saving model to RNN_weights.hdf5\n",
      "Epoch 338/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9726\n",
      "\n",
      "Epoch 00338: loss did not improve from 0.97212\n",
      "Epoch 339/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9742\n",
      "\n",
      "Epoch 00339: loss did not improve from 0.97212\n",
      "Epoch 340/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9751\n",
      "\n",
      "Epoch 00340: loss did not improve from 0.97212\n",
      "Epoch 341/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9656\n",
      "\n",
      "Epoch 00341: loss improved from 0.97212 to 0.96564, saving model to RNN_weights.hdf5\n",
      "Epoch 342/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 0.9742\n",
      "\n",
      "Epoch 00342: loss did not improve from 0.96564\n",
      "Epoch 343/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9660\n",
      "\n",
      "Epoch 00343: loss did not improve from 0.96564\n",
      "Epoch 344/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9708\n",
      "\n",
      "Epoch 00344: loss did not improve from 0.96564\n",
      "Epoch 345/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9694\n",
      "\n",
      "Epoch 00345: loss did not improve from 0.96564\n",
      "Epoch 346/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9645\n",
      "\n",
      "Epoch 00346: loss improved from 0.96564 to 0.96447, saving model to RNN_weights.hdf5\n",
      "Epoch 347/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9626\n",
      "\n",
      "Epoch 00347: loss improved from 0.96447 to 0.96262, saving model to RNN_weights.hdf5\n",
      "Epoch 348/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9657\n",
      "\n",
      "Epoch 00348: loss did not improve from 0.96262\n",
      "Epoch 349/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9664\n",
      "\n",
      "Epoch 00349: loss did not improve from 0.96262\n",
      "Epoch 350/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9580\n",
      "\n",
      "Epoch 00350: loss improved from 0.96262 to 0.95805, saving model to RNN_weights.hdf5\n",
      "Epoch 351/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9633\n",
      "\n",
      "Epoch 00351: loss did not improve from 0.95805\n",
      "Epoch 352/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9561\n",
      "\n",
      "Epoch 00352: loss improved from 0.95805 to 0.95614, saving model to RNN_weights.hdf5\n",
      "Epoch 353/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9609\n",
      "\n",
      "Epoch 00353: loss did not improve from 0.95614\n",
      "Epoch 354/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9658\n",
      "\n",
      "Epoch 00354: loss did not improve from 0.95614\n",
      "Epoch 355/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9598\n",
      "\n",
      "Epoch 00355: loss did not improve from 0.95614\n",
      "Epoch 356/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9583\n",
      "\n",
      "Epoch 00356: loss did not improve from 0.95614\n",
      "Epoch 357/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9552\n",
      "\n",
      "Epoch 00357: loss improved from 0.95614 to 0.95518, saving model to RNN_weights.hdf5\n",
      "Epoch 358/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9545\n",
      "\n",
      "Epoch 00358: loss improved from 0.95518 to 0.95453, saving model to RNN_weights.hdf5\n",
      "Epoch 359/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9501\n",
      "\n",
      "Epoch 00359: loss improved from 0.95453 to 0.95014, saving model to RNN_weights.hdf5\n",
      "Epoch 360/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9471\n",
      "\n",
      "Epoch 00360: loss improved from 0.95014 to 0.94705, saving model to RNN_weights.hdf5\n",
      "Epoch 361/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9491\n",
      "\n",
      "Epoch 00361: loss did not improve from 0.94705\n",
      "Epoch 362/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9533\n",
      "\n",
      "Epoch 00362: loss did not improve from 0.94705\n",
      "Epoch 363/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9475\n",
      "\n",
      "Epoch 00363: loss did not improve from 0.94705\n",
      "Epoch 364/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9569\n",
      "\n",
      "Epoch 00364: loss did not improve from 0.94705\n",
      "Epoch 365/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9483\n",
      "\n",
      "Epoch 00365: loss did not improve from 0.94705\n",
      "Epoch 366/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9440\n",
      "\n",
      "Epoch 00366: loss improved from 0.94705 to 0.94399, saving model to RNN_weights.hdf5\n",
      "Epoch 367/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9498\n",
      "\n",
      "Epoch 00367: loss did not improve from 0.94399\n",
      "Epoch 368/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9459\n",
      "\n",
      "Epoch 00368: loss did not improve from 0.94399\n",
      "Epoch 369/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9487\n",
      "\n",
      "Epoch 00369: loss did not improve from 0.94399\n",
      "Epoch 370/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9482\n",
      "\n",
      "Epoch 00370: loss did not improve from 0.94399\n",
      "Epoch 371/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9467\n",
      "\n",
      "Epoch 00371: loss did not improve from 0.94399\n",
      "Epoch 372/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9451\n",
      "\n",
      "Epoch 00372: loss did not improve from 0.94399\n",
      "Epoch 373/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9411\n",
      "\n",
      "Epoch 00373: loss improved from 0.94399 to 0.94110, saving model to RNN_weights.hdf5\n",
      "Epoch 374/700\n",
      "89752/89752 [==============================] - 117s 1ms/step - loss: 0.9387\n",
      "\n",
      "Epoch 00374: loss improved from 0.94110 to 0.93874, saving model to RNN_weights.hdf5\n",
      "Epoch 375/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 0.9448\n",
      "\n",
      "Epoch 00375: loss did not improve from 0.93874\n",
      "Epoch 376/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9346\n",
      "\n",
      "Epoch 00376: loss improved from 0.93874 to 0.93455, saving model to RNN_weights.hdf5\n",
      "Epoch 377/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9371\n",
      "\n",
      "Epoch 00377: loss did not improve from 0.93455\n",
      "Epoch 378/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9393\n",
      "\n",
      "Epoch 00378: loss did not improve from 0.93455\n",
      "Epoch 379/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9377\n",
      "\n",
      "Epoch 00379: loss did not improve from 0.93455\n",
      "Epoch 380/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9415\n",
      "\n",
      "Epoch 00380: loss did not improve from 0.93455\n",
      "Epoch 381/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9332\n",
      "\n",
      "Epoch 00381: loss improved from 0.93455 to 0.93317, saving model to RNN_weights.hdf5\n",
      "Epoch 382/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9356\n",
      "\n",
      "Epoch 00382: loss did not improve from 0.93317\n",
      "Epoch 383/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9365\n",
      "\n",
      "Epoch 00383: loss did not improve from 0.93317\n",
      "Epoch 384/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9382\n",
      "\n",
      "Epoch 00384: loss did not improve from 0.93317\n",
      "Epoch 385/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9284\n",
      "\n",
      "Epoch 00385: loss improved from 0.93317 to 0.92842, saving model to RNN_weights.hdf5\n",
      "Epoch 386/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9338\n",
      "\n",
      "Epoch 00386: loss did not improve from 0.92842\n",
      "Epoch 387/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9332\n",
      "\n",
      "Epoch 00387: loss did not improve from 0.92842\n",
      "Epoch 388/700\n",
      "89752/89752 [==============================] - 114s 1ms/step - loss: 0.9343\n",
      "\n",
      "Epoch 00388: loss did not improve from 0.92842\n",
      "Epoch 389/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9333\n",
      "\n",
      "Epoch 00389: loss did not improve from 0.92842\n",
      "Epoch 390/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9231\n",
      "\n",
      "Epoch 00390: loss improved from 0.92842 to 0.92310, saving model to RNN_weights.hdf5\n",
      "Epoch 391/700\n",
      "89752/89752 [==============================] - 115s 1ms/step - loss: 0.9292\n",
      "\n",
      "Epoch 00391: loss did not improve from 0.92310\n",
      "Epoch 392/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 0.9257\n",
      "\n",
      "Epoch 00392: loss did not improve from 0.92310\n",
      "Epoch 393/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 0.9324\n",
      "\n",
      "Epoch 00393: loss did not improve from 0.92310\n",
      "Epoch 394/700\n",
      "89752/89752 [==============================] - 116s 1ms/step - loss: 0.9166\n",
      "\n",
      "Epoch 00394: loss improved from 0.92310 to 0.91657, saving model to RNN_weights.hdf5\n",
      "Epoch 395/700\n",
      "89752/89752 [==============================] - 117s 1ms/step - loss: 0.9266\n",
      "\n",
      "Epoch 00395: loss did not improve from 0.91657\n",
      "Epoch 396/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.9238\n",
      "\n",
      "Epoch 00396: loss did not improve from 0.91657\n",
      "Epoch 397/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.9263\n",
      "\n",
      "Epoch 00397: loss did not improve from 0.91657\n",
      "Epoch 398/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.9251\n",
      "\n",
      "Epoch 00398: loss did not improve from 0.91657\n",
      "Epoch 399/700\n",
      "89752/89752 [==============================] - 119s 1ms/step - loss: 0.9244\n",
      "\n",
      "Epoch 00399: loss did not improve from 0.91657\n",
      "Epoch 400/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9205\n",
      "\n",
      "Epoch 00400: loss did not improve from 0.91657\n",
      "Epoch 401/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.9203\n",
      "\n",
      "Epoch 00401: loss did not improve from 0.91657\n",
      "Epoch 402/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9249\n",
      "\n",
      "Epoch 00402: loss did not improve from 0.91657\n",
      "Epoch 403/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.9154\n",
      "\n",
      "Epoch 00403: loss improved from 0.91657 to 0.91541, saving model to RNN_weights.hdf5\n",
      "Epoch 404/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9279\n",
      "\n",
      "Epoch 00404: loss did not improve from 0.91541\n",
      "Epoch 405/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9185\n",
      "\n",
      "Epoch 00405: loss did not improve from 0.91541\n",
      "Epoch 406/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9154\n",
      "\n",
      "Epoch 00406: loss did not improve from 0.91541\n",
      "Epoch 407/700\n",
      "89752/89752 [==============================] - 126s 1ms/step - loss: 0.9210\n",
      "\n",
      "Epoch 00407: loss did not improve from 0.91541\n",
      "Epoch 408/700\n",
      "89752/89752 [==============================] - 135s 2ms/step - loss: 0.9183\n",
      "\n",
      "Epoch 00408: loss did not improve from 0.91541\n",
      "Epoch 409/700\n",
      "89752/89752 [==============================] - 133s 1ms/step - loss: 0.9161\n",
      "\n",
      "Epoch 00409: loss did not improve from 0.91541\n",
      "Epoch 410/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 135s 2ms/step - loss: 0.9187\n",
      "\n",
      "Epoch 00410: loss did not improve from 0.91541\n",
      "Epoch 411/700\n",
      "89752/89752 [==============================] - 137s 2ms/step - loss: 0.9207\n",
      "\n",
      "Epoch 00411: loss did not improve from 0.91541\n",
      "Epoch 412/700\n",
      "89752/89752 [==============================] - 180s 2ms/step - loss: 0.9110\n",
      "\n",
      "Epoch 00412: loss improved from 0.91541 to 0.91101, saving model to RNN_weights.hdf5\n",
      "Epoch 413/700\n",
      "89752/89752 [==============================] - 215s 2ms/step - loss: 0.9177\n",
      "\n",
      "Epoch 00413: loss did not improve from 0.91101\n",
      "Epoch 414/700\n",
      "89752/89752 [==============================] - 187s 2ms/step - loss: 0.9192\n",
      "\n",
      "Epoch 00414: loss did not improve from 0.91101\n",
      "Epoch 415/700\n",
      "89752/89752 [==============================] - 203s 2ms/step - loss: 0.9109\n",
      "\n",
      "Epoch 00415: loss improved from 0.91101 to 0.91094, saving model to RNN_weights.hdf5\n",
      "Epoch 416/700\n",
      "89752/89752 [==============================] - 212s 2ms/step - loss: 0.9096\n",
      "\n",
      "Epoch 00416: loss improved from 0.91094 to 0.90961, saving model to RNN_weights.hdf5\n",
      "Epoch 417/700\n",
      "89752/89752 [==============================] - 213s 2ms/step - loss: 0.9128\n",
      "\n",
      "Epoch 00417: loss did not improve from 0.90961\n",
      "Epoch 418/700\n",
      "89752/89752 [==============================] - 218s 2ms/step - loss: 0.9086\n",
      "\n",
      "Epoch 00418: loss improved from 0.90961 to 0.90860, saving model to RNN_weights.hdf5\n",
      "Epoch 419/700\n",
      "89752/89752 [==============================] - 220s 2ms/step - loss: 0.9056\n",
      "\n",
      "Epoch 00419: loss improved from 0.90860 to 0.90557, saving model to RNN_weights.hdf5\n",
      "Epoch 420/700\n",
      "89752/89752 [==============================] - 125s 1ms/step - loss: 0.9129\n",
      "\n",
      "Epoch 00420: loss did not improve from 0.90557\n",
      "Epoch 421/700\n",
      "89752/89752 [==============================] - 132s 1ms/step - loss: 0.9026\n",
      "\n",
      "Epoch 00421: loss improved from 0.90557 to 0.90262, saving model to RNN_weights.hdf5\n",
      "Epoch 422/700\n",
      "89752/89752 [==============================] - 133s 1ms/step - loss: 0.9048\n",
      "\n",
      "Epoch 00422: loss did not improve from 0.90262\n",
      "Epoch 423/700\n",
      "89752/89752 [==============================] - 133s 1ms/step - loss: 0.9097\n",
      "\n",
      "Epoch 00423: loss did not improve from 0.90262\n",
      "Epoch 424/700\n",
      "89752/89752 [==============================] - 125s 1ms/step - loss: 0.9116\n",
      "\n",
      "Epoch 00424: loss did not improve from 0.90262\n",
      "Epoch 425/700\n",
      "89752/89752 [==============================] - 129s 1ms/step - loss: 0.9069\n",
      "\n",
      "Epoch 00425: loss did not improve from 0.90262\n",
      "Epoch 426/700\n",
      "89752/89752 [==============================] - 125s 1ms/step - loss: 0.9014\n",
      "\n",
      "Epoch 00426: loss improved from 0.90262 to 0.90144, saving model to RNN_weights.hdf5\n",
      "Epoch 427/700\n",
      "89752/89752 [==============================] - 126s 1ms/step - loss: 0.9054\n",
      "\n",
      "Epoch 00427: loss did not improve from 0.90144\n",
      "Epoch 428/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.9033\n",
      "\n",
      "Epoch 00428: loss did not improve from 0.90144\n",
      "Epoch 429/700\n",
      "89752/89752 [==============================] - 126s 1ms/step - loss: 0.9069\n",
      "\n",
      "Epoch 00429: loss did not improve from 0.90144\n",
      "Epoch 430/700\n",
      "89752/89752 [==============================] - 130s 1ms/step - loss: 0.9058\n",
      "\n",
      "Epoch 00430: loss did not improve from 0.90144\n",
      "Epoch 431/700\n",
      "89752/89752 [==============================] - 129s 1ms/step - loss: 0.8994\n",
      "\n",
      "Epoch 00431: loss improved from 0.90144 to 0.89939, saving model to RNN_weights.hdf5\n",
      "Epoch 432/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 0.9018\n",
      "\n",
      "Epoch 00432: loss did not improve from 0.89939\n",
      "Epoch 433/700\n",
      "89752/89752 [==============================] - 119s 1ms/step - loss: 0.9034\n",
      "\n",
      "Epoch 00433: loss did not improve from 0.89939\n",
      "Epoch 434/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.9075\n",
      "\n",
      "Epoch 00434: loss did not improve from 0.89939\n",
      "Epoch 435/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.8950\n",
      "\n",
      "Epoch 00435: loss improved from 0.89939 to 0.89504, saving model to RNN_weights.hdf5\n",
      "Epoch 436/700\n",
      "89752/89752 [==============================] - 119s 1ms/step - loss: 0.8989\n",
      "\n",
      "Epoch 00436: loss did not improve from 0.89504\n",
      "Epoch 437/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8959\n",
      "\n",
      "Epoch 00437: loss did not improve from 0.89504\n",
      "Epoch 438/700\n",
      "89752/89752 [==============================] - 118s 1ms/step - loss: 0.9013\n",
      "\n",
      "Epoch 00438: loss did not improve from 0.89504\n",
      "Epoch 439/700\n",
      "89752/89752 [==============================] - 119s 1ms/step - loss: 0.8975\n",
      "\n",
      "Epoch 00439: loss did not improve from 0.89504\n",
      "Epoch 440/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 0.9047\n",
      "\n",
      "Epoch 00440: loss did not improve from 0.89504\n",
      "Epoch 441/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 0.8949\n",
      "\n",
      "Epoch 00441: loss improved from 0.89504 to 0.89492, saving model to RNN_weights.hdf5\n",
      "Epoch 442/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 0.8976\n",
      "\n",
      "Epoch 00442: loss did not improve from 0.89492\n",
      "Epoch 443/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.9008\n",
      "\n",
      "Epoch 00443: loss did not improve from 0.89492\n",
      "Epoch 444/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.8899\n",
      "\n",
      "Epoch 00444: loss improved from 0.89492 to 0.88986, saving model to RNN_weights.hdf5\n",
      "Epoch 445/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8890\n",
      "\n",
      "Epoch 00445: loss improved from 0.88986 to 0.88902, saving model to RNN_weights.hdf5\n",
      "Epoch 446/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8950\n",
      "\n",
      "Epoch 00446: loss did not improve from 0.88902\n",
      "Epoch 447/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8958\n",
      "\n",
      "Epoch 00447: loss did not improve from 0.88902\n",
      "Epoch 448/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8927\n",
      "\n",
      "Epoch 00448: loss did not improve from 0.88902\n",
      "Epoch 449/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8937\n",
      "\n",
      "Epoch 00449: loss did not improve from 0.88902\n",
      "Epoch 450/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8905\n",
      "\n",
      "Epoch 00450: loss did not improve from 0.88902\n",
      "Epoch 451/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8927\n",
      "\n",
      "Epoch 00451: loss did not improve from 0.88902\n",
      "Epoch 452/700\n",
      "89752/89752 [==============================] - 136s 2ms/step - loss: 0.8852\n",
      "\n",
      "Epoch 00452: loss improved from 0.88902 to 0.88517, saving model to RNN_weights.hdf5\n",
      "Epoch 453/700\n",
      "89752/89752 [==============================] - 135s 2ms/step - loss: 0.8941\n",
      "\n",
      "Epoch 00453: loss did not improve from 0.88517\n",
      "Epoch 454/700\n",
      "89752/89752 [==============================] - 129s 1ms/step - loss: 0.8894\n",
      "\n",
      "Epoch 00454: loss did not improve from 0.88517\n",
      "Epoch 455/700\n",
      "89752/89752 [==============================] - 142s 2ms/step - loss: 0.8891\n",
      "\n",
      "Epoch 00455: loss did not improve from 0.88517\n",
      "Epoch 456/700\n",
      "89752/89752 [==============================] - 129s 1ms/step - loss: 0.8829\n",
      "\n",
      "Epoch 00456: loss improved from 0.88517 to 0.88288, saving model to RNN_weights.hdf5\n",
      "Epoch 457/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8899\n",
      "\n",
      "Epoch 00457: loss did not improve from 0.88288\n",
      "Epoch 458/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8887\n",
      "\n",
      "Epoch 00458: loss did not improve from 0.88288\n",
      "Epoch 459/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8792\n",
      "\n",
      "Epoch 00459: loss improved from 0.88288 to 0.87917, saving model to RNN_weights.hdf5\n",
      "Epoch 460/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8850\n",
      "\n",
      "Epoch 00460: loss did not improve from 0.87917\n",
      "Epoch 461/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8767\n",
      "\n",
      "Epoch 00461: loss improved from 0.87917 to 0.87669, saving model to RNN_weights.hdf5\n",
      "Epoch 462/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8835\n",
      "\n",
      "Epoch 00462: loss did not improve from 0.87669\n",
      "Epoch 463/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8813\n",
      "\n",
      "Epoch 00463: loss did not improve from 0.87669\n",
      "Epoch 464/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8805\n",
      "\n",
      "Epoch 00464: loss did not improve from 0.87669\n",
      "Epoch 465/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8781\n",
      "\n",
      "Epoch 00465: loss did not improve from 0.87669\n",
      "Epoch 466/700\n",
      "89752/89752 [==============================] - 125s 1ms/step - loss: 0.8810\n",
      "\n",
      "Epoch 00466: loss did not improve from 0.87669\n",
      "Epoch 467/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8905\n",
      "\n",
      "Epoch 00467: loss did not improve from 0.87669\n",
      "Epoch 468/700\n",
      "89752/89752 [==============================] - 125s 1ms/step - loss: 0.8787\n",
      "\n",
      "Epoch 00468: loss did not improve from 0.87669\n",
      "Epoch 469/700\n",
      "89752/89752 [==============================] - 124s 1ms/step - loss: 0.8868\n",
      "\n",
      "Epoch 00469: loss did not improve from 0.87669\n",
      "Epoch 470/700\n",
      "89752/89752 [==============================] - 126s 1ms/step - loss: 0.8792\n",
      "\n",
      "Epoch 00470: loss did not improve from 0.87669\n",
      "Epoch 471/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8825\n",
      "\n",
      "Epoch 00471: loss did not improve from 0.87669\n",
      "Epoch 472/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8800\n",
      "\n",
      "Epoch 00472: loss did not improve from 0.87669\n",
      "Epoch 473/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8860\n",
      "\n",
      "Epoch 00473: loss did not improve from 0.87669\n",
      "Epoch 474/700\n",
      "89752/89752 [==============================] - 121s 1ms/step - loss: 0.8827\n",
      "\n",
      "Epoch 00474: loss did not improve from 0.87669\n",
      "Epoch 475/700\n",
      "89752/89752 [==============================] - 120s 1ms/step - loss: 0.8797\n",
      "\n",
      "Epoch 00475: loss did not improve from 0.87669\n",
      "Epoch 476/700\n",
      "89752/89752 [==============================] - 119s 1ms/step - loss: 0.8720\n",
      "\n",
      "Epoch 00476: loss improved from 0.87669 to 0.87198, saving model to RNN_weights.hdf5\n",
      "Epoch 477/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8742\n",
      "\n",
      "Epoch 00477: loss did not improve from 0.87198\n",
      "Epoch 478/700\n",
      "89752/89752 [==============================] - 122s 1ms/step - loss: 0.8714\n",
      "\n",
      "Epoch 00478: loss improved from 0.87198 to 0.87138, saving model to RNN_weights.hdf5\n",
      "Epoch 479/700\n",
      "89752/89752 [==============================] - 137s 2ms/step - loss: 0.8734\n",
      "\n",
      "Epoch 00479: loss did not improve from 0.87138\n",
      "Epoch 480/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8747\n",
      "\n",
      "Epoch 00480: loss did not improve from 0.87138\n",
      "Epoch 481/700\n",
      "89752/89752 [==============================] - 130s 1ms/step - loss: 0.8789\n",
      "\n",
      "Epoch 00481: loss did not improve from 0.87138\n",
      "Epoch 482/700\n",
      "89752/89752 [==============================] - 130s 1ms/step - loss: 0.8772\n",
      "\n",
      "Epoch 00482: loss did not improve from 0.87138\n",
      "Epoch 483/700\n",
      "89752/89752 [==============================] - 123s 1ms/step - loss: 0.8763\n",
      "\n",
      "Epoch 00483: loss did not improve from 0.87138\n",
      "Epoch 484/700\n",
      "85248/89752 [===========================>..] - ETA: 6s - loss: 0.8656"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-d4846b6c41e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m700\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonGPU\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, epochs=700, batch_size=256, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading pre-trained weights\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-7a28de661125>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Plot loss values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Epoch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the graph, we can see the gradual decrease of cross-entropy loss, which is a good sign. There still exists a little downward slope, however it requires much more time to train and possibly some tuning in hyper parametes.\n",
    "\n",
    "Though, I think this is a good result for this kind of a model (cross-entropy loss 0.26763), which is much higher than was shown by MLE models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a dictionary that will convert the RNN output in numbers back to characters \n",
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" веду восточный мордор подзамочный город оживёт водоёмах сточных водах проёмах барочных окон e2 e4 ра \"\n"
     ]
    }
   ],
   "source": [
    "#Provide our model with random seed character from which it will generate a sequence of characters\n",
    "start = np.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "здат окну горов полигают оболе косорый год вышно бльнь поинм сорех чёрных пододит здёт роин зиву спор болет коробки свой фоека собода сельные мира стросок поле стали вывали соберик понале миме систу молице босточли бесе собой пол тхих просто прохиран врей послелных залаленнокнго слова полинил дорода полизно тебе лолотик тменти дород всё поле ребе собой пока ного бротить коров страший отсоровах устал оттаться току заслышал некоритовал просто дай нам просто конплекство косора дом стал ваш рэп баш "
     ]
    }
   ],
   "source": [
    "#Finally, generate rap\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, I choose LSTM RNN model among the presented models, since it has relatively low cross-entropy loss and more lively text generation.\n",
    "\n",
    "This model seems as a powerful tool to work with texts, but requires much higher resources to train.\n",
    "To my mind, in order to increase the quality of generated text by this model it is necessary to add much more training data and probably make more complex RNN structure. Since our data is now constrained to 14 tracks, the model sees some special words only ones and never again (e.g. 'хипари', 'дикари'), that is why sometimes it can partly borrow some verses with a little adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save our loss history in case we construct a graph\n",
    "hist_file = 'history.json' \n",
    "with open(hist_file, mode='w') as f:\n",
    "    history.to_json(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
